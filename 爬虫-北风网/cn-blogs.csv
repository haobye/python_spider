ASP.NET MVC5多语言切换快速实现方案
功能
实现动态切换语言,Demo 做了三种语言库可以切换,包括资源文件的定义,实体对象属性设置,后台代码Controller,IAuthorizationFilter,HtmlHelper的是实现,做法比较简单,配合我之前发布的# MVC Scaffolding SmartCode-Engine 更新 模板中新增了多语言资源文件的生成功能,发现我的这个框架和目前很流行的ABP框架是类似更有点像收费版的Asp.net Zero,只是我做的更加轻量级,更方便,更快速,可惜Asp.net Core 下的Scaffolding这块扩展不想MVC5那么容易.这块还需要研究,下一步就准备升级到asp.net core.

Github download Demo
 
具体实现方法


定义实体类通过Display属性定义Name ResourceType,需要读取的语言库资源文件







生成资源文件通过mvc scaffolding扩展工具会自动生成对应实体对象的3个资源文件默认中文,繁体,英文繁体需要自己翻译,英文内容根据字段名定义后已大写字母分割 DateTime 现实 Date Time

页面功能按钮语言资源文件库









 前后端代码实现语言切换功能


-选择切换语言

 



2.Js代码

/* multiple lang dropdown */
$('#dropdownlang-dropdown-menu').on('click', 'a', function () {
  const lang = this.text;
  const flag = this.firstElementChild.className;
  const culture = this.firstElementChild.getAttribute("culture");
  $('#dropdownlang').children()[0].className = flag;
  $('#dropdownlang').children()[1].innerHTML = lang;
  localStorage.setItem('lang-text', lang);
  localStorage.setItem('lang-css', flag);
  localStorage.setItem('lang-culture', culture);
  $.get('/Account/SetCulture?lang=' + culture).then(res => {
    if (res.success) {
      location.reload();
    }

  });
});
$(function () {
  const lang = localStorage.getItem('lang-text');
  const css = localStorage.getItem('lang-css');
  const culture = localStorage.getItem('lang-culture');
    scripttag = document.createElement("script");
    scripttag.type = "text/javascript";
    scripttag.src = src;
    document.body.appendChild(scripttag);
    $.parser.parse();
 
 
  };
  if (lang && css && culture) {
    $('#dropdownlang').children()[0].className = css;
    $('#dropdownlang').children()[1].innerHTML = lang;

  }
});

后端代码



[HttpGet]
    public ActionResult SetCulture(string lang) {
      //这里设置CultureInfo是多余的
      switch (lang.Trim())
      {
        case "en":
          CultureInfo.CurrentCulture = new CultureInfo("en-US");
          CultureInfo.CurrentUICulture = new CultureInfo("en-US");
          break;
        case "cn":
          CultureInfo.CurrentCulture = new CultureInfo("zh-CN");
          CultureInfo.CurrentUICulture = new CultureInfo("zh-CN");
          break;
        case "tw":
          CultureInfo.CurrentCulture = new CultureInfo("zh-TW");
          CultureInfo.CurrentUICulture = new CultureInfo("zh-TW");
          break;
      }
     //这里设置CultureInfo是多余的
      var cookie = new HttpCookie("culture", lang)
      {
        Expires = DateTime.Now.AddYears(1)
      };
      Response.Cookies.Add(cookie);
      return Json(new { success = true }, JsonRequestBehavior.AllowGet);

    }

CultureFilter 这是关键 这是没有使用RouteData,通过修改url来保存当前语言要改的地方很多还要修改路由规则,所以我就用Cookies来保存



public class CultureFilter : IAuthorizationFilter
  {
    private readonly string defaultCulture;

    public CultureFilter()
    {
      this.defaultCulture = "cn";
    }

    public void OnAuthorization(AuthorizationContext filterContext)
    {
      var culture = filterContext.HttpContext.Request.Cookies["culture"];
      var lang = defaultCulture;
      if (culture != null && culture.Value != null)
      {
        lang = culture.Value;
        filterContext.HttpContext.Response.Cookies.Set(culture);
      }
      switch (lang.Trim())
      {
        case "en":
          CultureInfo.CurrentCulture = new CultureInfo("en-US");
          CultureInfo.CurrentUICulture = new CultureInfo("en-US");
          //Thread.CurrentThread.CurrentUICulture = new CultureInfo("en-US");
          //Thread.CurrentThread.CurrentCulture = new CultureInfo("en-US");
          break;
        case "cn":
          CultureInfo.CurrentCulture = new CultureInfo("zh-CN");
          CultureInfo.CurrentUICulture = new CultureInfo("zh-CN");
          //Thread.CurrentThread.CurrentUICulture = new CultureInfo("zh-CN");
          //Thread.CurrentThread.CurrentCulture = new CultureInfo("zh-CN");
          break;
        case "tw":
          CultureInfo.CurrentCulture = new CultureInfo("zh-TW");
          CultureInfo.CurrentUICulture = new CultureInfo("zh-TW");
          //Thread.CurrentThread.CurrentUICulture = new CultureInfo("zh-TW");
          //Thread.CurrentThread.CurrentCulture = new CultureInfo("zh-TW");
          break;
      }
    }
  }

HtmlHelper 代码实现语言的输出



/// <summary>
    /// 多语言切换
    /// </summary>
    /// <param name="helper"></param>
    /// <param name="name"></param>
    /// <returns></returns>
    public static HtmlString L(this HtmlHelper helper, string name) {

      var resource = new System.Resources.ResourceManager(typeof(WebApp.resource.Global));
      var text = resource.GetString(name);
      return new HtmlString(text ?? name);
    }
    /// <summary>
    /// 前端easyui或是其它js相关的比如提示信息也需要转换必须在这里加载不同的语言文件
    /// </summary>
    public static HtmlString LangScriptTag(this HtmlHelper helper,string defaultsrc) {
      var src = defaultsrc;
      var lang = CultureInfo.CurrentCulture.Name;
      switch (lang)
      {
        case "en-US":
          src = "/Scripts/easyui/locale/easyui-lang-en.js";
          break;
        case "zh-CN":
          src = "/Scripts/easyui/locale/easyui-lang-zh_CN.js";
          break;
        case "zh-TW":
          src = "/Scripts/easyui/locale/easyui-lang-zh_TW.js";
          break;
        default:
          src = defaultsrc;
          break;
      }
      return new HtmlString($"<script src=\"{ src }\"></script>");
    }

//html代码
<div class="btn-group btn-group-sm">
<button onclick="append()" class="btn btn-default"> <i class="fa fa-plus"></i> @Html.L("Add") </button>
</div>
<div class="btn-group btn-group-sm">
<button onclick="removeit()" class="btn btn-default"> <i class="fa fa-trash-o"></i> @Html.L("Delete") </button>
</div>
<div class="btn-group btn-group-sm">
<button onclick="accept()" class="btn btn-default"> <i class="fa fa-floppy-o"></i> @Html.L("Save") </button>
</div>

 



https://www.cnblogs.com/neozhu/p/10285874.html
**************************************************
Android 实现卡片翻转的动画（翻牌动画）

Android 实现卡片翻转的动画（翻牌动画）
需求描述
　　点击卡片，卡片翻转过来显示内容。　　点击左边的卡片，将卡片翻转显示右边的图片结果。
功能实现
　　因为要翻转所以使用动画来完成翻转的动画。动画分为两部分，一部分是左边的布局以中心垂直线从左向右旋转，旋转 180 度之后隐藏，另一部分是右边的布局以中心垂直线从右向左旋转，旋转 180 度之后显示。　　这种动画涉及到播放顺序的问题，所以动画使用 Animator 属性动画实现。 布局 
    <FrameLayout
        android:id="@+id/activity_main_result_layout_fl"
        android:layout_width="@dimen/activity_main_result_width"
        android:layout_height="@dimen/activity_main_result_height"
        android:layout_gravity="center"
        android:layout_marginTop="@dimen/activity_main_result_margin_top">

        <ImageView
            android:id="@+id/activity_main_result_iv"
            android:layout_width="match_parent"
            android:layout_height="match_parent"
            android:src="@mipmap/result" />

        <TextView
            android:id="@+id/activity_main_result_tv"
            android:layout_width="match_parent"
            android:layout_height="match_parent"
            android:gravity="center"
            android:textColor="@android:color/white"
            android:textSize="@dimen/activity_main_text_size"
            android:visibility="gone" />

    </FrameLayout>

 动画文件 


rotate_in_anim .xml
<?xml version="1.0" encoding="utf-8"?>
<set xmlns:android="http://schemas.android.com/apk/res/android">
  <!--消失-->
  <objectAnimator
      android:duration="0"
      android:propertyName="alpha"
      android:valueFrom="1.0"
      android:valueTo="0.0" />

  <!--旋转-->
  <objectAnimator
      android:duration="1000"
      android:propertyName="rotationY"
      android:valueFrom="-180"
      android:valueTo="0" />

  <!--出现-->
  <objectAnimator
      android:duration="0"
      android:propertyName="alpha"
      android:startOffset="500"
      android:valueFrom="0.0"
      android:valueTo="1.0" />
</set>



rotate_out_anim.xml
<?xml version="1.0" encoding="utf-8"?>
<set xmlns:android="http://schemas.android.com/apk/res/android">
  <!--旋转-->
  <objectAnimator
      android:duration="1000"
      android:propertyName="rotationY"
      android:valueFrom="0"
      android:valueTo="180" />

  <!--消失-->
  <objectAnimator
      android:duration="0"
      android:propertyName="alpha"
      android:startOffset="500"
      android:valueFrom="1.0"
      android:valueTo="0.0" />
</set>



 播放动画 
    tv_result.setVisibility(View.VISIBLE);
    tv_sure.setVisibility(View.VISIBLE);
    AnimatorSet inAnimator = (AnimatorSet) AnimatorInflater.loadAnimator(this, R.animator.rotate_in_anim);
    AnimatorSet outAnimator = (AnimatorSet) AnimatorInflater.loadAnimator(this, R.animator.rotate_out_anim);
    int distance = 16000;
    float scale = getResources().getDisplayMetrics().density * distance;
    iv_result.setCameraDistance(scale); //设置镜头距离
    tv_result.setCameraDistance(scale); //设置镜头距离
    outAnimator.setTarget(iv_result);
    inAnimator.setTarget(tv_result);
    outAnimator.start();
    inAnimator.start();
    outAnimator.addListener(new AnimatorListenerAdapter() {
        @Override
        public void onAnimationEnd(Animator animation) {
            super.onAnimationEnd(animation);
            iv_result.setVisibility(View.GONE);
            iv_result.setAlpha(1.0f);
            iv_result.setRotationY(0.0f);
        }
    });

　　注意：动画的实现方式是使用了属性动画 Animator 实现的，如果动画需要再次显示，那么在动画结束之后就需要将控件的属性值设置为初始值，因为属性动画会修改控件的属性值为动画结束时的属性值。
参考文章
http://lishuaishuai.iteye.com/blog/2297056

https://www.cnblogs.com/zhangmiao14/p/10285862.html
**************************************************
分布式系统关注点——「无状态」详解

如果这是第二次看到我的文章，欢迎右侧扫码订阅我哟~  👉
本文长度为2728字，建议阅读8分钟。
坚持原创，每一篇都是用心之作～

 
 
前面聊完的2个章节「数据一致性」和「高可用」其实本质是一个通过提升复杂度让整体更完善的方式。
 
接下去我们开始聊一些让系统更简单，更容易维护的东西——「易伸缩」，首当其冲的第一篇文章就是「stateless」，也叫「无状态」。
 
z哥带你先来认识一下「状态」是什么。
 
 
一、初识「状态」
之前在「负载均衡」的第四篇（分布式系统关注点——做了「负载均衡」就可以随便加机器了吗？）中提到过一个例子，我们再翻出来一下。
 

开发Z哥对运维Y弟喊：“Y弟，现在系统好卡，刚上了一波活动，赶紧帮我加几台机器上去顶一下。”
 
Y弟回复说：“没问题，分分钟搞定”。
 
然后就发现数据库的压力迅速上升，DBA就吼了：“Z哥，你丫的搞什么呢？数据库要被你弄垮了”。
 
然后客服那边接框也爆炸了，越来越多的用户说刚登陆后没多久，操作着就退出了，接着登陆，又退出了，到底还做不做生意了。

 
这个案例中的问题，产生的根本原因是因为系统中存在着大量「有状态」的业务处理过程。
 
 
二、「有状态」和「无状态」
 N.Wirth曾经在它1984年出版的书中将程序的定义经典的概括为：程序=数据结构+算法。（这个概括也是这本书的书名）
 
这是一个很有意思的启发，受它的影响，z哥认为程序做的事情本质就是“数据的移动和组合”，以此来达到我们所期望的结果。而如何移动、如何组合是由“算法”来定的，所以z哥延伸出一个新的定义：数据+算法=成果。
 
通过程序处理所得到的“成果”其实和你平时生活中完成的任何事情所得到的“成果”是一样的。任何一个“成果”都是你通过一系列的“行动”将最开始的“原料”进行加工、转化，最终得到你所期望的“成果”。
 

 
比如，你将常温的水，通过“倒入水壶”、“通电加热”等工作后变成了100度的水，就是这样一个过程。
 
正如烧水的例子，大多数时候得到一个“成果”往往需要好几道“行动”才能完成。

这个时候如果想降低这几道“行动”总的成本（如：时间）该怎么办呢？
自然就是提炼出反复要做的事情，让其只做一次。而这个事情在程序中，就是将一部分“数据”放到一个「暂存区」（一般就是本地内存），以提供给相关的“行动”共用。 

但是如此一来，就导致了需要增加一道关系，以表示每一个“行动”与哪一个「暂存区」关联。因为在程序里，“行动”可能是「多线程」的。
 
这时，这个“行动”就变成「有状态」的了。

 

题外话：共用同一个「暂存区」的多个“行动”所处的环境经常被称作「上下文」。

 
 
我们再来深入聊聊「有状态」。
 
「暂存区」里存的是「数据」，所以可以理解为“有数据”就等价于“有状态”。
 
「数据」在程序中的作用范围分为「局部」和「全局」（对应局部变量和全局变量），因此「状态」其实也可以分为两种，一种是局部的「会话状态」，一种是全局的「资源状态」。
 

题外话：因为有些服务端不单单负责运算，还会提供其自身范围内的「数据」出去，这些「数据」属于服务端完整的一部分，被称作「资源」。所以，理论上「资源」可以被每个「会话」来使用，因此是全局的状态。

 
本文聊的「有状态」都指的是「会话状态」。
 
 
与「有状态」相反的是「无状态」，「无状态」意味着每次“加工”的所需的“原料”全部由外界提供，服务端内部不做任何的「暂存区」。并且请求可以提交到服务端的任意副本节点上，处理结果都是完全一样的。
 
有一类方法天生是「无状态」，就是负责表达移动和组合的“算法”。因为它的本质就是：


接收“原料”（入参）


“加工”并返回“成果”（出参）


 
 
为什么网上主流的观点都在说要将方法多做成「无状态」的呢？
 
因为我们更习惯于编写「有状态」的代码，但是「有状态」不利于系统的易伸缩性和可维护性。
 
在分布式系统中，「有状态」意味着一个用户的请求必须被提交到保存有其相关状态信息的服务器上，否则这些请求可能无法被理解，导致服务器端无法对用户请求进行自由调度（例如双11的时候临时加再多的机器都没用）。
 
同时也导致了容错性不好，倘若保有用户信息的服务器宕机，那么该用户最近的所有交互操作将无法被透明地移送至备用服务器上，除非该服务器时刻与主服务器同步全部用户的状态信息。
 
这两个问题在负载均衡的第四篇（分布式系统关注点——做了「负载均衡」就可以随便加机器了吗？）中也有提到。
 
 
但是如果想获得更好的伸缩性，就需要尽量将「有状态」的处理机制改造成「无状态」的处理机制。
 
 
三、「无状态」化处理
将「有状态」的处理过程改造成「无状态」的，思路比较简单，内容不多。
 
首先，状态信息前置，丰富入参，将处理需要的数据尽可能都通过上游的客户端放到入参中传过来。

当然，这个方案的弊端也很明显：网络数据包的大小会更大一些。
 
 
另外，客户端与服务端的交互中如果涉及到多次交互，则需要来回传递后续服务端处理中所需的数据，以避免需要在服务端暂存。

▲橙色请求，绿色响应
 
这些改造的目的都是为了尽量少出现类似下面的代码。
 

func(){
    return i++;
}

 
而是变成：
 

func(i){
    return i+1;
}

 
 
要更好的做好这个「无状态」化的工作，依赖于你在架构设计或者项目设计中的合理分层。
 
尽量将会话状态相关的处理上浮到最前面的层，因为只有最前面的层才与系统使用者接触，如此一来，其它的下层就可以将「无状态」作为一个普遍性的标准去做。
 
与此同时，由于会话状态集中在最前面的层，所以哪怕真的状态丢失了，重建状态的成本相对也小很多。
 
比如三层架构的话，保证BLL和DAL都不要有状态，代码的可维护性大大提高。
 
如果是分布式系统的话，保证那些被服务化的程序都不要有状态。除了能提高可维护性，也大大有利于做灰度发布、A/B测试。
 

题外话：在这里，提到做分层的目的是为了说明，只有将IO密集型程序和CPU密集型程序分离，才是通往「无状态」真正的出路。一旦分离后，CPU密集型的程序自然就是「无状态」了。
 
如此也能更好的做「弹性扩容」。因为常见的需要「弹性扩容」的场景一般指的就是CPU负荷过大的时候。

 
 
最后，如果前面的都不合适，可以将共享存储作为降级预案来运用，如远程缓存、数据库等。然后当状态丢失的时候可以从这些共享存储中恢复。
 
所以，最理想的状态存放点。要么在最前端，要么在最底层的存储层。

 
 
四、总结
任何事物都是有两面性的，正如前面提到的，我们并不是要所有的业务处理都改造成「无状态」，而只是挑其中的一部分。最终还是看“价值”，看“性价比”。
 
比如，将一个以“状态”为核心的即时聊天工具的所有处理过程都改造成「无状态」的，就有点得不偿失了。
 
 

 
相关文章：


分布式系统关注点——初识「高可用」


分布式系统关注点——仅需这一篇，吃透「负载均衡」妥妥的


分布式系统关注点——「负载均衡」到底该如何实施？


分布式系统关注点——做了「负载均衡」就可以随便加机器了吗？这三招来帮你！


分布式系统关注点——99%的人都能看懂的「熔断」以及最佳实践


分布式系统关注点——想通关「限流」？只要这一篇


分布式系统关注点——让你的系统“坚挺不倒”的最后一个大招——「降级」


分布式系统关注点——99%的人都能看懂的「补偿」以及最佳实践


 

 
作者：Zachary
出处：https://www.cnblogs.com/Zachary-Fan/p/stateless.html
 
如果你喜欢这篇文章，可以点一下右下角的「推荐」。
这样可以给我一点反馈。: )
谢谢你的举手之劳。
 

▶关于作者：张帆（Zachary，个人微信号：Zachary-ZF）。坚持用心打磨每一篇高质量原创。欢迎扫描右侧的二维码~。
定期发表原创内容：架构设计丨分布式系统丨产品丨运营丨一些思考。








 
https://www.cnblogs.com/Zachary-Fan/p/stateless.html
**************************************************
jdk1.8之线程中断
在Core Java中有这样一句话："没有任何语言方面的需求要求一个被中断的程序应该终止。中断一个线程只是为了引起该线程的注意，被中断线程可以决定如何应对中断 "
线程中断不会使线程立即退出，而是给线程发送一个通知，告知目标线程有人希望你退出。至于目标线程接收到通知后如何处理，则完全由目标线程自行决定。
线程中断有关的三个方法

void Thread.interrupt();//中断线程
boolean Thread.isInterrupted()//判断是否中断
static boolean Thread.interrupted()//判断是否中断，并清除当前中断状态
Thread.interrupt()方法是一个实例方法，它通知目标线程中断，也就是设置中断标志位。中断标志位表示当前线程已经被中断了。
Thread.isInterrupted()方法也是实例方法，它判断当前线程是否有被中断（通过检查中断标志位）。

静态方法Thread.interrupted()也是用来判断当前线程的中断状态，但同时会清除当前线程的中断标志位状态。
运行中的线程不会因为interrupt()而中断，因为它仅仅是一个信号（status）
    public static void main(String[] intsmaze) throws InterruptedException {
        Thread t1=new Thread()
        {
            public void run()
            {
                while(true){ }
            }
        };     
        t1.start();
        Thread.sleep(2000);
        t1.interrupt();
    }
这个程序虽然对t1进程了中断，但是在t1中并没有中断处理的逻辑，因此即使t1线程被置上了中断状态，但是这个中断不会发生任何作用。
如果希望t1在中断后退出，必须为他增加相应的中断处理代码，如下
    public static void main(String[] intsmaze) throws InterruptedException {
        Thread t1=new Thread()
        {
            public void run()
            {
                while(true)
                {
                    if(Thread.currentThread().isInterrupted())//判断当前线程是否中断。
                    {
                        System.out.println("intsmaze Interrupt");
                        break;
                    }
                }
            }
        };
        t1.start();
        Thread.sleep(2000);
        t1.interrupt();
    }
等待中的线程（wait(long),sleep(long),join(long)收到中断信号会抛出InterruptedException
public static native void sleep(long millis) throws InterruptedException;会抛出一个中断异常。当线程在休眠sleep时，如果被中断就会产生该异常，此时它会清楚中断标志，如果不加处理，那么在下一次循环开始时，就无法捕获这个中断。
如果注释掉catch中的Thread.currentThread().interrupt();我们可以发现，程序一直运行，线程没有停止；反之放开该注释，则发现程序运行结束了。
    public static void main(String[] intsmaze) throws InterruptedException {
        Thread t1=new Thread()
        {
            public void run()
            {
                while(true)
                {
                    if(Thread.currentThread().isInterrupted())
                    {
                        System.out.println("intsmaze Interrupt");
                        break;
                    }               
                    try {
                        Thread.sleep(6000);
                    } catch (InterruptedException e) {
                        System.out.println("Interrupt when intsmaze sleep");
                        Thread.currentThread().interrupt();//设置中断状态
                    }
                }
            }
        };
        
        t1.start();
        Thread.sleep(2000);
        t1.interrupt();
    }
BLOCKED
如果线程在等待锁，对线程对象调用interrupt()只是会设置线程的中断标志位，线程依然会处于BLOCKED状态，也就是说，interrupt()并不能使一个在等待锁的线程真正”中断”。通过前面的代码可以看到，中断是通过循环不判进行Thread.currentThread().isInterrupted()判断的。
在使用synchronized关键字获取锁的过程中不响应中断请求，这是synchronized的局限性。如果想在等待获取锁的过程中能响应中断，应该使用显式锁，Lock接口，它支持以响应中断的方式获取锁。
NEW/TERMINATE
如果线程尚未启动(NEW)，或者已经结束(TERMINATED)，则调用interrupt()对它没有任何效果，中断标志位也不会被设置。比如说，以下代码的输出都是false。
IO操作
如果线程在等待IO操作，尤其是网络IO，则会有一些特殊的处理。
如果IO通道是可中断的，即实现了InterruptibleChannel接口，则线程的中断标志位会被设置，同时，线程会收到异常ClosedByInterruptException。
如果线程阻塞于Selector调用，则线程的中断标志位会被设置，同时，阻塞的调用会立即返回。
我们重点介绍另一种情况，InputStream的read调用，该操作是不可中断的，如果流中没有数据，read会阻塞 (但线程状态依然是RUNNABLE)，且不响应interrupt()，与synchronized类似，调用interrupt()只会设置线程的中断标志，而不会真正”中断”它，我们看段代码。
public class InterruptReadDemo {
    private static class A extends Thread {
        @Override
        public void run() {
            while(!Thread.currentThread().isInterrupted()){
                try {
                    System.out.println(System.in.read());
                } catch (IOException e) {
                    e.printStackTrace();
                }    
            }
            System.out.println("exit");
        }
    }
    public static void main(String[] args) throws InterruptedException {
        A t = new A();
        t.start();
        Thread.sleep(100);
        t.interrupt();
    }
}
线程t启动后调用System.in.read()从标准输入读入一个字符，不要输入任何字符，我们会看到，调用interrupt()不会中断read()，线程会一直运行。
拿两年前的笔记出来冒个泡

https://www.cnblogs.com/intsmaze/p/10285850.html
**************************************************
视频编解码基础概念
错误难免，逐渐完善。
1. 概述
音视频领域由早期使用的模拟化技术逐渐发展为现在的数字化技术。数字化的主要好处有：可靠性高、能够消除传输及存储损耗，便于计算机处理及网络传输等。
数字化后，音视频处理就进入了计算机技术领域，音视频处理本质上就是对计算机数据的处理。
图像信息经采集后生成的原始视频数据，数据量非常大，对于某些采集后直接本地播放的应用场合，不需要考虑压缩技术。
但现实中更多的应用场合，涉及视频的传输与存储，传输网络与存储设备无法容忍原始视频数据的巨大数据量，必须将原始视频数据经过编码压缩后，再进行传输与存储。
本文仅考虑视频压缩技术，不考虑音频。
2. 视频压缩原理
2.1 熵与冗余
引自参考资料[1]

在所有的实际节目素材中，存在着两种类型的信号分量：即异常的、不可预见的信号分量和可以预见的信号分量。
异常分量称为熵，它是信号中的真正信息。其余部分称为冗余，因为它不是必需的信息。
冗余可以是空间性的，如在图象的大片区域中，邻近象素几乎具有相同的数值。冗余也可以是时间性的， 例如连续图象之间的相似部分。
在所有的压缩系统编码器中都是将熵与冗余相分离，只有熵被编码和传输，而在解码器中再从编码器的发送的信号中计算出冗余。

2.2 帧内编码
帧内编码是空间域编码，利用图像空间性冗余度进行图像压缩，处理的是一幅独立的图像，不会跨越多幅图像。
空间域压缩依赖于一幅图像中相邻像素间的相似性和图案区的主要空间域频率。
JPEG标准用于静止图像(即图片)，只使用了空间域压缩，只使用帧内编码。
2.3 帧间编码
帧间编码是时间域编码，是利用一组连续图像间的时间性冗余度进行图像压缩。如果某帧图像可被解码器使用，那么解码器只须利用两帧图像的差异即可得到下一帧图像。
比如运动平缓的几帧图像的相似性大，差异性小，而运动剧烈的几幅图像则相似性小，差异性大。当得到一帧完整的图像信息后，可以利用与后一帧图像的差异值推算得到后一帧图像，这样就实现了数据量的压缩。
时间域编码依赖于连续图像帧间的相似性，尽可能利用已接收处理的图像信息来“预测”生成当前图像。
MPEG标准用于运动图像(即视频)，会使用空间域编码和时间域编码，因此是帧内编码和帧间编码结合使用。
2.4 运动矢量
一组连续图像记录了目标的运动。运动矢量用于衡量两帧图像间目标的运动程度，运动矢量由水平位移量和垂直位移量二者构成。
2.5 运动补偿
目标的运动降低了图像间的相似性，增加了差异数据量。而运动补偿则通过运行矢量来降低图像间的差异数据量。
下图为运动补偿的示意图。当某一目标运动时，其位置会变化但形状颜色等基本不变。编码器则可利用运动矢量减低图像差值，解码器根据图像差值中的运动适量移动目标到合适的位置即可。
假设图中是理想情况，目标除移动位置外其他任何属性无任何变化，则两幅图像间的差值仅包含运动矢量这一数据量。显然运动补偿可以显著减少图像差值数据量。

2.6 双向预测
先看示意图：

连续的三幅图像中，目标块有垂直位置上的移动，背景块无位置移动。我们考虑如何取得当前帧图像(画面N)。
画面N中，目标向上移动后，露出背景块。
画面N-1中，因为背景块被目标块遮挡住了，因此没有背景块相关信息。
画面N+1中，完整包含背景块的数据，因此画面N可以从画面N-1中取得背景块。
如何可以得到画面N呢？解码器可以先解码得到画面N-1和画面N+1，通过画面N-1中的目标块数据结合运动矢量即可得到画面N中的目标块数据，通过画面N+1中的背景块数据则可得到画面N中的背景块数据。
三幅画面的解码顺序为：N-1, N+1, N。三幅画面的显示顺序为：N-1, N, N+1。画面N通过其前一幅画面N-1和后一幅画面N+1推算(预测，predicted)得到，因此这种方式称为双向预测(或前面预测、双向参考)。
2.7 I帧，IDR帧，P帧，B帧
I帧
I帧(Intra-coded picture, 帧内编码帧，常称为关键帧)包含一幅完整的图像信息，属于帧内编码图像，不含运动矢量，在解码时不需要参考其他帧图像。
因此在I帧图像处可以切换频道，而不会导致图像丢失或无法解码。I帧图像用于阻止误差的累积和扩散。
在闭合式GOP中，每个GOP的第一个帧一定是I帧，且当前GOP的数据不会参考前后GOP的数据。
IDR帧
IDR帧(Instantaneous Decoding Refresh picture, 即时解码刷新帧)是一种特殊的I帧。当解码器解码到IDR帧时，
会将DPB(Decoded Picture Buffer，指前后向参考帧列表)清空，将已解码的数据全部输出或抛弃，然后开始一次
全新的解码序列。IDR帧之后的图像不会参考IDR帧之前的图像。
P帧
P帧(Predictive-coded picture, 预测编码图像帧)是帧间编码帧，利用之前的I帧或P帧进行预测编码。
B帧
B帧(Bi-directionally predicted picture, 双向预测编码图像帧)是帧间编码帧，利用之前和(或)之后的I帧或P帧进行双向预测编码。
B帧不可以作为参考帧。
2.8 GOP
GOP(Group Of Pictures, 图像组)是一组连续的图像，由一个I帧和多个B/P帧组成，是编解码器存取的基本单位。
GOP结构常用的两个参数M和N，M指定GOP中首个P帧和I帧之间的距离，N指定一个GOP的大小。
例如M=1，N=15，GOP结构为：IPBBPBBPBBPBBPB
GOP有两种：闭合式GOP和开放式GOP
闭合式GOP
闭合式GOP只需要参考本GOP内的图像即可，不需参考前后GOP的数据。这种模式决定了，闭合式GOP的显示顺序总是以I帧开始以P帧结束
开放式GOP
开放式GOP中的B帧解码时可能要用到其前一个GOP或后一个GOP的某些帧。码流里面包含B帧的时候才会出现开放式GOP。
开放式GOP和闭合式GOP中I帧、P帧、B帧的依赖关系如下图所示：

2.9 DTS和PTS
DTS(Decoding Time Stamp, 解码时间戳)，表示packet的解码时间。
PTS(Presentation Time Stamp, 显示时间戳)，表示packet解码后数据的显示时间。
音频中DTS和PTS是相同的。视频中由于B帧需要双向预测，B帧依赖于其前和其后的帧，因此含B帧的视频解码顺序与显示顺序不同，即DTS与PTS不同。当然，不含B帧的视频，其DTS和PTS是相同的。
下图以一个开放式GOP示意图为例，说明视频流的解码顺序和显示顺序

采集顺序指图像传感器采集原始信号得到图像帧的顺序。
编码顺序指编码器编码后图像帧的顺序。存储到磁盘的本地视频文件中图像帧的顺序与编码顺序相同。
传输顺序指编码后的流在网络中传输过程中图像帧的顺序。
解码顺序指解码器解码图像帧的顺序。
显示顺序指图像帧在显示器上显示的顺序。
采集顺序与显示顺序相同。编码顺序、传输顺序和解码顺序相同。
图中“B[1]”帧依赖于“I[0]”帧和“P[3]”帧，因此“P[3]”帧必须比“B[1]”帧先解码。这就导致了解码顺序和显示顺序的不一致，后显示的帧需要先解码。
3. 参考资料
[1] 泰克Tektronic, MPEG基础和协议分析指南
[2] 视频直播的理论知识，https://www.jianshu.com/p/04b5b1e4ff27
[3] open GOP & close GOP, https://www.jianshu.com/p/d30c051b4106
[4] I帧/B帧/P帧/GOP, https://blog.csdn.net/abcsunl/article/details/68190136
[5] FFmpeg音视频同步原理与实现, https://www.jianshu.com/p/3578e794f6b5
[6] FFmpeg 音视频同步, https://www.jianshu.com/p/27279255f67e
4. 修改记录
2018-12-08 V1.0 初稿

https://www.cnblogs.com/leisure_chn/p/10285829.html
**************************************************
“别更新了，学不动了” 之：全栈开发者 2019 应该学些什么？
 

转载请注明出处：葡萄城官网，葡萄城为开发者提供专业的开发工具、解决方案和服务，赋能开发者。
原文转载自 公众号 infoqchina

 
对于什么是全栈开发者并没有一个明确的定义。但是，有一件事是肯定的：2019 年对全栈开发者的需求量很大。在本文中，我将向你概述一些趋势，你可以尝试根据这些趋势来确定你可能要投入的时间。
简单地说，全栈开发者就是可以构建完整应用程序的人。他们了解前端和后端技术、工具和服务，并结合所有这些技能开发出可以在生产环境中运行的东西。
这是美国全栈开发者在 2019 年的工资走势：

人生苦短，所以尽量少做无用功。如果你希望保持最新状态并成为全栈开发者，以下是你需要了解并考虑列入学习计划的 2019 年技术趋势。
 
前端
基础
HTML、CSS 和 JavaScript 是必须掌握的，你还需要学习 React、Vue 或 Angular 等前端框架或库。但是，你应该选择哪一个？对于一个真正的全栈开发者，你可以在 2019 年选择这三个框架中的任何一个。
来自React 16 的更新
你需要了解 React 的基础知识及其基于单向数据流架构的组件。今年我们看到了 React 16 的大量更新和 2019 年即将发布的一些小版本更新。
新的生命周期方法；




React 16.6 中的 Suspense for Code Splitting（已发布）；


带有 React Hooks 的 16.x 版本（2019 年第一季度）；


带有并发模式的 16.x 版本（2019 年第二季度）；


带有 Suspense for Data Fetching 的 16.x 版本（2019 年中）。




这意味着你需要知道如何使用 React.lazy() 和 < React.Suspense> 进行代码拆分，使用 React.memo 进行优化，并时刻关注新功能，如 React Hooks，它可能会给 React 生态系统带来重大改变。
我们现在还有标准化的 React Context API，你应该对它有一个基本的了解。
React 生态系统将在 2019 年继续发展和演化。它这不仅限于 Web，在移动、物联网和 AR/VR 等不同平台上移植和使用 React 的能力将使其变得越来越重要，并在 2019 年领先于其他 2 个库。
Vue 3.0
2018 年，Vue 持续获得开发者的青睐，2019 年将会继续增长……但它是否足以超越其他两大玩家？我们拭目以待。
Vue 生态系统正在不断发展，而且，随着 Vue 3.0 的发布极其改进的 Vue CLI，2019 年的开发者体验将比以往更好。
开发者可以使用 Vue Native 进行跨平台开发（就像 React Native 那样），我们已经很接近 React 那样的大型生态系统，但还是有一大段距离。
Vue 有一个非常有趣的趋势，它将在 2019 年继续增长：阿里巴巴、百度、腾讯、小米和 DJI 等中国科技巨头更喜欢 Vue。预计中国市场将继续保持快速增长，因为 Vue 是一个独立的开源库，与西方的大型科技巨头无关。
Angular Ivy 和 Angular Elements
新的渲染引擎 Ivy 即将推出，性能将会得到大幅提升。
Ivy 将成为 Angular 渲染引擎的第三个化身，它的目标是成为更小、更快、更简单的编译器。
Angular Elements 将使我们能够在 Angular 以外的其他环境中使用 Angular 组件。简单地说就是你可以构建可以被添加到不使用 Angular 的 HTML 页面中的组件，有点像 Web 组件。现在，我可以使用 Angular 创建世界上最好的组件，并将它交给我的朋友，她将它用在她的 React 应用程序中！
2019 年，Angular 将继续做他们擅长的事情：提供一个功能齐全的框架，用于构建丰富的 Web 应用程序。
Angular、Vue、React——更小更快
总的来说，2019 年将看到这 3 个前端库的发展。如前所述，你只要掌握其中一个，就已经为进入新的一年做好了准备。预计在 2019 年，这些库都会发生微小的变化，提高渲染速度并缩小库的体积……但它们都不会带来任何重大改进来压倒其他库。
CLI 将会风靡
你必须使用 babel、webpack、eslint、测试库和其他工具搭建项目脚手架的日子已经一去不复返了。我的意思是，我们仍然可以这么做，但 CLI 确实让这种体验变得更好了。




Angular CLI；


Create React App 2；


Vue CLI。




2019 年，我们将在 CLI 中看到越来越多的改进体验。
状态管理




Vue 将继续使用 Vuex 进行状态管理。


Angular 将继续主要使用 RxJS。


随着新的 Context API 的问世和 GraphQL + Apollo 的普及，React 今年则遭遇了一点危机。很长一段时间以来，Redux 第一次被认为不是状态管理的明智选择。你仍然需要学习 Redux，因为你可以从 Redux 中学到一些有用的计算机科学原理，如事件溯源和 CQRS。




新的 Context API、Redux 和 GraphQL
Apollo 内置的离线客户端缓存将使 Apollo + GraphQL 在 2019 年成为 Redux 的一个重要替代品（当然，从技术上讲，可以同时使用它们）。新的 Context API 问世了，很多人称它为 Redux 终结者。
2019 年，你需要了解它们三者，了解它们的工作原理，以及它们可以用来解决哪些问题。但如果从就业方面来看，学习 Redux 仍然是一个很好的选择。
服务器端渲染
服务器端渲染在 JavaScript 领域仍然是一个待解决的问题。我们知道，单页应用程序和客户端渲染很容易让项目出现代码膨胀，而且需要向客户端发送太多的 JavaScript 代码，而且可能会影响你的 SEO（但可能没有你想象的那么多）。
有一些方法可以解决这个问题，例如：PRPL 模式、prerender.io，或者你可以这么想，其实谷歌机器人在抓取单页应用程序时没有那么糟糕。
目前，如果要进行服务器端渲染，可以使用：




用于 React 的 Next.js；


用于 Vue 的 Nuxt.js；


用于 Angular 的 Angular Universal。




静态页面正在重新刮起一阵流行风，你可以看看 JAM Stack：
https://www.netlify.com/blog/2017/06/06/jamstack-vs-isomorphic-server-side-rendering/
它的主要思想是：预构建标记（静态页面），通过利用服务器的 API 在客户端成为动态单页面应用程序。这将在 2019 年真正改变服务器端渲染，我预测会有更多人使用像 GatsbyJS 这样的工具，而不是自己构建复杂的服务器端渲染逻辑。
 
Web组件
浏览器采用的 Web 组件终于离我们想要的标准越来越近了。2019 年，我们将看到更多关于 Web 组件的讨论，但它仍然不会在 2019 年达到临界点。你可以密切地关注它们，但不需要花费大量时间在掌握如何构建 Web 组件上。
你可以了解 React、Angular、Vue 和普通 HTML 的组件，但很难说 Web 组件会在什么时候得到大规模采用并为我们带来主要的好处。
性能
每个人都喜欢谈论性能。2019 年，代码拆分可能会成为标准实践，更多新的优化图像格式（如 WebP）将会发挥越来越重要的作用。
人们仍然会讨厌谷歌的 AMP。
你应该学习并为 2019 年做好准备的是：




针对 Angular、React、Vue 的特定优化；


代码拆分；


Tree Shanking；


只传输必要的 JavaScript 代码；


更加关注你正在使用的 NPM 库，并最大限度地减少库的大小；


制定性能预算；


通过使用 CDN 和浏览器优先级工具更好地确定资源优先级。




PWA
渐进式 Web 应用程序在 2019 年仍然会很热门，但它最复杂的功能可能不会流行起来（即推送通知）。
大多数情况下，你将使用 HTTPS、App Shell 和 Service Worker 来获得一些额外的脱机功能、安全性和性能。你应该学习如何构建 PWA，并使用像 Lighthouse 这样的工具来测试它。
Safari 最终为 PWA 添加了一些支持，实现渐进式 Web 应用程序功能可能会更容易一些。但说到底，你需要先学会使用 manifest.json 文件和 Service Worker。谷歌正在这方面努力推进，但不要指望在 2019 年会看到任何突破。
 
后端
别担心！2019 年的后端世界并不会像前端世界那样疯狂。
HTTPS 无处不在
需要将用户输入的数据发送到服务器的网站必须使用 HTTPS。如果你没有使用 HTTPS，谷歌将会惩罚你。幸运的是，HTTPS Everywhere 或 Gaddy 让迁移到 HTTPS 变得更容易。
REST 与 GraphQL
RESTful API 在 2019 年还会存在，你需要学习如何实现和设计这些 API。你应该学会使用 Node.js 和 Express.js 来创建 API 服务器，在 2019 年，这两个框架的组合仍然会占主导地位。
现在出现了很多有关 GraphQL 的炒作，但它还不是可以赢得所有市场的大赢家。了解 GraphQL 可以解决哪些问题，以及如何在 RESTful API 中用它来进行路由优化。这将是 2019 年最重要的趋势：不是如何单独使用 GraphQL，而是如何在极少数情况下使用 GraphQL 优化一些 RESTful API 路由。
HTTP2
HTTP2 变得越来越普遍，你需要知道如何使用这个协议来优化内容的传输。此外，HTTP3 正在开发当中，你可以关注它，但它并不是你在 2019 年需要过分关注的东西。
基础设施即服务
需要自己构建和管理服务器的场景越来越少，以下是 2019 年的主要选择。




Digital Ocean——用于简单的服务器。


Heroku——用于简单和集成的服务器和部署。


Now——用于超级简单的部署。


Firebase——用于托管基础设施和数据库。


AWS——几乎任何你想要的东西，你可以永远不需要考虑自己管理服务器。




你需要学习 SQL

Firebase、AWS 等托管数据库将继续增长，但你还是需要学习 SQL。2019 年，像 PostgreSQL 这样的数据库将继续发展，而像 MongoDB 这样的 NoSQL 数据库似乎会有所下降。你可能需要了解每种方案的优点和缺点，因为在数据库领域并没有可以解决所有问题的完美解决方案。
不要把搜索给忘了
搜索可能不是绝对必要的，但它是 Web 的重要组成部分。2019 年，全栈开发者可以试着了解下面两个平台：




Elasticsearch；


Algolia Search；




你可能需要学习 Redis
了解使用 Redis 作为缓存以及内存存储的工作原理。缓存和内存存储是 2019 年需要学习的重要概念，可以用它们来优化你的系统。Redis 是理解这些概念的一个很好的起点。
 
测试 
学习三种测试类型
很多人都在讨论这个话题，但为了简单问题，可以将测试分解为三种类型：




单元测试：给定输入，测试输出，用于测试单个函数或类。


集成测试：测试流程或组件是否按预期运行（包括副作用）。


端到端测试：测试用户的实际行为，不仅仅是测试一个简单的功能。




保持简单
测试框架有很多选择，但下面是 2019 年最好的两个组合：




Jest（https://jestjs.io/）


Mocha + Chai + Sinon + Istanbul




将 Jest 视为一体化的测试框架，就不需要像第二个选项那样添加其他工具和库。如果你想要简单些，只需使用 Jest。如果你想要更多可定制性和模块化，请选择 Mocha。
如果你还了解这些，那是锦上添花：Mock、Spy、存根和快照测试。
适当的端到端测试就可以了
进行端到端测试需要公司投入大量的成本，所以在你的职业生涯中有可能会也有可能不会遇到这种测试。但不管怎样，在 2019 年，你最好可以学习这些框架，或至少可以了解一下：




Cypress；


Nightwatch；


Protractor，适合 Angular 爱好者。




 
移动开发
跟移动开发说再见？

移动开发在 2019 年的日子可能会有点难过。应用程序的下载量不像过去那么多，而且最热门的下载要么是游戏，要么是大型科技公司的应用程序。2019 年，移动端 Web 浏览量将超过原生移动应用程序。因此，对于全栈开发者和移动开发者而言，他们应该将更多的关注点放在移动设备 Web 应用程序上（例如使用 PWA）。
iOS 和 Android 仍然是企业所需要的重要开发技能，但在过去几年中对它们的需求一直在下降，似乎出现了从原生移动开发到 React Native 引领的混合开发（或接近原生）的重大转变。如果你看一下上面的图表，React Native 已经取代了 Swift，它是原生 iOS 开发的主要编程语言。
以下是你需要关注的开发技术：
React Native 在 2018 年遭遇了一些挫折，一些大公司在博文说他们正在放弃它。但这些公司都曾经尝试将 React Native 添加到他们现有的 iOS 或 Android 代码库中。如果你是这方面的新手，对于你来说，它仍然是 2019 年的一个很好的选择。它将会继续增长下去。
Flutter 在 2018 年非常火爆，但现在判断它在 2019 年将会怎样发展还为时过早。你需要关注它，但到目前为止，它并没有带来比 React Native 更显著的优势。
Ionic 和 NativeScript 的使用将在 2019 年逐渐减少，除非你正在使用 Angular，否则你不应该关注它们。
所以，在 2019 年，请继续关注 React Native。
 
工具
你应该使用的 NPM 包




Prettier——让你可以专注于你正在写的代码，而不是去关心代码的格式；
https://prettier.io/


eslint——保持代码整洁；
https://eslint.org/


date-fns——moment.js 的轻量级替代品；
https://date-fns.org/


lodash——主要用于 throttle() 和 debounce() 函数；
https://lodash.com/


rambda——如果你真的喜欢函数式编程。
https://ramdajs.com/




 
JavaScript
JavaScript 是饱受争议的编程语言之一。2018 年，静态类型在 JavaScript 的动态类型领域变得越来越受欢迎。那么竞争者有哪些？




TypeScript：可以编译为 JavaScript 的 JavaScript 超集。


Flow：JavaScript 的静态类型检查器。


Reason：利用了 JavaScript 和 OCaml 生态系统的类型语言。


PureScript：一种强类型语言，可以编译为 JavaScript，使用 Haskell 开发。


Elm：纯粹的函数式编程语言，可以编译成 JavaScript。




关于静态与动态类型语言的讨论由来已久，不会很快就得出结论。以上这些都不会取代 JavaScript 作为 Web 主要编程语言的主导地位。但是，Angular 和 Vue 都采用了 TypeScript，并将其作为开发者社区的标准，因此，TypeScript 可能会继续增长，并超越上述其他语言。
你需要学习 TypeScript 的基础知识及其原理（以及静态类型的好处），但要注意，它并非写出好代码的唯一方法。要写出好代码，可以先关注如何写出好的单元测试。
模块捆绑器
Webpack 4 和 Parcel 是 2019 年的主要工具。它们都朝着降低复杂性和更多“为用户着想”的方向发展，很多前端库都提供了 CLI。学习这两个工具，但请记住，CLI 在项目开始时帮你消除掉最初 80％的复杂性。如果要发布 NPM 包，请使用 Rollup。
 
计算机科学基础
数据结构 + 算法
讨论技术趋势的文章很少会提到计算机科学基础知识。但这可能是最重要的主题，而且我可以非常自信地说，这个趋势具有 99.99999％的准确率：如果你想在 2019 年和未来几年成为一个全栈开发者，计算机科学基础是非常重要的。计算机科学基本原理不怎么会发生变化，并且已经存在了很长时间，不会像开发库那样，一旦有新东西出来就变得过时了。
容器和 serverless
容器为我们提供了与几年前完全不同的架构，其中的一个主要的想法是 serverless。serverless 并不是说不需要服务器了，而是说有人为你管理服务器（基础设施），你可以专注于自己的应用程序逻辑，无需担心扩展性等问题。
serverless 的流行始于 2017 年，并持续到了 2018 年。2019 年，我们将看到一些相同的常见用例，比如 AWS API Gateway 与 AWS Lambda 的结合，供前端应用程序代码调用。
在降低成本的同时提高性能是一个好主意，如果冷启动问题在 2019 年可以得到解决，那么它将变得越来越流行。
平台即服务 / 后端即服务
亚马逊、谷歌和 Azure 将在 2019 年争夺服务器市场，它们当中的每一个都提供了全托管的服务。
AppSync、Amplify、App Services、App Engine 等服务将继续发展，但由于程序员很难放弃如此多的控制权（除非是小型的个人项目），所以它们并不会真正有大起色。
2019 年，Azure 将主导企业市场，AWS 将主导一般的开发者市场，而谷歌将主导机器学习市场。
机器学习
2019 年，你需要学习并了解如何通过以下 API 使用机器学习模型：




Google Cloud AI；


亚马逊机器学习；


Azure 机器学习；




除了之前列出的平台即服务和后端即服务，还会有更多的服务出现，这些大公司提供的机器学习 API 和模型将在 2019 年成为一个更重要的趋势。你应该学会在未来的项目中使用其中一些（不用担心，它们使用起来没有那么难，就像使用大多数其他 API 一样）。
2019 年，我们将可以看到机器学习 API 在 Web 上的应用，而不是从头开始构建自己的机器学习模型。因为与上述大型科技巨头不同，大多数人或公司无法为机器学习提供足够的资源或数据。
WebAssembly
WebAssembly 集将继续缓慢改进，但仍然只有一小部分开发者会使用它（主要用于游戏、图像处理）。你可以先了解它，在几年后等它成为主流时你就是这方面的专家了。
以上是我的个人意见，不管怎样，学习新东西绝不是一个坏主意。
但不论技术风向如何变化，葡萄城一直都秉承着为开发者着想，赋能开发者的企业理念，同时为开发者提供技术领先、功能可靠的一站式开发工具、解决方案及技术服务。
 
https://www.cnblogs.com/powertoolsteam/p/10283365.html
**************************************************
SpringBoot 2.x（九）：遇到跨域不再慌
什么是跨域
首先，我们需要了解一下一个URL是怎么组成的：

// 协议 + 域名（子域名 + 主域名） + 端口号 + 资源地址
http: + // + www.baidu.com + :8080/

只要协议，子域名，主域名，端口号这四项组成部分中有一项不同，就可以认为是不同的域，不同的域之间互相访问资源，就被称之为跨域。
随着前后端分离开发的越来越普及，会经常遇到跨域的问题，当我们在浏览器中看到这样的错误时，就需要意识到遇到了跨域：

解决跨域的几种方案
首先，我们使用vue-cli来快速构建一个前端项目，然后使用axios来向后台发送ajax请求。然后在控制台中打印出返回信息。这里就不再多做赘述，后面我会单独写一篇文章来讲一下如何使用vue-cli快速创建一个vue项目。

这里不再讲解使用jsonp的方式来解决跨域，因为jsonp方式只能通过get请求方式来传递参数，而且有一些不便之处。

下面的几种方式都是通过跨域访问技术CORS(Cross-Origin Resource Sharing）来解决的。具体的实现原理我们不做深入的探究，这节课的目的是解决跨域问题~
方法一：注解
在Spring Boot 中给我们提供了一个注解@CrossOrigin来实现跨域，这个注解可以实现方法级别的细粒度的跨域控制。我们可以在类或者方添加该注解，如果在类上添加该注解，该类下的所有接口都可以通过跨域访问，如果在方法上添加注解，那么仅仅只限于加注解的方法可以访问。
@RestController
@RequestMapping("/user")
@CrossOrigin
public class UserController {
   @Autowired
    private UserService userService;

   @RequestMapping("/findAll")
    public Object findAll(){
        return userService.list();
    }
}
现在再去测试一下：

Bingo，成功！
方法二：实现WebMvcConfigurer
这里可以通过实现WebMvcConfigurer接口中的addCorsMappings()方法来实现跨域。
@Configuration
class CORSConfiguration implements WebMvcConfigurer {
    @Override
    public void addCorsMappings(CorsRegistry registry) {
        registry.addMapping("/**");
    }
}
下面我们将刚刚加上的注解给注释掉，看看能不能访问到这个接口：

不出我们所料，果然还是可以的~
方法三：Filter
我们可以通过实现Fiter接口在请求中添加一些Header来解决跨域的问题：
@Component
public class CORSFilter implements Filter {

    @Override
    public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain)
            throws IOException, ServletException {
        HttpServletResponse res = (HttpServletResponse) response;
        res.addHeader("Access-Control-Allow-Credentials", "true");
        res.addHeader("Access-Control-Allow-Origin", "*");
        res.addHeader("Access-Control-Allow-Methods", "GET, POST, DELETE, PUT");
        res.addHeader("Access-Control-Allow-Headers", "Content-Type,X-CAF-Authorization-Token,sessionToken,X-TOKEN");
        if (((HttpServletRequest) request).getMethod().equals("OPTIONS")) {
            response.getWriter().println("ok");
            return;
        }
        chain.doFilter(request, response);
    }
    @Override
    public void destroy() {
    }
    @Override
    public void init(FilterConfig filterConfig) throws ServletException {
    }
}
不出意外的话，应该也可以在控制台获取到返回信息。
Nginx配置解决跨域问题
如果我们在项目中使用了Nginx，可以在Nginx中添加以下的配置来解决跨域（原理其实和Filter类似，只不过把活儿丢给了运维🤔）
location / {
   add_header Access-Control-Allow-Origin *;
   add_header Access-Control-Allow-Headers X-Requested-With;
   add_header Access-Control-Allow-Methods GET,POST,PUT,DELETE,OPTIONS;

   if ($request_method = 'OPTIONS') {
     return 204;
   }
}
原创文章，才疏学浅，如有不对之处，万望告知！
公众号

您的推荐就是对我最大的支持！

https://www.cnblogs.com/viyoung/p/10285786.html
**************************************************
Java String：重要到别人只能当老二的字符串类

字符串，是Java中最重要的类。这句肯定的推断不是Java之父詹姆斯·高斯林说的，而是沉默王二说的，因此你不必怀疑它的准确性。
关于字符串，有很多的面试题，但我总觉得理论知识绕来绕去没多大意思。你比如说：String cmower = new String("沉默王二");定义了几个对象?
我总觉得问我这样的问题，就好像是在拷问我：“既然你家买了冰箱，你难道不应该知道冰箱制冷的原理？”
再说，为什么要用String cmower = new String("沉默王二");而不是String cmower = "沉默王二";？
我劝各位面试官不要再缠住这样的问题不放了，切记“学以致用”。理论知识如果一直是在绕弯弯，那真的毫无价值。如果要我来做面试官，我想要问的问题是：“你平常是怎么判断两个字符串相等的？是用equals()还是==？”
前言就说这么多。接下来，我们来探讨几个实用的知识点。
01 字符串是不可变的
我们来看一下String类的定义：
public final class String    implements java.io.Serializable, Comparable<String>, CharSequence {}
可以发现，String类是final类型的，因此不能被继承。
如果类可以被继承，那么就会破坏类的不可变性机制。因为子类可以覆盖父类的方法，并且可以改变父类的成员变量值，一旦子类以父类的形式出现时，就不能保证类是不可变的。
String类的不可变性有什么好处呢？
1）作为HashMap的键。
因为字符串是不可变的，因此它在创建的时候哈希码（hash code）就计算好了。这也就意味着每次在使用一个字符串的哈希码的时候不用重新计算一次，这样更加高效，很适合作为HashMap中的键。
2）线程安全。
同一个字符串对象可以被多个线程共享，如果访问频繁的话，可以省略同步和锁等待的时间，从而提升性能。
3）字符串常量池的需要。
稍后介绍。
特别要注意的是，String类的所有方法都没有改变字符串本身的值，都是返回了一个新的对象。
02 字符串常量池
在Java中，常用的创建字符串的方式有两种：
String cmower = "沉默王二";String cmowsan = new String("沉默王三");
cmower使用双引号，cmowsan使用new关键字，它们有什么区别呢？
答案如下：
String cmower = "沉默王二";String cmower1 = "沉默王二";System.out.println(cmower == cmower1); // 输出trueString cmowsan = new String("沉默王三");String cmowsan1 = new String("沉默王三");System.out.println(cmowsan == cmowsan1); // 输出false
双引号创建的相同字符串使用==判断时结果为true，而new关键字创建的相同字符串使用==判断时结果为false。
这是为什么呢？
String在Java中使用过于频繁，为了避免在系统中产生大量的String对象，Java的设计者引入了“字符串常量池”的概念。
当使用双引号创建一个字符串时，首先会检查字符串常量池中是否有相同的字符串对象，如果有，则直接从常量池中取出对象引用；如果没有，则新建字符串对象，并将其放入字符串常量池中，并返回对象引用。
这也就是说，"沉默王二"是放在字符串常量池中的，cmower和cmower1两个字符串对象引用是相同的。
而new关键字创建的字符串对象是不涉及字符串常量池的，直接放在堆中，也就是说，虽然cmowsan和cmowsan1都叫沉默王三，但不一个人。
强烈建议：不要使用new关键字的形式创建字符串对象。
03 +号和StringBuilder
由于字符串是不可变的，因此字符串在进行拼接的时候会创建新的字符串对象。大家都知道，内存是一定的，因此对象创建多了就会影响系统性能。
StringBuilder正是为了解决字符串拼接产生太多中间对象的问题而提供的一个类，可以通过append()方法把字符串添加到已有序列的末尾，非常高效。
那么有人在进行字符串拼接的时候，就会产生疑惑：“我到底是用+号还是StringBuilder？”
我们先来看这样一段代码：
String chenmo = "沉默";String wanger = "王二";System.out.println(chenmo + wanger);
这段代码是怎么编译的呢？可以使用JAD（Java反编译工具）来看一看。
String s = "\u5A0C\u5910\u7CAF";String s1 = "\u941C\u5B29\u7C29";System.out.println((new StringBuilder()).append(s).append(s1).toString());
你是不是看到了StringBuilder的影子？
没错，使用+号进行字符串拼接的时候，Java编译器实际是通过StringBuilder类来完成的。
难道可以使用+号来随意拼接字符串？反正Java编译器已经自动地为我们优化了。
但事实并非如此，来看这样一段代码：
String cmowers = "";for (int i = 0; i < 9; i++) {    cmowers += "沉默王二";}System.out.println(cmowers);
闭上眼睛先想一想，Java编译器会怎么做？我们期望的结果是在循环外部就创建StringBuilder，Java编译器能如我们所愿吗？
JAD反编译后的结果如下：
String s = "";for(int i = 0; i < 10; i++)    s = (new StringBuilder()).append(s).append("\u5A0C\u5910\u7CAF\u941C\u5B29\u7C29").toString();System.out.println(s);
这么看来，StringBuilder是在for循环内部创建的，也就是说会创建10次。天呐，这可不是我们期望的结果！我们只希望StringBuilder创建一次。
没办法，Java编译器是做不到的，只能靠我们自己：
StringBuilder cmowers = new StringBuilder();for (int i = 0; i < 9; i++) {    cmowers.append("沉默王二");}System.out.println(cmowers);
强烈建议：如果只是三四个字符串的拼接，尽管使用+号操作符，别想什么性能优化（举个例子，你离目的地只有100米，你是打算打个出租车，还是自己步行走过去？）；如果遇到多于四个字符串的拼接，或者需要用到循环来拼接，那就选择StringBuilder。
在我年轻的时候，我还会犯这样一个错误：
StringBuilder cmowers = new StringBuilder();for (int i = 0; i < 9; i++) {    cmowers.append("沉默王二" + "和他的读者朋友们");}System.out.println(cmowers);
我去，竟然在append()方法的内部使用+号！因为这个错误，我差点没被领导打死。你可要小心点。
04 关于concat()
除了使用+号和StringBuilder对字符串进行拼接，还可以使用String类的concat()方法。
concat()方法只不过是String类的一个方法而已，为什么我要单独拎出来说呢？
因为之前我要在JSP页面的EL表达式中拼接字符串，刚开始想到的是用+号操作符，但EL表达式不是Java，+号操作符是不能拼接字符串的。我当时竟然没想起来用concat()！
重新铭记一下：
${item.username.concat('-').concat(item.realname)}
05 关于intern()
关于字符串的性能问题，我常在一些技术文章中看到这样的建议：“如果一个字符串使用的频率非常高，建议使用String.intern()将其缓存。”
但我并不建议你这么做，因为这个方法要显式的调用，这样很麻烦；况且，在代码编写阶段，怎么可能知道哪个字符串使用频率很高呢？
06 关于StringUtils
据我的编程经验来看，字符串的操作往往需要用到一个工具类，那就是org.apache.commons.lang3.StringUtils（null安全的，也就是说，StringUtils类的方法可以接受为null的字符串，但不会抛出NullPointerException）。
不过，我最常用的方法就那么几个：


方法等价



IsEmpty(String str)
str == null || str.length == 0


isBlank(String str)
str == null || str.length == 0 || str.trim().length == 0


join(Object[] arrey)
把数组中的元素连接成一个字符串返回



 
 
推荐阅读：
Java异常处理：给程序罩一层保险Java集合类：我其实没那么简单
https://www.cnblogs.com/qing-gee/p/10277688.html
**************************************************
并发concurrent---1
背景：并发知识是一个程序员段位升级的体现，同样也是进入BAT的必经之路，有必要把并发知识重新梳理一遍。
 
说到并发concurrent，肯定首先想到了线程，创建线程有两种方法：1、从Java.lang.Thread类派生一个新的线程类，重载它的run()方法；2、实现Runnalbe接口，重载Runnalbe接口中的run()方法；建议使用方法二创建线程，因为，如果是通过扩展 Thread类的方法来创建线程，那么这个自定义类就不能再去扩展其他的类，也就无法实现更加复杂的功能；而实现Runnable接口的方法来定义该类为线程类，这样就可以避免Java单继承所带来的局限性，也更符合面向对象编程的思想，最重要的就是使用实现Runnable接口的方式创建的线程可以处理同一资源，从而实现资源的共享。
创建线程的两种方法：

 1 package www.concurent.test;
 2 public class TraditionalThread {
 3 
 4     public static void main(String[] args) {
 5         //Thread1:
 6         Thread thread = new Thread() {
 7             @Override
 8             public void run() {
 9                 while(true) {
10                     try {
11                         Thread.sleep(1000);
12                     } catch (InterruptedException e) {
13                         e.printStackTrace();
14                     }
15                     System.out.println("thread1: "+Thread.currentThread().getName());
16                 }
17             }
18         }; 
19         thread.start();
20         
21         //Thread2:
22         //Runnable变量是线程要运行的代码的宿主，更适合面向对象思想的线程方法
23         Thread thread2 = new Thread(new Runnable() {
24             @Override
25             public void run() {
26                 while(true) {
27                     try {
28                         Thread.sleep(1000);
29                     } catch (InterruptedException e) {
30                         e.printStackTrace();
31                     }
32                     System.out.println("thread2: "+Thread.currentThread().getName());
33                 }
34             }
35         });
36         thread2.start();
37     }
38 }

线程和Timer定时器很类似，下面介绍了两种和线程相似的定时器写法：1、定时一天之后调用方法查询天气情况接口，然后每隔60秒后继续调用该方法；2、定时每天00:39:32调用查询天气情况接口，通过Hutool工具和Timer定时器调用HTTP天气状况接口的返回结果如下截图：(result2得到了天津的天气状况)

通过Hutool工具和Timer定时器调用HTTP天气状况接口：

 1 import java.util.Calendar;
 2 import java.util.Date;
 3 import java.util.Timer;
 4 import java.util.TimerTask;
 5 import cn.hutool.http.HttpUtil;
 6 
 7 public class TraditionalTimerTest {
 8     //时间间隔
 9      private static final long PERIOD_DAY = 24 * 60 * 60 * 1000;
10     //Timer 定时器
11     public static void main(String[] args) {
12         Calendar cl = Calendar.getInstance();
13         cl.set(Calendar.HOUR_OF_DAY, 0);
14         cl.set(Calendar.MINUTE, 39);
15         cl.set(Calendar.SECOND, 32);
16         Date date = cl.getTime();
17         Date dateNow = new Date();
18         //如果第一次执行定时任务的时间 小于 当前的时间
19         //此时要在 第一次执行定时任务的时间 加一天，以便此任务在下个时间点执行。如果不加一天，任务会立即执行。
20         if (date.before(dateNow)) {
21             Calendar clAdd = Calendar.getInstance();
22             clAdd.setTime(dateNow);
23             clAdd.add(Calendar.DAY_OF_MONTH, 1);
24              date = clAdd.getTime();
25         }
26         //Timer1:
27         new Timer().schedule(new TimerTask() {
28             @Override
29             public void run() {
30                 System.out.println("hello");
31                 //Hutool调用http接口
32                 String result1 = HttpUtil.get("http://t.weather.sojson.com/api/weather/city/101030100");
33                 System.out.println("result1: "+result1);
34             }
35             //一天之后调用方法查询天气情况接口，然后每隔60秒后继续调用该方法
36         },PERIOD_DAY , 1000*60);
37         //Timer2:
38         new Timer().schedule(new TimerTask() {
39             @Override
40             public void run() {
41                 //Hutool调用http接口
42                 String result2 = HttpUtil.get("http://t.weather.sojson.com/api/weather/city/101030100");
43                 System.out.println("result2: " + result2);
44             }
45             //定时每天00:39:32调用查询天气情况接口
46         }, date , PERIOD_DAY);
47     }
48 
49 }

 如果是单个线程调用都还ok，要是有多个线程同时调用那就会出现并发产生；比如有一个方法Output()是经过charAt(i)获取字符串i的字符并且打印再控制台，然后线程A和线程B同时调用Output()方法，此时就会出现线程不安全问题(如银行取钱和转账同时进行)，也就是并发；执行结果发现，线程A为执行完毕线程B就开始执行了，为了能后保证当有一个线程来执行某个方法时，其他的线程不能进来执行该方法，实现排他性，可以通过synchronized和ReentrantLock来实现线程同步；二者其实区别不大，synchronized由于是底层JVM实现的互斥，因此效率会高一些，而ReentrantLock的功能则比synchronized更多，比如定时获取某个锁，多个等待条件等，另外synchronized 会让线程阻塞，ReentrantLock会让线程等待，但是从行为效果上来看是一样的；下面有个例子：并发结果如截图显示，理想状态是打印“huawei”或者“isoftstone”，但是由于并发打印出来诸如此类“ishuaweoftstoni”结果。

FYI:

 1 import java.util.concurrent.locks.Lock;
 2 import java.util.concurrent.locks.ReentrantLock;
 3 
 4 public class MyThreadSynchronized {
 5     public static void main(String[] args) {
 6         //要想调用内部类的对象，必须有外部类的实例对象
 7         new MyThreadSynchronized().init();   // 外部类的实例对象
 8     }
 9     public void init() {
10         final Outputer outputer = new Outputer();
11         //thread1:
12         new Thread(new Runnable() {
13             @Override
14             public void run() {
15                 while(!false) {
16                     try {
17                         Thread.sleep(100);
18                     } catch (InterruptedException e) {
19                         e.printStackTrace();
20                     }
21                     outputer.output3("huawei");
22                 }
23             }
24         }).start();
25         
26         //thread2:
27         new Thread(new Runnable() {
28             @Override
29             public void run() {
30                 while(!false) {
31                     try {
32                         Thread.sleep(100);
33                     } catch (InterruptedException e) {
34                         e.printStackTrace();
35                     }
36                     outputer.output("isoftstone");
37                 }
38             }
39         }).start();
40     }
41     //当有一个线程来执行某个方法时，其他的线程不能进来执行该方法，排他性、独一无二；
42     //使用synchronized/lock同步，且线程用的同步锁是同一个同步对象，可用this互斥或方法.class
43     //synchronized由于是底层JVM实现的互斥，因此效率会高一些
44     //ReentrantLock的功能则比synchronized更多，比如定时获取某个锁，多个等待条件
45     //synchronized 会让线程阻塞，ReentrantLock会让线程等待，但是从行为效果上来看是一样的；
46 class Outputer{
47     //内部类  静态方法中不能new内部类的实例对象
48         //synchronized:
49         public synchronized void output(String name) {
50             for(int i = 0; i<name.length(); i++) {
51                 System.out.print(name.charAt(i));
52             }
53             System.out.println();// switch line
54         }
55         
56         //线程不安全
57         public  void output3(String name) {
58             for(int i = 0; i<name.length(); i++) {
59                 System.out.print(name.charAt(i));
60             }
61             System.out.println();// switch line
62         }
63         
64         //lock：
65         public void ouputlock(String name) {
66             Lock lock = new ReentrantLock();
67             lock.lock();   // 上锁同步
68             try {
69                 for (int i = 0; i < name.length(); i++) {
70                     System.out.print(name.charAt(i));
71                 }
72                 System.out.println();// switch line
73             } finally {
74                 lock.unlock();  // 解锁
75             }
76         }
77         
78 }
79     
80 }

 
https://www.cnblogs.com/taojietaoge/p/10285607.html
**************************************************
MVC、MVCS、MVVM、MVP、VIPER等这么多架构模式哪一个好呢？
在项目开启阶段，其中一个很重要的环节就是选架构。
那么面对目前已知的这么多架构模式我们该怎么选择呢？这确实是个很让人头疼的问题！
 

下面我就在这里梳理一下目前常见的一些架构模式。
先逐个对它们的分析，然后在从中找到它们的规律，之后就可以以不变应万变，不会再被这些虚头巴脑的名词所迷惑。
 

本篇文章主要从两个维度进行分析：
一、任务分配方式
二、逻辑分层方式
 
先看一下MVC、MVCS、MVVM、MVP、VIPER架构模式的任务分配方式
 

MVC
MVC是最经典的架构模式，它出现的时间非常早，也是最被人所熟知的。
MVC架构的任务分工为：
 
M-model：
1.数据结构表示 
2.读取本地数据
3.写数据到本地
4.处理弱业务
 
C-Controller：
1.处理主要业务逻辑
2.处理交互事件
3.协调V-M数据流
 
V-View：
1.展示数据
2.处理非逻辑交互事件。
 
根据上面描述，总之一句话概括：
M：管理数据， C：处理数据， V：展示数据。
 

MVCS
看名字就感觉这个MVCS架构模式是从MVC中分化出来的，事实上也确实如此。
S为Store的简称，意思为“存储，保存”。
下面来看一下多出一个S后，它们的分工有什么变化呢？
S-Store：
1.负责数据的存储，数据本地持久化。
 
M-Model：
1.数据结构表示 
2.读取本地数据
3.处理弱业务
 
C-Controller：
1.处理主要业务逻辑
2.处理交互事件
3.协调V-M数据流
 
V-View：
1.展示数据
2.处理非逻辑交互事件。
 
从上面的分工可知C，V同MVC架构是完全一样的，只有M的数据存储任务被分离了出来。即：S分担了M的数据管理任务，那么M和S其实都是数据管理的逻辑范畴了。
根据上面描述，总之一句话概括：
（M+S）：管理数据， C：处理数据， V：展示数据。

 

MVVM
MVVM为了解决前端的响应式编程而生，由于前端网页混合了HTML、CSS和JavaScript，而且页面众多，代码的组织和维护难度复杂，所以通过ViewModel实现View和Model的双向绑定。
但是移动端不是前端，从业务处理逻辑上讲，移动端要比前端处理的逻辑更多，你问我有啥依据。你可以把手机的网断掉，进入带有离线功能的APP，一套业务走下来，没啥问题。但是用浏览器打开呢，纵然添加了缓存，也是不能将一套业务走完的。
所以说，移动端要比前端处理的逻辑多！
 
看到MVVM你会有疑问，为啥没有C了，是不是用这个MVVM就不需要C了呢？如果你是移动端的同学，我给你讲是有C的。
MVVM架构在移动端的完整叫法是：M-V-C-VM。
MVVM架构的任务分工为：
M-model： 
1.数据结构表示 
2.读取本地数据
3.写数据到本地
4.处理弱业务
 
C-Controller：
1.处理交互事件
2.协调V-M数据流
 
VM-ViewModel:
1.处理主要业务逻辑
 
V-View：
1.展示数据
2.处理非逻辑交互事件。
 
从上面的分工可知，VM分担了C中的数据加工任务，将业务处理放到了ViewModel中，其他的M,V同MVC架构完全一样。
总之一句话概括：
M：管理数据， （C+VM）：处理数据， V：展示数据。

 

MVP
MVP从MVC衍生而来，从名称上看只是将C换成了P。其他都一样。而事实上呢？
它们也确实这样，P承担了C的任务而已。
区别是：它们两个的数据流向不一样
 

 


MVC的数据流向图
 
 
 

MVP的数据流向图
 
对比一下，就可以一样看出了。
MVC框架中，V的数据从Model中拿
MVP框架中，V的数据从Presenter中拿。
MVP架构的任务分工为：
 
M-model： 
1.数据结构表示 
2.读取本地数据
3.写数据到本地
4.处理弱业务
 
P-Presenter：
1.处理主要业务逻辑
2.处理交互事件
3.协调V，M数据流，从M读取数据，将数据通过接口供V调用。
 
V-View：
1.展示数据
2.处理非逻辑交互事件。
 
根据上面描述，总之一句话概括：
M：管理数据， P：处理数据， V：展示数据。




 

VIPER
VIPER是责任粒度划分比较细的一个架构模式，是按照“责任单一原则”的标志来走的，每个类所承担的任务更简单。
VIPER架构的任务分工为：
E-Entity：
1.数据结构表示 
2.读取本地数据
3.写数据到本地
 
I-Interactor：
1.处理主要业务逻辑
 
P-Presenter:
1.处理弱业务
2.处理交互事件
 
R-Routing:
1.处理视图的展示顺序逻辑或者说是控制器的跳转控制
 
V-View：
1.展示数据
2.处理非逻辑交互事件。
 
根据上面描述，可粗略的概括为：
E：管理数据， （I+P+R）：处理数据， V：展示数据。




 

架构从逻辑分层上讲，常见有两种：
三层架构：展示层，业务层，数据层。
四层架构：展示层，业务层，网络层，本地数据层。
 
架构从任务分配上讲，常见有五种：
MVC、MVCS、MVVM、MVP、VIPER
 
而通常在工程中，这两个维度的思想是同时存在的。
比如：三层MVC架构，四层MVC架构。
前面的层级表示逻辑分层方式
后面的形式表示任务分配方式
 
对于上面讲的五种任务分配方式，因为是已经被人熟知，所有被大工程所采用。
 
但是目前有个疑惑
如果有时候一个业务模块很负责时，会不断的讲业务分拆。会产生很多种目录与文件。
如果模块内视图控制器跳转逻辑负责时，会引入中介者模式进行统一管理跳转。
这时，模块内的任务分配文件相对于这五种架构模式，显得有点四不像了。
这时就会疑惑，我这到底用的是什么架构模式啊？
 
通过上面五种架构责任划分的介绍，我们可以知道
无论是什么架构模式，它们的区别是：任务的分配方式不同罢了。
虽然我们在任务分配后的文件和目录四不像，但是可以满足我们的业务需求和功能扩展，这就够了。
不要被形式上所限制。
 
那么什么是好的架构模式呢？
 
个人认为比较好的架构模式为：三层MVC架构
任务分配方法是以MVC任务分配方案为基础，按照一定的原则进行个性化分配。
采用如下分配原则：
1.保留当前角色的主要功能，拆分次要功能。
2.弱业务功能放到Model中，尽量不要放到Controller里去。
3.拆分出去的业务功能尽量封装成可复用组件、对象或协议。
4.控制好拆分粒度，调用接口少参或无参。
 
这样不管形式如何变化，只有架构逻辑分层在，同时满足业务需要和功能扩展就是好架构。




 
 












https://www.cnblogs.com/zhou--fei/p/10285600.html
**************************************************
自主机器人“罗德尼”：第一部分

Phil Hopley 著
Conmajia 译
2019 年 1 月 16 日
原文发表于 CodeProject（2019 年 1 月 15 日）.
本文是 House Bot 机器人操作系统的第一部分.


全文约 6000 字，建议阅读时间 15 分钟.

 源码 61.7 KB
 3D 打印文件 70.9 KB
简介
罗德尼是我设计的一个自主家庭机器人. 这是这个项目系列的第一篇文章. 在这部分我主要阐述概念，如何选择单板机，安装 ROS（robot operating system）以及编写第一部分用到的控制软件.
背景
早在 1970、1980 年代，我买了两本书：David L. Heiserman1 的《如何打造自编程机器人》和 Tod Loofbourrow2 的《如何打造计算机控制的机器人》. 当时我打算用我的 Z80 处理器板来制作机器人，可惜这个机器人一直没能诞生. 这么多年过去了，现在有很多类似树莓派（Raspberry Pi）和 Arduino 之类的袖珍单板电脑. 这些新玩意儿让制作复杂的家庭机器人变得特别简单.
在我那两本启蒙读物中，作者给机器人起名叫罗德尼（Rodney）和迈克（Mike），顺其自然，我给我的机器人起名叫罗德尼.





图 1 勾起我兴趣的两本机器人制作书籍

我在 CodeProject 上看到过两篇关于机器人的文章，给我启发很大.
第一篇文章是《人人都造机器人！》（Let's build a robot! 3）介绍了机器人的基本概念和一些很棒的想法，不过我很怀疑靠这些概念怎么造机器人？这篇文章给我的最大启发是可以用一个小显示器当做机器人的脑袋. 最有用的还是文章里一个 Pi Robot 的博客链接，让我第一次认识了机器人操作系统（ROS）. 这实际上是机器人编程的标准，就像维基上对 ROS 的解释：

ROS（机器人操作系统）为软件开发者创建机器人程序提供各种库函数和工具. 它提供了硬件的抽象，设备驱动，库函数，可视化驱动，消息传递机制，分包管理等等内容. ROS 作为开源软件，通过 BSD 许可授权.

ROS 其实算不是一个真正的操作系统，更像是一个用于 Linux 上的中间件（middleware），尤其适合运行在 Ubuntu 上. 网上可以找到大量开源的 ROS 代码用于各种传感器，可以帮助你集中精力开发机器人程序. ROS 的维基也全是满满的干货，如果你还不熟悉 ROS，去维基看看就对了.
第二篇文章是《PiRex：远程控制的树莓派机器人》（PiRex – remote controlled Raspberry Pi based robot 4）. 尽管涵盖的内容不如第一篇那么广泛，这篇文章完整阐述了一个机器人项目. 两篇文章都用到了树莓派，还是比较便宜的.
所以，罗德尼不用再像 80 年代我计划的那样先要自制一块处理器板，直接用上了树莓派3B，带 1GB 内存，相当给力.
ROS 和树莓派
这里我将解释如何在项目中使用 ROS 和它提供的工具来测试代码. 我没打算写一篇 ROS 的教程，网上可以找打一大堆，比我能写出来的好得多. 当然我会时不时提供一些 ROS 维基上的链接作为延伸阅读. 现在嘛，我先列出一个提纲和部分 ROS 的名词，休闲读者也可以快速浏览.

这是一个分布式系统，机器人代码可以在通过网络连接的多台设备上运行
一个节点（node）负责执行某一任务
节点以功能包（package）的形式组织，作为一套文件夹或者文件的集合
可以用多种编程语言来设计节点，我用的是 C++ 和 Python
节点互相之间用称为话题（topic）的单向流通信
话题属于即时消息（message），消息是话题的数据结构
ROS 自带标准消息，也可以定义自己的消息结构
节点也可以通过服务器／客户端（server/client）的阻塞式协议，使用服务（service）进行通信
节点还可以通过动作（action）协议进行非阻塞式面向目标任务的通信
所有节点向系统中唯一的主机（master）注册，即使用分布式系统，主机也是唯一的
用 catkin 构建和编译代码
独立的节点可以用 rosrun 命令调用，也可以用启动工具在一个命令行下启用多个节点
系统包含了一个参数服务器（parameter server），节点可以在运行过程中存取参数
系统还包含检查、硬件仿真等多种工具


这里有一篇 Intel 发表的关于 ROS 的概述文章，感兴趣的读者可以参考.

既然决定了用树莓派作为处理器，ROS 作为软件系统，第一件要做的事情就是在树莓派上安装 ROS.
ROS 的下载、安装方法可以参考这个网页. 但是更轻松的方法是用我制作的预装了 ROS，可以在树莓派上运行的 Ubuntu 系统镜像. 你可以在 Ubiquity Robotics 网站上下载这个镜像. 镜像中预装的是 Kinetic 版的 ROS，内置了访问树莓派摄像头的 raspicam_node 包. 当然你可以用别的镜像，GitHub 上可以下载 Ubiquity 提交的功能包源码.
罗德尼用到的其他外设有：

7 寸触摸屏
摄像头模块

显示屏显示机器人的状态信息，web 内容，同时显示机器人的表情，算是给了它一张“脸”. 摄像头是机器人的“眼睛”，能进行人脸识别，让它先认出主人来.
这张是触摸屏的照片，树莓派和摄像头安在屏幕背面，用 3D 打印的零件组装起来. 零件的 .stl 文件我已经打包出来，点击下载.


（正面）

（反面）
图 2 罗德尼的脑袋（正面和反面）

鉴于 ROS 可以在分布式网络上运行，我把我的 Ubuntu PC 机也装上了 ROS，用来开发节点，运行 ROS 测试工具、仿真工具等等.
机器人的任务
设计项目最基本的是确定需求. 对于罗德尼，我打算指定一些想让它完成的功能. 《人人都造机器人！》里罗列了很多家庭机器人可以做的工作，比如：

帮我带个话
既然机器人可以认出家庭成员，那么让它给某人带个话就很自然了. 我可以说机器人，帮我告诉某某下午六点来车站接我. 之后，即使他电话静音了或者正在嗨音乐，机器人也会跑到他的房间，找到他提醒他.

听起来不错，也许我的项目可以从这个任务入手. 当然应稍作改进，毕竟我设计的罗德尼可以通过网络浏览器来控制并设置任务.
带个话给某人是一个大任务，可以拆分成小的设计目标，各自分别完成：

使用摄像头观察环境，搜索并识别人脸，找到后显示消息（在脸上）
面部识别和语音合成.，这样罗德尼可以真正地带个话
通过键盘和摇杆遥控机器人移动
辅助导航的激光雷达或者类似的传感器
自主移动
安排任务完成后提示

对机器人来说，一个看起来很简单的任务都能列出这么长的单子. 具体实现起来，也不是一两句话能说明白的. 接下来，我把带个话定为“任务 1”，分别设计实现上面各个目标.
任务 1，目标 1
要完成这个设计目标，需要：

用 RC 舵机控制机器人的脑袋（摄像头）平移和倾斜
读取树莓派摄像头拍摄的图片
检测识别人脸
依序控制这些动作


平移指舵机在水平方向旋转，倾斜指垂直方向旋转.

本文是第一篇文章，先关注移动机器人的脑袋. 很显然，需要两个舵机分别控制平移（pan）和倾斜（tilt）. 为了扩展，我还添加了一套额外的舵机（2 个）. 这些舵机都是通过 PWM 信号控制的，但是树莓派只有 1 个硬件 PWM 通道，所以要使用软件方式在 GPIO 口输出 PWM 信号. 另外需要注意避免跳舵，我用了一块单独的电路板进行控制.
PiBorg, the UltraBorg 上有一种模块，通过 I2C 总线把 4 路舵机和 4 路 HC-SR04 超声波传感器连到树莓派. 不过我上一个项目还剩了不少 Arduino Nano，我打算用它来做控制板.
ROS 社区有很多杰作可以应用，这样我能专心设计我的机器人程序，其他方面，尽量发扬“拿来主义”的精神. 使用 rosserial_arduino 包，可以把 Arduino 通过串口连接到 ROS，这个包的文档在这里阅读.
使用前，要在 ROS 目标机和 Arduino IDE 里安装 rosserial_arduino，如果用了自定义消息，那还要重新编译 Arduino 库. 这些操作在教程里都有讲解.
现在来编写控制各个舵机位置的 ROS 功能包. 这个包的节点将处理平移／倾斜的消息，把它转化成独立的位置消息传给 Arduino，由后者负责控制操作舵机. 第一条消息判断需要移动哪个关节，移动多少；第二条消息发送一个编号和角度值给 Arduino，启动对应的舵机. 这样划分功能后，Arduino 编程时就不必考虑调用它的 ROS，只负责操作舵机.
第一条消息我用 ROS 内置的 sensor_msgs/JointState 实现. ROS 里标准的位置单位是弧度，Arduino 接受的角度单位是度，所以在设计节点的时候还要先把弧度换算成度. JointState 里有不少字段暂时都用不上，虽然有点浪费，不过使用 ROS 内置消息的好处在于可以方便地使用它提供的各种配套工具.
第二条消息定义了待操作舵机编号和角度. 这里我用了自定义的消息，避免 Arduino 源码里不必要的资源浪费.
现在来编写用于这两条消息的功能包，以及用于 ROS 的 Arduino 源码.
控制舵机的功能包消息我命名为 servo_msgs. 创建好后，它将生成 C++ 和 Python 代码，同时重新编译 Arduino 库生成 .h 文件.
实现这个功能包的代码文件包存在 servo_msgs 文件夹下，包括了一份说明文件以及每个 ROS 功能包都需要的 CmakeList.txt 和 package.xml 文件. 这些文件的意义以及如何创建功能包，可以参考这篇教程.
msg 文件夹包含了消息的定义文件 servo_array.msg：
# index references the servo that the angle is for, e.g. 0, 1, 2 or 3
# angle is the angle to set the servo to
uint8 index
uint16 angle
除了编程语法的一点区别，这个定义文件可以理解成 C 语言的结构体. 这个消息将作为 ROS 话题发送给 Arduino，它包含了两个元素，分别指定舵机编号（index）和要设置的角度（angle）.
这样就完成了一个简单的 ROS 功能包. 接下来将实现用于平移／倾斜的 pan_tilt 功能包. 这个包的文件都在 pan_tilt 文件夹下，节点名 pan_tilt_node.
有几个子文件夹，config 包含了配置文件 config.yaml，用来设置给到参数服务器的参数. 这样就能修改系统配置而不需要重新编译代码了. 启动文件的内容如下：
# Configuration for pan/tilt devices
# In Rodney index0 is for the head and index 1 is spare
servo:
  index0:
    pan:
      servo: 0          
      joint_name: 'head_pan'
    tilt:
      servo: 1
      flip_rotation: true
      max: 0.349066 
      min: -1.39626
      joint_name: 'head_tilt'
  index1:
    pan:
      servo: 2
    tilt:
      servo: 3
index0 和 index1 分别给两个平移／倾斜设备（以下简称设备）提供参数：

servo 指定负责当前关节的舵机编号
joint_name 指定 joint_state 消息里关节的名称
max 和 min 用于限制关节移动幅度，单位是弧度
flip_rotation 下文解释

ROS 惯例是右手定则（right-hand rule），因此关节的值会围绕正轴的逆时针方向上增大. 现在在罗德尼的头部倾斜舵机安装的方式遵循左手定则，所以需要设置 flip_rotation 为真. pan_tilt_node 可以保证传给 Arduino 的舵机方向是正确的，这点不用担心.
cfg 文件夹下的 pan_tilt.cfg 文件用于动态配置服务器，这样可以随时对舵机进行微调（trim）. 这个文件实际上是 Python 脚本：
#!/usr/bin/env python
PACKAGE = "pan_tilt"

from dynamic_reconfigure.parameter_generator_catkin import *

gen = ParameterGenerator()

gen.add("index0_pan_trim",  int_t, 0, "Index 0 - Pan Trim",  0,  -45, 45)
gen.add("index0_tilt_trim", int_t, 0, "Index 0 - Tilt Trim", 0,  -45, 45)
gen.add("index1_pan_trim",  int_t, 0, "Index 1 - Pan Trim",  0,  -45, 45)
gen.add("index1_tilt_trim", int_t, 0, "Index 1 - Tilt Trim", 0,  -45, 45)

exit(gen.generate(PACKAGE, "pan_tilt_node", "PanTilt"))
关于动态配置服务器详细内容，可以看这篇文章. 在这里，我添加了 4 个参数，用来分别配置 4 个舵机，默认值都是 0，转动范围 -45\(^\circ\) 到 45\(^\circ\).
launch 文件夹包含了全部启动文件. 其中的 pan_tilt_test.launch 是测试专用的. 它实际上是个 XML 文件：
<?xml version="1.0" ?>
<launch>
  <rosparam command="load" file="$(find pan_tilt)/config/config.yaml" />
  <node pkg="pan_tilt" type="pan_tilt_node" name="pan_tilt_node" output="screen" />
  <node pkg="rosserial_python" type="serial_node.py" name="serial_node" output="screen" args="/dev/ttyUSB0" />
</launch>
关于启动文件的详细信息，可以看这篇文章. 在启动文件里，首先用 load 命令加载了配置文件：
<rosparam command="load" file="$(find pan_tilt)/config/config.yaml" />
接下来执行了 pan_tilt 功能包的 pan_tilt_node 节点，通过指定输出为 screen，让信息直接显示到运行程序的终端上.
<node pkg="pan_tilt" type="pan_tilt_node" name="pan_tilt_node" output="screen" />
最后运行 roserial 和 Arduino 进行通信. 我用的 Arduino Nano 是通过 USB 连接到 PC 的，所以指定设备名 /dev/ttyUSB0.
<node pkg="rosserial_python" type="serial_node.py" name="serial_node" output="screen" args="/dev/ttyUSB0" />
剩下的 include 和 src 文件夹里是功能包的 C++ 源码. pan_tilt_node.cpp 文件包含了 PanTiltNode 类的定义和程序的主函数.
主函数用 pan_tilt_node 初始化 ROS，生成节点实例，同时将回调函数绑定到动态配置服务器.
int main(int argc, char **argv)
{
    ros::init(argc, argv, "pan_tilt_node");    
    
    PanTiltNode *pan_tiltnode = new PanTiltNode();
    
    dynamic_reconfigure::Server<pan_tilt::PanTiltConfig> server;
    dynamic_reconfigure::Server<pan_tilt::PanTiltConfig>::CallbackType f;
      
    f = boost::bind(&PanTiltNode::reconfCallback, pan_tiltnode, _1, _2);
    server.setCallback(f);
        
    std::string node_name = ros::this_node::getName();
    ROS_INFO("%s started", node_name.c_str());
    ros::spin();
    return 0;
}
PanTiltNode 类在构造函数里加载参数服务器的参数：
// 构造函数 
PanTiltNode::PanTiltNode()
{
    double max_radians;
    double min_radians;
    int temp;

    /* 从参数服务器获取参数，如果获取失败，则使用默认值 */

    // 指定舵机功能
    n_.param("/servo/index0/pan/servo",  pan_servo_[0],  0);
    n_.param("/servo/index0/tilt/servo", tilt_servo_[0], 1);
    n_.param("/servo/index1/pan/servo",  pan_servo_[1],  2);
    n_.param("/servo/index1/tilt/servo", tilt_servo_[1], 3);

    // 检查舵机安装方式是否符合右手定则
    n_.param("/servo/index0/pan/flip_rotation", pan_flip_rotation_[0], false);
    n_.param("/servo/index0/tilt/flip_rotation", tilt_flip_rotation_[0], false);
    n_.param("/servo/index1/pan/flip_rotation", pan_flip_rotation_[1], false);
    n_.param("/servo/index1/tilt/flip_rotation", tilt_flip_rotation_[1], false);

    /* 取值范围. 为了满足右手定则，这些值可能需要进行翻转. */
    n_.param("/servo/index0/pan/max", max_radians, M_PI/2.0);
    n_.param("/servo/index0/pan/min", min_radians, -(M_PI/2.0));
    pan_max_[0] = (int)signedRadianToServoDegrees(max_radians, pan_flip_rotation_[0]);
    pan_min_[0] = (int)signedRadianToServoDegrees(min_radians, pan_flip_rotation_[0]);
    if(true == pan_flip_rotation_[0])
    {
        temp = pan_max_[0];
        pan_max_[0] = pan_min_[0];
        pan_min_[0] = temp;
    }

    n_.param("/servo/index0/tilt/max", max_radians, M_PI/2.0);
    n_.param("/servo/index0/tilt/min", min_radians, -(M_PI/2.0));
    tilt_max_[0] = (int)signedRadianToServoDegrees(max_radians, tilt_flip_rotation_[0]);
    tilt_min_[0] = (int)signedRadianToServoDegrees(min_radians, tilt_flip_rotation_[0]);
    if(true == tilt_flip_rotation_[0])
    {
        temp = tilt_max_[0];
        tilt_max_[0] = tilt_min_[0];
        tilt_min_[0] = temp;
    }

    n_.param("/servo/index1/pan/max", max_radians, M_PI/2.0);
    n_.param("/servo/index1/pan/min", min_radians, -(M_PI/2.0));
    pan_max_[1] = (int)signedRadianToServoDegrees(max_radians, pan_flip_rotation_[1]);  
    pan_min_[1] = (int)signedRadianToServoDegrees(min_radians, pan_flip_rotation_[1]);
    if(true == pan_flip_rotation_[1])
    {
        temp = pan_max_[1];
        pan_max_[1] = pan_min_[1];
        pan_min_[1] = temp;
    }

    n_.param("/servo/index1/tilt/max", max_radians, M_PI/2.0);
    n_.param("/servo/index1/tilt/min", min_radians, -(M_PI/2.0));
    tilt_max_[1] = (int)signedRadianToServoDegrees(max_radians, tilt_flip_rotation_[1]);
    tilt_min_[1] = (int)signedRadianToServoDegrees(min_radians, tilt_flip_rotation_[1]);
    if(true == tilt_flip_rotation_[1])
    {
        temp = tilt_max_[1];
        tilt_max_[1] = tilt_min_[1];
        tilt_min_[1] = temp;
    }

    // 关节名
    n_.param<std::string>("/servo/index0/pan/joint_name", pan_joint_names_[0], "reserved_pan0");
    n_.param<std::string>("/servo/index0/tilt/joint_name", tilt_joint_names_[0], "reserved_tilt0");
    n_.param<std::string>("/servo/index1/pan/joint_name", pan_joint_names_[1], "reserved_pan1");
    n_.param<std::string>("/servo/index1/tilt/joint_name", tilt_joint_names_[1], "reserved_tilt1");

    first_index0_msg_received_ = false;
    first_index1_msg_received_ = false;

    // 锁存已发布的节点
    servo_array_pub_ = n_.advertise<servo_msgs::servo_array>("/servo", 10, true);

    // 订阅话题
    joint_state_sub_ = n_.subscribe("/pan_tilt_node/joints", 10, &PanTiltNode::panTiltCB, this);
}
调用 param 的时候会从参数服务器读取，如果读取失败，就使用默认值：
n_.param("/servo/index0/pan_servo", pan_servo_[0], 0);
构造函数最后两行订阅了话题，指定发布节点的话题. 接收到指定话题时将执行对应的回调函数 panTiltCB：
// 移动关节的回调函数
void PanTiltNode::panTiltCB(const sensor_msgs::JointState& joint)
{
    bool index0 = false;
    bool index1 = false;

    /* 在消息的列表里查找关节名. 位置（旋转）值均为正弧度值，符合右手定则，
     * 需要根据舵机方向换算成角度值. 
     */
    for (unsigned int i = 0; i < joint.name.size(); i++)
    {         
        // Is it one of the pan or tilt joints
        if(pan_joint_names_[0] == joint.name[i])
        {
            // Index 0 平移
            index0_pan_ = (int)signedRadianToServoDegrees(joint.position[i], pan_flip_rotation_[0]);
            index0 = true;
        }
        else if(pan_joint_names_[1] == joint.name[i])
        {
            // Index 1 平移
            index1_pan_ = (int)signedRadianToServoDegrees(joint.position[i], pan_flip_rotation_[1]);
            index1 = true;            
        }
        else if(tilt_joint_names_[0] == joint.name[i])
        {
            // Index 0 倾斜
            index0_tilt_ = (int)signedRadianToServoDegrees(joint.position[i], tilt_flip_rotation_[0]);
            index0 = true;                        
        }
        else if (tilt_joint_names_[1] == joint.name[i])
        {
            // Index 1 倾斜
            index1_tilt_ = (int)signedRadianToServoDegrees(joint.position[i], tilt_flip_rotation_[1]);
            index1 = true;
        }
    }

    if(index0 == true)
    {
        first_index0_msg_received_ = true;
        movePanTilt(index0_pan_, index0_tilt_, index0_pan_trim_, index0_tilt_trim_, 0);        
    }

    if(index1 == true)
    {
        first_index1_msg_received_ = true; 
        movePanTilt(index1_pan_, index1_tilt_, index1_pan_trim_, index0_tilt_trim_, 1);
    }       
}
回调函数针对接收消息中的每个名字反复执行，直至找到已知的关节名. 找到名字后，回调函数调用 signedRadianToServoDegrees 函数按照 ROS 标准和方向对关节名关联的正值进行转化，并把结果送到舵机.
随后，回调函数调用 movePanTilt 函数给对应的数值里加上微调偏移，微调舵机，检查范围，然后用舵机的编号和位置发布两条消息，一条发给平移舵机，一条发给倾斜舵机.
void PanTiltNode::movePanTilt(int pan_value, int tilt_value, int pan_trim, int tilt_trim, int index)
{
    int pan;
    int tilt;
    servo_msgs::servo_array servo;

    pan = pan_trim + pan_value;
    tilt = tilt_trim + tilt_value;

    pan = checkMaxMin(pan, pan_max_[index], pan_min_[index]);
    tilt = checkMaxMin(tilt, tilt_max_[index], tilt_min_[index]);

    // 发送平移位置
    servo.index = (unsigned int)pan_servo_[index];
    servo.angle = (unsigned int)pan;
    servo_array_pub_.publish(servo);

    // 发送偏移位置
    servo.index = (unsigned int)tilt_servo_[index];
    servo.angle = (unsigned int)tilt;
    servo_array_pub_.publish(servo);    
}
这里设计了两个助手函数，第一个用来检查最大／最小值范围.
int PanTiltNode::checkMaxMin(int current_value, int max, int min)
{
    int value = current_value;

    if (value > max)
    {
        value = max;
    }

    if (value < min)
    {
        value = min;
    }

    return (value);
}
第二个助手函数用来把 ROS 标准单位和方向换算成适合舵机的数值.
// 将正弧度值换算成舵机使用的角度值. 0 弧度相当于 90 度.
double PanTiltNode::signedRadianToServoDegrees(double rad, bool flip_rotation)
{
    double retVal;
    
    if(true == flip_rotation)
    {
        retVal = ((-rad/(2.0*M_PI))*360.0)+90.0;
    }        
    else
    {
        retVal = ((rad/(2.0*M_PI))*360.0)+90.0;
    }

    return retVal;
}
动态参数服务器回调保存了微调参数，随后调用两次 movePanTilt，每个设备一次.
// 这个回调会在动态配置参数变化的时候执行
void PanTiltNode::reconfCallback(pan_tilt::PanTiltConfig &config, uint32_t level)
{
    index0_pan_trim_ = config.index0_pan_trim;
    index0_tilt_trim_ = config.index0_tilt_trim;
    index1_pan_trim_ = config.index1_pan_trim;
    index1_tilt_trim_ = config.index1_tilt_trim;

    // 只有收到位置消息才执行
    if(first_index0_msg_received_ == true)
    {
        // 用新微调值发送新消息
        movePanTilt(index0_pan_, index0_tilt_, index0_pan_trim_, index0_tilt_trim_, 0);        
    }

    if(first_index1_msg_received_ == true)
    {
        movePanTilt(index1_pan_, index1_tilt_, index1_pan_trim_, index1_tilt_trim_, 1);
    }
}
pan_tilt_node.h 文件包含了 PanTiltNode 类定义.
完成了平移／倾斜功能包后，现在来编写 Arduino 源码. 这份源码是以 rosserial 例程作为模板来写的，包含了平移／倾斜节点里用到的各元素，支持多个舵机.
setup 函数对节点进行了初始化，订阅了舵机话题. 4 台舵机分别连接到 Arduino 的 PWM 引脚 9、6、 5 和 10. loop 函数里，调用了 spinOnce，随后延迟 1 毫秒. spinOnce 实际上会执行 servo_cb 回调函数. 这个函数每次收到舵机消息时都会执行.
/*
 * 基于 rosserial 舵机例程
 * 最多可以控制 4 台舵机
 * 节点订阅舵机话题，并作为 rodney_msgs::servo_array 消息运行.
 * 消息包含两个元素，编号和角度.
 * 编号范围：0-3
 * 角度范围：0-180
 *
 * D5 -> PWM 输出口，舵机 2
 * D6 -> PWM 输出口，舵机 1
 * D9 -> PWM 输出口，舵机 0
 * D10 -> PWM 输出口，舵机 3
 */

#if (ARDUINO >= 100)
 #include <Arduino.h>
#else
 #include <WProgram.h>
#endif

#include <Servo.h> 
#include <ros.h>
#include <servo_msgs/servo_array.h>

/* 定义连接舵机的 PWM 端口 */
#define SERVO_0 9
#define SERVO_1 6
#define SERVO_2 5
#define SERVO_3 10

ros::NodeHandle  nh;

Servo servo0;
Servo servo1;
Servo servo2;
Servo servo3;

void servo_cb( const servo_msgs::servo_array& cmd_msg)
{  
  /* Which servo to drive */
  switch(cmd_msg.index)
  {
    case 0:
      nh.logdebug("Servo 0 ");
      servo0.write(cmd_msg.angle); //设置舵机 0 角度，范围 0-180
      break;

    case 1:
      nh.logdebug("Servo 1 ");
      servo1.write(cmd_msg.angle); //设置舵机 1 角度，范围 0-180
      break;

    case 2:
      nh.logdebug("Servo 2 ");
      servo2.write(cmd_msg.angle); //设置舵机 2 角度，范围 0-180
      break;

    case 3:
      nh.logdebug("Servo 3 ");
      servo3.write(cmd_msg.angle); //设置舵机 3 角度，范围 0-180
      break;
      
    default:
      nh.logdebug("No Servo");
      break;
  }  
}

ros::Subscriber<servo_msgs::servo_array> sub("servo", servo_cb);

void setup()
{
  nh.initNode();
  nh.subscribe(sub);
  
  
  servo0.attach(SERVO_0); // 关联舵机输出引脚
  servo1.attach(SERVO_1);
  servo2.attach(SERVO_2);
  servo3.attach(SERVO_3);

  // Defaults
  servo0.write(90);
  servo1.write(120); 
}

void loop(){
  nh.spinOnce();
  delay(1);
}
使用源码
上面的程序编译烧录到 Arduino 板子前，要先编译前面写的 ROS 功能包，并且重新编译用于 Arduino 的 ROS 库. 由于我用的 Linux 版 Arduino IDE，我将在两个平台上进行编译. 我决定在树莓派上运行节点，PC 机上运行测试工具. 当然，现在还没有用到专用的树莓派硬件，所以也可以在 PC 上运行节点. 阅读下面的代码时，要注意区分运行的平台（树莓派或 PC）. 这些代码都存在 rodney_ws（树莓派）和 test_ws（PC）文件夹下.
在 PC 上编译 ROS 功能包
ROS 使用的是 catkin 编译环境，首先创建工作区并初始化：
$ mkdir -p ~/test_ws/src
$ cd ~/test_ws/
$ catkin_make
把功能包文件夹 pan_tilt、servo_msgs 拷到 ~/test_ws/src 文件夹下并编译：
$ cd ~/test_ws/ 
$ catkin_make
如果以上步骤没有出错，那么编译即告成功.
编译 Arduino ROS 库
编译 ros_lib 库的命令行如下：
$ source ~/test_ws/devel/setup.bash
$ cd ~/Arduino/libraries
$ rm -rf ros_lib
$ rosrun rosserial_arduino make_libraries.py .
如果编译没有问题，~/Arduino/libraries/ros_lib/servo_msgs 文件夹下会生成 servo_array.h 头文件.
编译并烧录 Arduino
把 rodney_control 文件夹复制到 ~/Arduino/Projects 下. 运行 Arduino IDE，打开 rodney_control.ino 文件. 在工具→开发板菜单里选择 Arduino 开发板型号（我这里用的是 Arduino Nano）. 在工具→处理器菜单里选择处理器型号（ATmega328）.
用 USB 线把 Arduino Nano 连接到 PC 上，在工具→端口菜单里选择对应的端口（/dev/ttyUSB0）.
点击上传按钮，如果一切正常，源代码将编译并烧录到 Arduino 里.
Arduino 电路
制作罗德尼的时候，供电问题必须要考虑到. 在我的设计里，Arduino 从树莓派的 USB 口取电，舵机则用 4 节 5 号（AA）可充电电池供电. 图 3 是供电电路的示意图.


图 3 罗德尼供电电路图

为了测试，我在面包板上搭建了上图的电路，并且只接了头部平移／倾斜设备.


图 4 罗德尼供电测试电路

在树莓派上编译 ROS 功能包
还是用类似的命令，创建 catkin 工作区并初始化：
$ mkdir -p ~/rodney_ws/src
$ cd ~/rodney_ws/
$ catkin_make
把 pan_tilt 和 servo_msgs 文件夹复制到 ~/rodney_ws/src 然后编译：
$ cd ~/rodney_ws/ 
$ catkin_make
如果无错，则编译完成.
小提示
在 PC 和树莓派上运行 ROS 代码和工具时，可能需要在多个命令行终端上执行同样的命令. 下一节我还是会写完整的命令，不过我可以使用我写的 .bash 文件简化命令输入.

首先编辑 .bashrc：


$ cd ~/
$ nano .bashrc


在文件最后添加 source /home/ubuntu/rodney_ws/devel/setup.bash，保存退出.

PC 在运行测试代码时需要知道 ROS 主机位置（端口），所以 PC 的 .bashrc 里我添加了下面的语句：


alias rodney='source ~/test_ws/devel/setup.bash; \
export ROS_MASTER_URI=http://ubiquityrobot:11311'


一个 rodney 就可以一次运行上面两个命令，再不用敲命令敲到手软啦.

运行代码
一切准备就绪，随时可以运行代码了. 用 USB 线把 Arduino 连到树莓派，用启动文件打开节点：
$ cd ~/rodney_ws/
$ source devel/setup.bash
$ roslaunch pan_tilt pan_tilt_test.launch
如果主机节点没有运行，那么启动命令会同时启动主机节点 roscore.
终端上会显示：

参数服务器的参数列表
节点列表，包括了 pan_tilt_node 和 serial_node
主机地址
上面两个节点的启动过程
代码里的日志信息

这时就可以用 ROS 的工具来检查、测试系统了. 新开一个命令行终端（PC），输入命令：
$ cd ~/test_ws
$ source devel/setup.bash
如果节点是在同一设备上运行的，而工具程序在另一台设备上运行，那么需要指定主机的地址：
$ export ROS_MASTER_URI=http://ubiquityrobot:11311
现在可以运行图形工具了：
$ rqt_graph


图 5 ROS 图形工具

用这个工具可以看到节点的运行情况以及和 /servo 话题的连接情况. 图中可以看到 /pan_tilt_node/joints 话题.
现在在 PC 上打开一个终端，用 rostopic 发送一条消息移动设备：
$ cd ~/test_ws
$ source devel/setup.bash
$ export ROS_MASTER_URI=http://ubiquityrobot:11311
$ rostopic pub -1 /pan_tilt_node/joints sensor_msgs/JointState '{header: {seq: 0, stamp: {secs: 0, nsecs: 0},
frame_id: ""}, name: [ "head_pan","tilt_pan"], position: [0,0.349066], velocity: [], effort: []}'
最后一行命令会在 rostopic 里发布一个 /pan_tilt_node/joints 话题的实例，使用 sensor_msgs/JointState 消息类型，平移位置 0，倾斜位置 0.349066，舵机会执行相应动作.
本文里舵机收到命令后直接移动到位. 下一篇文章里，我将添加一些代码，让舵机移动得更优雅.
用 rostopic 命令要输入的东西有点多，也可以用 rqt GUI：
$ rosrun rqt_gui rqt_gui
这个命令会运行一个图形界面，可以选择消息发布者，发布消息和内容.


图 6 rpt 图形界面

在装配各零件的时候，很可能有一定的机械误差，所以平移／倾斜会偏离中点一定角度，这时候可以对它进行微调，把两个舵机设置到中心位置：
$ rostopic pub -1 /pan_tilt_node/joints sensor_msgs/JointState '{header: {seq: 0, stamp: {secs: 0, nsecs: 0}, frame_id: ""}, name: [ "head_pan","tilt_pan"], position: [0,0], velocity: [], effort: []}'
在新终端里，运行 rqt_reconfigure 命令：
$ cd ~/test_ws 
$ source devel/setup.bash 
$ export ROS_MASTER_URI=http://ubiquityrobot:11311 
$ rosrun rqt_reconfigure rqt_reconfigure
这个命令会打开类似下面的窗口，微调参数可以通过这个界面调整：


图 7 微调参数设置界面

调整到满意之后，就可以用得到的值更新 pan_tilt.cfg 配置文件里的默认值了，这样下次节点会使用这些校正过的值启动.
要关闭节点，在终端里按 Ctrl-C.
平移／倾斜设备
平移／倾斜设备使用的日本双叶舵机，一个型号是 S3003，另一个是 S3305. S3305 自带金属固定组件，如图 8 所示.


图 8 舵机和固定组件

当然我用的是 3D 打印的零件. 考虑到显示器和树莓派的重量对舵机轴向压力，我用了一个载荷组件来减轻这个问题. 这个组件相当于舵机的外骨骼，可以增强舵机承载的机械强度. 也可以通过把屏幕固定，只移动摄像头的方式代替现在的方案，不过这样看起来就不像一个机器人了. 图 9 展示了我设计的装配件效果.



图 9 自制装配件

兴趣点
这篇文章里，我实现了用树莓派运行字节编写的 ROS 节点，用 Arduino 控制舵机. 下一篇文章，我将继续研究设计目标 1，添加一个封装在 ROS 节点中的 Python 人脸识别库，然后添加一个节点来控制罗德尼脑袋的移动.
如果说现在罗德尼还只是一副躯壳，那么它即将拥有灵魂！


图 10 罗德尼认出我了！

历史

首次发表：2018/07/28
第二版：2018/07/31 修正了 package.xml 的错误
第三版：2019/01/09 改用 sensor_msgs/JointState

许可
本文以及任何相关的源代码和文件都是根据 GNU通用公共许可证（GPLv3）授权的.
关于作者

Phil Hopley，来自英国🇬🇧，高级软件工程师，已退休，爱好徒步和划船，闲暇时会做点软硬件小玩意儿.



Heiserman 也是《Apple II C语言编程》一书作者.↩
Loofbourrow 是 ViralGains 公司 CEO.↩
本文获得了 CodeProject 2016 年 7 月号 Everything Else 奖状第 2 名.↩
本文获得了 CodeProject 2018 年 3 月号 Everything Else 奖状第 2 名.↩



https://www.cnblogs.com/conmajia/p/rodney-robot-part-1.html
**************************************************
auto类型推导
引言
auto : 类型推导. 在使用c++的时候会经常使用, 就像在考虑STL时迭代器类型, 写模板的时候使用auto能少写代码, 也能帮助我们避免一些隐患的细节.
auto初始化

使用auto型别推导要求必须在定义时初始化, 毕竟需要根据对象的类型推导左值对象的型别.

auto j;     // error. 必须初始化
auto i = 0; // i 推导型别为 int
vector<int> v; 
auto vv = v.cbegin();   // vv  推导型别为 const int*

但是auto型别推导会忽略引用和顶层const, 所以要对对象加上想要的修饰.

const int ci = 0;
auto i = ci;    // i 推导型别为 int, 忽略了顶层const
int &ri = i;
auto ii = ri;   //ii 推导型别为 int, 忽略了引用

C11之前只能通过()和=对变量初始化, C++11增加了对定义的对象初始化的方法，可以使用{}对变量初始化

c11之前的初始化方法
int i(0);   // i 初始化 0
int j = 0;  // j 初始化 0
c11后的初始化方法
auto i(0); auto j = i;  // 支持c11前
auto ii{0}; // 使用 {} 进行初始化, 但是auto推导只能接受一个参数
auto jj = { 0 };    // jj 的推导型别为 initializer_list<int>型别
上面jj的推导居然不是int型别, 而是 initializer_list<int>, 这不能怪auto推导出问题, 这主要是后者的对象初始化就是使用={}, 可以说是auto推导的是最精确的型别. 不管新添的初始化方法, 找一个习惯的就行了.
auto与for
auto最常见的就是与for联用, 特别是类型特别复杂的时候. 但是auto又有多种选择, 如 : auto, auto &等, 不同的选择其效率也不一样.

auto, 即 for(auto i:range) . 这使range中的每一个元素都会产生一个副本, 所以即使修改了 i 也不会实际影响到range.
const auto, 及for(const auto i : range). 这也会是range的每一个元素产生一个副本, 但是这个副本竟不能被修改.
auto &, 即for(auto &i : range). 引用, 因为i 直接引用range里面的元素, 所以并不会产生一个副本, 但是 i 的修改也会影响range里元素的值. 通常我们需要修改range是会考虑用到.
const auto&, 即for(const auto &&i : range). i 直接引用range里面的元素, 所以并不会产生一个副本, 并且i 也不能修改. 一般初始化的是一个左值时而且是读取range里的元素时都是用const auto&而不用auto, 因为前者不会产生副本, 效率要高. 当然一般初始化的是一个左值时效率低, 但是如果是右值还是使用const auto效率高, 因为const auto &需要把 i 存储在内存中的一个位置，间接访问会更消耗时间
auto&&, 即for(auto &&i : range). 如果初始化是左值, 那么 i 就是左值引用, 如果初始化是右值, 那么 i 就是右值引用,

还有const auto &, 当然具体的选择还是看具体的情况而定.
最后, 当用auto推导多维数组的时, 保证除最内层循环外, 其他的外层循环都应该是引用类型, 否则很容易出错, 即 :
int a[10][10][10];
for (const auto &i : a)
    for(const auto &j : i)
        for(const auto k : j)
            ;
最好使用auto型别推导
1. 初始化

在定义对象的时候可能或多或少会忘记对变量进行初始化, 但当我们使用该变量的时候就会出错, 而且问题也不好找出来, 但是使用auto定义对象就要求必须初始化有时还能减少代码量, 上面我们已经分析过了.
使用auto初始化在平台上还有一点好处, 比如 :

vector<int> v;
unsigned size = v.size();   // size()返回size_t型别
auto sizet = v.size();
​ 在32的平台unsigned代表的是32位, size_t是32位, 在64的平台unsigned代表的也是23位, 但是size_t却是64位, 这样平台差异可能就会带来问题, 使用auto代替就没有这样的问题.
不过只有这几点可能不会让人心动, 下面我们还有auto的好处.
2. STL使用型别推导
还记得在前言中个说过调用STL最好使用auto推导型别, 如果你还记得map与pair 吗? 是这样 map<pair<key, type>>? 还是map<pair<const key, type>>? 答案是最后一种, 那么现在我们就来分析的使用auto推导还是显示型别比较好.
int main()
{
    std::map<string, std::function<type(type, type)>>func = {
        { "+", [](auto i, auto j)->auto {return i + j; } },
        { "-", [](auto i, auto j)->auto {return i - j; } },
        { "*", [](auto i, auto j)->auto {return i * j; } },
        { "/", [](auto i, auto j)->auto {return i / j; } }
    };

    for (const auto &i : func) ;

    for(const std::pair<string, std::function<type(type, type)>> &pa : func) ;

    system("pause");
    exit(EXIT_SUCCESS);
}
看到上面的例子毫无问题, 但是深究起来显示型别还是些不完美. 我们知道map的key不能被改变, 所以显示型别的string与map的const string不是匹配, 编译器就会将map对象都会产生一个临时对象再隐式的转为string, 等等. 是不是注意到有一点了, 为了型别匹配赋值会产生临时变量, 那岂不是每一循环都会产生一个临时变量, 但是auto型别推导就是精确匹配的, 不会产生临时变量.
可能觉得将显示型别的key改为const string就能解决这个问题了, 确实是这样, 但是如果没有注意到这一点细节, 那就会损失效率了, 使用auto可以完全不想这些问题啊.
当然使用显示型别还是型推导看实际也看个人, 不是必要.
auto与函数返回类型
auto不能被声明为返回值，auto不能作为形参，auto不能被修饰为模板参数. 那么这里auto还能怎么和函数关联起来? 能.
auto放在函数名前面告诉编译器，真正的返回值在函数声明之后. 简单说auto可以作为返回值占位符来使返回值后置.
就像这样来写.
auto Return(std::size_t N) -> std::size_t
{
    return N;
}
既然c++规定可以这样写肯定有其意义. 其实这个写法主要用于template中, 当返回值的类型是一个模板类型时使用, 而返回值类型通过decltype来推导.
这里就解释一下decltype的简单运用. , decltype也是类似与auto的关键字, 都能够进行参数类型推导, 但是decltype必须要接受一个参数, 如下:
int i = 1;
decltype(i) j = 1;
auto与模板函数连用时用模板参数作为返回值. 因为编译器并不能直接推断出返回值为类型参数的实际类型, 所以在STL中采用traits编程解决这个问题, 这里时另一种实现方法.
首先看一个错误的例子:
template<class T1, class T2, class T3>
    T3 fun(T1 t1, T2 t2) {...}
T3的类型要在函数返回的时候才能知道, 而函数这样写就必须要编译期间就要知道返回值类型. 所以编译器会报错.
以下这样写就是正确的, 但是必须保证编译器能推导出类型.
template<class T1, class T2, class T3>
    T1 fun(T1 t1, T3 t3) {...}
使用auto将返回值类型放在最后, 就是告诉编译器真正的返回值在编译后动态获取, 而auto在这里的作用也称为返回值占位
template<class T1, class T2>
    auto fun(T1 t1, T2, t2) -> decltype(*t1) {...}
以上可以将返回类型放在函数尾做尾置是C11中的要求, 但是C14已经可以将返回型别放推导在函数头. 如 :
template<class T1>
decltype(auto)fun() {...}   // 这样的写法同上式一样
虽然规定能够这样写, 有时为了兼容也还是写成尾置.
auto与new运算符
我们可以使用auto来推断出new对象的类型, 但是局限在于, 必须对new出来的对象进行单一的初始化.
auto i = new int; // 这中写法根本没有用到auto的推导哦, 因为new的类型已经确定了

auto i = new auto(1);       // 这里就是用到了auto推导
auto size = new auto;       // error, 不能推导出size的类型
auto j = new auto(1,2);     // error, 只能接收一个初始化值
在const中我们分析到顶层const会被忽略, 所以auto是无法推断出顶层const, 即 :
auto i = new const auto(1);     // 这里auto并没有推导出顶层const, 所以i的类型实际上是int
const auto j = new const auto(1);   // 只有显示的定义j的类型是const
如果想直接推导出顶层const的话, 最好还是decltype进行推导.
注意 : auto推导只能推导出int, double等, 不能推导出short类型.
总结
本节对C11的auto用法做了一个浅显的分析, 分别对使用auto的好处, 定义时注意{}对象也必须初始化, auto在与for连用的时候要根据实际参数确定选择哪种实现, 这样效率才会达到最大, 当然一般都使用const auto&和auto&&. 最后还对auto与函数返回值关联, 可以将返回型别放在函数名尾也可以, 这样的做法一般在模板中将模板参数作为返回值才考虑用, 平时也不必这样定义函数.
参考 :
<< Effective Modern C++ >>
auto, auto&, const auto&以及其它形式的auto变种在for-range loop的选择

https://www.cnblogs.com/0xfffffff0/p/10285472.html
**************************************************
AOP 还在配置吗改用打标签模式吧！
为什么我喜欢打标签来配置AOP
1. 配置多很混乱，代码里面很难分辨出来哪些是AOP容器(比如属性注入)
2. 对于代码生成器生成的代码里面还需要手动加到配置里面
3. 连java spring现在都是清一色的注解来代替xml，这个就是趋势所在
 
我基于Autofac开发了一个基于标签来配置AOP的扩展
 
NUGET ：Install-Package Autofac.Annotation
 
开源地址：
https://github.com/yuzd/Autofac.Annotation
帮忙点个star 谢谢！
 
特色
1.打个Bean标签就能注入到AOP
2.打个Autowired标签自动装配注入
3.打个Value标签自动注入配置值(Soure标签配合使用)具体使用方法看下面的例子
4.支持拦截器
5.更多等你发现
 
如何使用
 


var builder = new ContainerBuilder();

// 注册autofac打标签模式
builder.RegisterModule(new AutofacAnnotationModule(typeof(AnotationTest).Assembly));
//如果需要开启支持循环注入
//builder.RegisterModule(new AutofacAnnotationModule(typeof(AnotationTest).Assembly).SetAllowCircularDependencies(true));
var container = builder.Build();
var serviceB = container.Resolve<B>();

 

 
AutofacAnnotationModule有两种构造方法

可以传一个Assebly列表 （这种方式会注册传入的Assebly里面打了标签的类）
可以传一个AsseblyName列表 (这种方式是先会根据AsseblyName查找Assebly 然后在注册)

 
支持的标签说明
 
Bean标签
说明：只能打在class上面 把某个类注册到autofac容器 例如：
1.无构造方法的方式 等同于 builder.RegisterType();


//把class A 注册到容器
[Bean]
public class A
{
    public string Name { get; set; }
}

 

 
2.指定Scope [需要指定AutofacScope属性 如果不指定为则默认为AutofacScope.InstancePerDependency]


 [Bean(AutofacScope = AutofacScope.SingleInstance)]
    public class A
    {
        public string Name { get; set; }
    }

 

 
3.指定类型注册 等同于 builder.RegisterType().As()


    public class B
    {

    }
    //将class A6以父类B注册到容器
    [Bean(typeof(B))]
    public class A6:B
    {

    }

 

 
4.指定名字注册 等同于 builder.RegisterType().Keyed("a4")


    [Bean("a4")]//注册A4到容器 并给他起了一个名字叫a4 假设容器有多个A4被注册就可以用这个名字来区别自动装配
    public class A4
    {
        public string School { get; set; } = "测试2";
    }

 

 
 
5.其他属性说明


InjectProperties 是否默认装配属性 【默认为true】
InjectPropertyType 属性自动装配的类型




Autowired 【默认值】代表打了Autowired标签的才会自动装配
ALL 代表会装配所有 等同于 builder.RegisterType().PropertiesAutowired()




AutoActivate 【默认为false】 如果为true代表autofac build完成后会自动创建 具体请参考 autofac官方文档
Ownership 【默认为空】 具体请参考 autofac官方文档
Interceptor 【默认为空】指定拦截器的Type
InterceptorType 拦截器类型 拦截器必须实现 Castle.DynamicProxy的 IInterceptor 接口， 有以下两种




Interface 【默认值】代表是接口型
Class 代表是class类型 这种的话是需要将要拦截的方法标virtual




InterceptorKey 如果同一个类型的拦截器有多个 可以指定Key
InitMethod 当实例被创建后执行的方法名称 类似Spring的init-method 可以是有参数(只能1个参数类型是IComponentContext)和无参数的方法
DestroyMetnod 当实例被Release时执行的方法 类似Spring的destroy-method 必须是无参数的方法


 


  [Bean(InitMethod = "start",DestroyMetnod = "destroy")]
    public class A30
    {
        [Value("aaaaa")]
        public string Test { get; set; }

        public A29 a29;

        void start(IComponentContext context)
        {
            this.Test = "bbbb";
            a29 = context.Resolve<A29>();
        }

        void destroy()
        {
            this.Test = null;
            a29.Test = null;
        }
    }
    

 

 


    public class B
    {

    }
    
    [Bean(typeof(B),"a5")]
    public class A5:B
    {
        public string School { get; set; } = "测试a5";
        public override string GetSchool()
        {
            return this.School;
        }
    }

 

 
Autowired 自动装配
可以打在Field Property 构造方法的Parameter上面 其中Field 和 Property 支持在父类
 


    [Bean]
    public class A16
    {
    public A16([Autowired]A21 a21)
        {
            Name = name;
            A21 = a21;
        }
        
        [Autowired("A13")]
        public B b1;


        [Autowired]
        public B B { get; set; }
        
    //Required默认为true 如果装载错误会抛异常出来。如果指定为false则不抛异常
    [Autowired("adadada",Required = false)]
        public B b1;
    }

 

 
Value 和 PropertySource
 
PropertySource类似Spring里面的PropertySource 可以指定数据源 支持 xml json格式 支持内嵌资源
1.json格式的文件


{
  "a10": "aaaaaaaaa1",
  "list": [ 1, 2, 3 ],
  "dic": {
    "name": "name1"
  },
  "testInitField": 1,
  "testInitProperty": 1,
}

 

 


   [Bean]
    [PropertySource("/file/appsettings1.json")]
    public class A10
    {
        public A10([Value("#{a10}")]string school,[Value("#{list}")]List<int> list,[Value("#{dic}")]Dictionary<string,string> dic)
        {
            this.School = school;
            this.list = list;
            this.dic = dic;

        }
        public string School { get; set; }
        public List<int> list { get; set; } 
        public Dictionary<string,string> dic { get; set; } 
        
    [Value("#{testInitField}")]
        public int test;
        
    [Value("#{testInitProperty}")]
        public int test2 { get; set; }
        
    //可以直接指定值
    [Value("2")]
    public int test3 { get; set; }
    }

 

2. xml格式的文件


<?xml version="1.0" encoding="utf-8" ?>
<autofac>
  <a11>aaaaaaaaa1</a11>
  <list name="0">1</list>
  <list name="1">2</list>
  <list name="2">3</list>
  <dic name="name">name1</dic>
</autofac>

 



    [Bean]
    [PropertySource("/file/appsettings1.xml")]
    public class A11
    {
        public A11([Value("#{a11}")]string school,[Value("#{list}")]List<int> list,[Value("#{dic}")]Dictionary<string,string> dic)
        {
            this.School = school;
            this.list = list;
            this.dic = dic;

        }
        public string School { get; set; }
        public List<int> list { get; set; } 
        public Dictionary<string,string> dic { get; set; } 
    }

 

3.不指定PropertySource的话会默认从工程目录的 appsettings.json获取值

https://www.cnblogs.com/yudongdong/p/10285392.html
**************************************************
深度解密HTTP通信细节
目录

HTTP报文截获
背景介绍
抓包
mac本地
远程docker

请求 && 分析
关闭服务进程
关闭docker
重启docker
正常请求


HTTP协议分析
整体介绍
编码
MIME类型
URI/URL/URN
HTTP方法
状态码
报文格式

HTTP协议进阶
代理
缓存
cookie
实体和编码
国际化支持
重定向与负载均衡
HTTP重定向
DNS重定向

HTTP连接
并行连接
持久连接
管道化连接


小结
参考资料

上一篇文章中，我们学会了用wireshark和tcpdump来分析TCP的“三次握手，四次挥手”，非常好用。这哥俩就是传说中的锤子，拿着锤子，看什么都像钉子！在这篇文章中，我对准了HTTP这颗钉子砸下去，咳咳。
为了对网络数据包的“流转”有更加深刻的理解，我在docker（远程）上部署一个服务，支持http方式调用。从客户端（本地）用http方式请求其中的一个接口，并得到响应数据。同时本地通过wireshark抓包，远程用tcpdump抓包，然后分析过程中的所有通信细节。悲剧是把美好的东西撕碎给人看，而我则是把复杂的东西撕碎了给人看。
文章稍长，请在看本文时保持耐心。我先通过工具获取HTTP通信的数据包，再来抽丝剥茧，深入二进制的天地里，解密HTTP所有的通信细节。分析过程中，由点到面，将相关知识串接起来。保证全篇读完之后，你对HTTP的理解会上升一个台阶！
HTTP报文截获
背景介绍
我手头现在有一个地理几何相关的服务，它提供一组接口对外使用。其中有一个接口是Fence2Area. 使用方传入一个围栏（由点的列表组成，点由<经度，纬度>表示）、点的坐标系类型（谷歌地图用的是wgs84, 国内腾讯、高德用的是soso, 而百度用的是另一套自己的坐标系），接口输出的则是围栏的面积。
我请求服务的“Fence2Area”接口，输入围栏(fence)顶点(lng, lat)坐标、坐标系类型(coordtype)，输出的则是多边形的面积(area).
一次正常的请求示例url, 这个大家都不陌生（我用docker_ip代替真实的ip）:
http://docker_ip:7080/data?cmd=Fence2Area&meta={"caller":"test","TraceId":"test"}&request={"fence":[{"lng":10.2,"lat":10.2}, {"lng":10.2,"lat":8.2}, {"lng":8.2,"lat":8.2}, {"lng":8.2,"lat":10.2}],"coordtype":2}
请求发出后，服务器进行处理，之后，客户端收到返回的数据如下：
{
    "data": {
        "area": 48764135597.842606
    },
    "errstr": ""
}
area字段表示面积，errstr表示出错信息，空说明没有出错。
抓包
在真正发送请求之前，需要进行抓包前的设置。在本地mac，我用wireshark; 而在远程docker上，我用tcpdump工具。
mac本地
设置wireshark包过滤器，监控本地主机和远程docker之间的通信。
ip.addr eq docker_ip
点击开始捕获。
远程docker
该服务通过7080端口对外提供，使用如下命令捕获网络包：
tcpdump -w /tmp/testHttp.cap port 7080 -s0
请求 && 分析
准备工作做完，我选了一个神圣的时刻，在本地通过浏览器访问如下url:
http://docker_ip:7080/data?cmd=Fence2Area&meta={"caller":"test","TraceId":"test"}&request={"fence":[{"lng":10.2,"lat":10.2}, {"lng":10.2,"lat":8.2}, {"lng":8.2,"lat":8.2}, {"lng":8.2,"lat":10.2}],"coordtype":2}
这样本地的wireshark和远程的tcpdump都能抓取到HTTP网络数据包。
关闭服务进程
正式请求之前，我们先看一下几种特殊的情形。
首先，关闭gcs服务进程，请求直接返回RST报文。

如上图，我在请求的时候，访问服务端的另一个端口5010, 这个端口没有服务监听，和关闭gcs服务进程是同样的效果。可以看到，客户端发送SYN报文，但直接被远程docker RST掉了。因为服务端操作系统找不到监听此端口的进程。
关闭docker
关闭docker, 由于发送的SYN报文段得不到响应，因此会进行重试，mac下重试的次数为10次。

先每隔1秒重试了5次，再用“指数退避”的时间间隔重试，2s, 4s, 8s, 16s, 32s. 最后结束。
重启docker
先进行一次正常的访问，随后重启docker。并再次在本地访问以上url, 浏览器这时还是用的上一次的端口，访问到服务端后，因为它已经重启了，所以服务端已经没有这个连接的消息了。因此会返回一个RST报文。
正常请求
服务正常启动，正常发送请求，这次请求成功，那是当然的，嘿嘿！

这是在mac上用wireshark捕获的数据包，共7个包，前三个包为3次握手的包，第四个包为HTTP层发送的请求数据，第五个包为服务端的TCP 确认报文，第六个包为服务端在HTTP层发送的响应数据，第七个包为mac对第六个包的确认报文。
重点来关注后面几个包，先看第四个包，
0x0000:  4500 0295 0000 4000 3606 623b ac17 ccdc
0x0010:  0a60 5cd4 db9b 1ba8 a59a 46ce 6d03 e87d
0x0020:  8018 1015 0ee7 0000 0101 080a 2e4c b2ef
0x0030:  0f20 3acf 4745 5420 2f64 6174 613f 636d
0x0040:  643d 4665 6e63 6532 4172 6561 266d 6574
0x0050:  613d 7b25 3232 6361 6c6c 6572 2532 323a
0x0060:  2532 3274 6573 7425 3232 2c25 3232 5472
0x0070:  6163 6549 6425 3232 3a25 3232 7465 7374
0x0080:  2532 327d 2672 6571 7565 7374 3d7b 2532
0x0090:  3266 656e 6365 2532 323a 5b7b 2532 326c
0x00a0:  6e67 2532 323a 3130 2e32 2c25 3232 6c61
0x00b0:  7425 3232 3a31 302e 327d 2c25 3230 7b25
0x00c0:  3232 6c6e 6725 3232 3a31 302e 322c 2532
0x00d0:  326c 6174 2532 323a 382e 327d 2c25 3230
0x00e0:  7b25 3232 6c6e 6725 3232 3a38 2e32 2c25
0x00f0:  3232 6c61 7425 3232 3a38 2e32 7d2c 2532
0x0100:  307b 2532 326c 6e67 2532 323a 382e 322c
0x0110:  2532 326c 6174 2532 323a 3130 2e32 7d5d
0x0120:  2c25 3232 636f 6f72 6474 7970 6525 3232
0x0130:  3a32 7d20 4854 5450 2f31 2e31 0d0a 486f
0x0140:  7374 3a20 3130 2e39 362e 3932 2e32 3132
0x0150:  3a37 3038 300d 0a55 7067 7261 6465 2d49
0x0160:  6e73 6563 7572 652d 5265 7175 6573 7473
0x0170:  3a20 310d 0a41 6363 6570 743a 2074 6578
0x0180:  742f 6874 6d6c 2c61 7070 6c69 6361 7469
0x0190:  6f6e 2f78 6874 6d6c 2b78 6d6c 2c61 7070
0x01a0:  6c69 6361 7469 6f6e 2f78 6d6c 3b71 3d30
0x01b0:  2e39 2c2a 2f2a 3b71 3d30 2e38 0d0a 5573
0x01c0:  6572 2d41 6765 6e74 3a20 4d6f 7a69 6c6c
0x01d0:  612f 352e 3020 284d 6163 696e 746f 7368
0x01e0:  3b20 496e 7465 6c20 4d61 6320 4f53 2058
0x01f0:  2031 305f 3133 5f36 2920 4170 706c 6557
0x0200:  6562 4b69 742f 3630 352e 312e 3135 2028
0x0210:  4b48 544d 4c2c 206c 696b 6520 4765 636b
0x0220:  6f29 2056 6572 7369 6f6e 2f31 322e 302e
0x0230:  3220 5361 6661 7269 2f36 3035 2e31 2e31
0x0240:  350d 0a41 6363 6570 742d 4c61 6e67 7561
0x0250:  6765 3a20 7a68 2d63 6e0d 0a41 6363 6570
0x0260:  742d 456e 636f 6469 6e67 3a20 677a 6970
0x0270:  2c20 6465 666c 6174 650d 0a43 6f6e 6e65
0x0280:  6374 696f 6e3a 206b 6565 702d 616c 6976
0x0290:  650d 0a0d 0a
我们来逐字节分析。



字节值
字节含义




0x4
IP版本为ipv4


0x5
首部长度为5 * 4字节=20B


0x00
服务类型，现在基本都置为0


0x0295
总长度为661字节，即整个包的长度是661字节


0x0000
标识。同一个数据报的唯一标识。当IP数据报被拆分时，会复制到每一个数据中。


0x4000
3bit 标志 + 13bit 片偏移。3bit 标志对应 R、DF、MF。目前只有后两位有效，DF位：为1表示不分片，为0表示分片。MF：为1表示“更多的片”，为0表示这是最后一片。13bit 片位移：本分片在原先数据报文中相对首位的偏移位。（需要再乘以8 )


0x36
生存时间TTL。IP报文所允许通过的路由器的最大数量。每经过一个路由器，TTL减1，当为 0 时，路由器将该数据报丢弃。TTL 字段是由发送端初始设置一个 8 bit字段.推荐的初始值由分配数字 RFC 指定。发送 ICMP 回显应答时经常把 TTL 设为最大值 255。TTL可以防止数据报陷入路由循环。 此处为54.


0x06
协议类型。指出IP报文携带的数据使用的是哪种协议，以便目的主机的IP层能知道要将数据报上交到哪个进程。TCP 的协议号为6，UDP 的协议号为17。ICMP 的协议号为1，IGMP 的协议号为2。该 IP 报文携带的数据使用 TCP 协议，得到了验证。


0x623b
16bitIP首部校验和。


0xac17 ccdc
32bit源ip地址。


0x0a60 5cd4
32bit目的ip地址。



剩余的数据部分即为TCP协议相关的。TCP也是20B固定长度+可变长度部分。



字节值
字节含义




0xdb9b
16bit源端口。56219


0x1ba8
16bit目的端口7080


0xa59a 46ce
32bit序列号。2778351310


0x6d03 e87d
32bit确认号。1828972669


0x8
4bit首部长度，以4byte为单位。共8*4=32字节。因此TCP报文的可选长度为32-20=12字节


0b000000
6bit保留位。目前置为0.


0b011000
6bitTCP标志位。从左到右依次是紧急 URG、确认 ACK、推送 PSH、复位 RST、同步 SYN 、终止 FIN。ack有效，同时psh有效


0x1015
滑动窗口大小，滑动窗口即tcp接收缓冲区的大小，用于tcp拥塞控制。4117


0x0ee7
16bit校验和。


0x0000
紧急指针。仅在 URG = 1时才有意义，它指出本报文段中的紧急数据的字节数。当 URG = 1 时，发送方 TCP 就把紧急数据插入到本报文段数据的最前面，而在紧急数据后面的数据仍是普通数据。



可变长度部分，协议如下：



字节值
字节含义




0x01
无操作


0x01
无操作


0x0402
表示支持SACK


0x080a 2e4c b2ef 0f20 3acf
时间戳。Ts val=0x2e4c b2ef=776778479, ecr=0x0f20 3acf=253770447



剩下来的就是数据部分了。我们一行一行地看。因为http是字符流，所以我们先看一下ascii字符集，执行命令：
man ascii
可以得到ascii码，我们直接看十六进制的结果：




行首地址
字节流
字符




0x0030
4745 5420 2f64 6174 613f 636d
GE T /d at a? cm


0x0040
643d 4665 6e63 6532 4172 6561 266d 6574
d= Fe nc e2 Ar ea &m et


0x0050
613d 7b25 3232 6361 6c6c 6572 2532 323a
a= {% 22 ca ll er %2 2:


0x0060
2532 3274 6573 7425 3232 2c25 3232 5472
%2 2t es t% 22 ,% 22 Tr


0x0070
6163 6549 6425 3232 3a25 3232 7465 7374
ac eI d% 22 :% 22 te st


0x0080
2532 327d 2672 6571 7565 7374 3d7b 2532
%2 2} &r eq ue st ={ %2


0x0090
3266 656e 6365 2532 323a 5b7b 2532 326c
2f en ce %2 2: [{ %2 2l


0x00a0
6e67 2532 323a 3130 2e32 2c25 3232 6c61
ng %2 2: 10 .2 ,% 22 la


0x00b0
7425 3232 3a31 302e 327d 2c25 3230 7b25
t% 22 :1 0. 2} ,% 20 {%


0x00c0
3232 6c6e 6725 3232 3a31 302e 322c 2532
22 ln g% 22 :1 0. 2, %2


0x00d0
326c 6174 2532 323a 382e 327d 2c25 3230
2l at %2 2: 8. 2} ,% 20


0x00e0
7b25 3232 6c6e 6725 3232 3a38 2e32 2c25
{% 22 ln g% 22 :8 .2 ,%


0x00f0
3232 6c61 7425 3232 3a38 2e32 7d2c 2532
22 la t% 22 :8 .2 }, %2


0x0100
307b 2532 326c 6e67 2532 323a 382e 322c
0{ %2 2l ng %2 2: 8. 2,


0x0110
2532 326c 6174 2532 323a 3130 2e32 7d5d
%2 2l at %2 2: 10 .2 } ]


0x0120
2c25 3232 636f 6f72 6474 7970 6525 3232
,% 22 co or dt yp e% 22


0x0130
3a32 7d20 4854 5450 2f31 2e31 0d0a 486f
:2 } HT TP /1 .1 crnl Ho


0x0140
7374 3a20 3130 2e39 362e 3932 2e32 3132
st : 10 .9 6. 92 .2 12


0x0150
3a37 3038 300d 0a55 7067 7261 6465 2d49
:7 08 0cr nlU pg ra de -I


0x0160
6e73 6563 7572 652d 5265 7175 6573 7473
ns ec ur e- Re qu es ts


0x0170
3a20 310d 0a41 6363 6570 743a 2074 6578
: 1cr nlA cc ep t: t ex


0x0180
742f 6874 6d6c 2c61 7070 6c69 6361 7469
t/ ht ml ,a pp li ca ti


0x0190
6f6e 2f78 6874 6d6c 2b78 6d6c 2c61 7070
on /x ht ml +x ml ,a pp


0x01a0
6c69 6361 7469 6f6e 2f78 6d6c 3b71 3d30
li ca ti on /x ml ;q =0


0x01b0
2e39 2c2a 2f2a 3b71 3d30 2e38 0d0a 5573
.9 ,* /* ;q =0 .8 crnl Us


0x01c0
6572 2d41 6765 6e74 3a20 4d6f 7a69 6c6c
er -A ge nt : Mo zi ll


0x01d0
612f 352e 3020 284d 6163 696e 746f 7368
a/ 5. 0 (M ac in to sh


0x01e0
3b20 496e 7465 6c20 4d61 6320 4f53 2058
; In te l Ma c OS X


0x01f0
2031 305f 3133 5f36 2920 4170 706c 6557
1 0_ 13 _6 ) Ap pl eW


0x0200
6562 4b69 742f 3630 352e 312e 3135 2028
eb Ki t/ 60 5. 1. 15 (


0x0210
4b48 544d 4c2c 206c 696b 6520 4765 636b
KH TM L, l i k e Ge ck


0x0220
6f29 2056 6572 7369 6f6e 2f31 322e 302e
o) V er si o n /1 2. 0.


0x0230
3220 5361 6661 7269 2f36 3035 2e31 2e31
2 Sa fa ri /6 05 .1 .1


0x0240
350d 0a41 6363 6570 742d 4c61 6e67 7561
5cr nlA cc ep t- La ng ua


0x0250
6765 3a20 7a68 2d63 6e0d 0a41 6363 6570
ge : zh -c ncr nlA cc ep


0x0260
742d 456e 636f 6469 6e67 3a20 677a 6970
t- En co di ng : gz ip


0x0270
2c20 6465 666c 6174 650d 0a43 6f6e 6e65
, de fl at ecr nlC on ne


0x0280
6374 696f 6e3a 206b 6565 702d 616c 6976
ct io n: k ee p- al iv


0x0290
650d 0a0d 0a
ecr nl cr nl



把上表的最后一列连起来，就是：
GET /data?cmd=Fence2Area&meta={%22caller%22:%22test%22,%22TraceId%22:%22test%22}&request={%22fence%22:[{%22lng%22:10.2,%22lat%22:10.2},%20{%22lng%22:10.2,%22lat%22:8.2},%20{%22lng%22:8.2,%22lat%22:8.2},%20{%22lng%22:8.2,%22lat%22:10.2}],%22coordtype%22:2} HTTP/1.1 
Host: 10.96.92.212:7080 
Upgrade-Insecure-Requests: 1 
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8 
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.0.2 Safari/605.1.15 
Accept-Language: zh-cn 
Accept-Encoding: gzip, deflate 
Connection: keep-alive 
 
其中，cr nl表示回车，换行。
docker收到数据后，会回复一个ack包。第四个包的总长度为661字节，去掉IP头部20字节，TCP头部固定部分20字节，TCP头部可选长度为12字节，共52字节，因此TCP数据部分总长度为661-52=609字节。另外，序列号为2778351310.
再来看第5个包，字节流如下：
0x0000:  4500 0034 d28b 4000 4006 8810 0a60 5cd4
0x0010:  ac17 ccdc 1ba8 db9b 6d03 e87d a59a 492f
0x0020:  8010 00ec e04e 0000 0101 080a 0f20 3af7
0x0030:  2e4c b2ef



字节值
字节含义




0x4
IP版本为ipv4


0x5
首部长度为5 * 4字节=20B


0x00
服务类型，现在基本都置为0


0x0034
总长度为52字节，即整个包的长度是52字节


0xd28b
标识。同一个数据报的唯一标识。当IP数据报被拆分时，会复制到每一个数据中。


0x4000
3bit 标志 + 13bit 片偏移。3bit 标志对应 R、DF、MF。目前只有后两位有效，DF位：为1表示不分片，为0表示分片。MF：为1表示“更多的片”，为0表示这是最后一片。13bit 片位移：本分片在原先数据报文中相对首位的偏移位。（需要再乘以8 )


0x40
生存时间TTL。IP报文所允许通过的路由器的最大数量。每经过一个路由器，TTL减1，当为 0 时，路由器将该数据报丢弃。TTL 字段是由发送端初始设置一个 8 bit字段.推荐的初始值由分配数字 RFC 指定。发送 ICMP 回显应答时经常把 TTL 设为最大值 255。TTL可以防止数据报陷入路由循环。 此处为54.


0x06
协议类型。指出IP报文携带的数据使用的是哪种协议，以便目的主机的IP层能知道要将数据报上交到哪个进程。TCP 的协议号为6，UDP 的协议号为17。ICMP 的协议号为1，IGMP 的协议号为2。该 IP 报文携带的数据使用 TCP 协议，得到了验证。


0x8810
16bitIP首部校验和。


0x0a60 5cd4
32bit源ip地址。


0xac17 ccdc
32bit目的ip地址。



剩余的数据部分即为TCP协议相关的。TCP也是20B固定长度+可变长度部分。



字节值
字节含义




0x1ba8
16bit源端口7080


0xdb9b
16bit目的端口。56219


0x6d03 e87d
32bit序列号。1828972669


0xa59a 492f
32bit确认号。2778351919. 第三个包的序列号为2778351310, 加上数据长度609, 正好相等。


0x8
4bit首部长度，以4byte为单位。共8*4=32字节。因此TCP报文的可选长度为32-20=12字节


0b000000
6bit保留位。目前置为0.


0b010000
6bitTCP标志位。从左到右依次是紧急 URG、确认 ACK、推送 PSH、复位 RST、同步 SYN 、终止 FIN。ack有效


0x00ec
滑动窗口大小，滑动窗口即tcp接收缓冲区的大小，用于tcp拥塞控制。4117


0xe04e
16bit校验和。


0x0000
紧急指针。仅在 URG = 1时才有意义，它指出本报文段中的紧急数据的字节数。当 URG = 1 时，发送方 TCP 就把紧急数据插入到本报文段数据的最前面，而在紧急数据后面的数据仍是普通数据。



可变长度部分，协议如下：



字节值
字节含义




0x01
无操作


0x01
无操作


0x0402
表示支持SACK


0x080a 2e4c b2ef 0f20 3acf
时间戳。Ts val=253770487, ecr=776778479



数据部分为空，这个包仅为确认包。
再来看第六个包，字节流如下：
0x0000:  4500 00f9 d28c 4000 4006 874a 0a60 5cd4
0x0010:  ac17 ccdc 1ba8 db9b 6d03 e87d a59a 492f
0x0020:  8018 00ec e113 0000 0101 080a 0f20 3af8
0x0030:  2e4c b2ef 4854 5450 2f31 2e31 2032 3030
0x0040:  204f 4b0d 0a41 6363 6573 732d 436f 6e74
0x0050:  726f 6c2d 416c 6c6f 772d 4f72 6967 696e
0x0060:  3a20 2a0d 0a44 6174 653a 2054 6875 2c20
0x0070:  3033 204a 616e 2032 3031 3920 3132 3a32
0x0080:  333a 3437 2047 4d54 0d0a 436f 6e74 656e
0x0090:  742d 4c65 6e67 7468 3a20 3438 0d0a 436f
0x00a0:  6e74 656e 742d 5479 7065 3a20 7465 7874
0x00b0:  2f70 6c61 696e 3b20 6368 6172 7365 743d
0x00c0:  7574 662d 380d 0a0d 0a7b 2264 6174 6122
0x00d0:  3a7b 2261 7265 6122 3a34 3837 3634 3133
0x00e0:  3535 3937 2e38 3432 3630 367d 2c22 6572
0x00f0:  7273 7472 223a 2222 7d



字节值
字节含义




0x4
IP版本为ipv4


0x5
首部长度为5 * 4字节=20B


0x00
服务类型，现在基本都置为0


0x00f9
总长度为249字节，即整个包的长度是249字节


0xd28c
标识。同一个数据报的唯一标识。当IP数据报被拆分时，会复制到每一个数据中。


0x4000
3bit 标志 + 13bit 片偏移。3bit 标志对应 R、DF、MF。目前只有后两位有效，DF位：为1表示不分片，为0表示分片。MF：为1表示“更多的片”，为0表示这是最后一片。13bit 片位移：本分片在原先数据报文中相对首位的偏移位。（需要再乘以8 )


0x40
生存时间TTL。IP报文所允许通过的路由器的最大数量。每经过一个路由器，TTL减1，当为 0 时，路由器将该数据报丢弃。TTL 字段是由发送端初始设置一个 8 bit字段.推荐的初始值由分配数字 RFC 指定。发送 ICMP 回显应答时经常把 TTL 设为最大值 255。TTL可以防止数据报陷入路由循环。 此处为64.


0x06
协议类型。指出IP报文携带的数据使用的是哪种协议，以便目的主机的IP层能知道要将数据报上交到哪个进程。TCP 的协议号为6，UDP 的协议号为17。ICMP 的协议号为1，IGMP 的协议号为2。该 IP 报文携带的数据使用 TCP 协议，得到了验证。


0x874a
16bitIP首部校验和。


0x0a60 5cd4
32bit源ip地址。


0xac17 ccdc
32bit目的ip地址。



剩余的数据部分即为TCP协议相关的。TCP也是20B固定长度+可变长度部分。



字节值
字节含义




0x1ba8
16bit源端口7080


0xdb9b
16bit目的端口。56219


0x6d03 e87d
32bit序列号。1828972669


0xa59a 492f
32bit确认号。2778351919


0x8
4bit首部长度，以4byte为单位。共8*4=32字节。因此TCP报文的可选长度为32-20=12字节


0b000000
6bit保留位。目前置为0.


0b011000
6bitTCP标志位。从左到右依次是紧急 URG、确认 ACK、推送 PSH、复位 RST、同步 SYN 、终止 FIN。ack有效，同时psh有效


0x00ec
滑动窗口大小，滑动窗口即tcp接收缓冲区的大小，用于tcp拥塞控制。236


0xe113
16bit校验和。


0x0000
紧急指针。仅在 URG = 1时才有意义，它指出本报文段中的紧急数据的字节数。当 URG = 1 时，发送方 TCP 就把紧急数据插入到本报文段数据的最前面，而在紧急数据后面的数据仍是普通数据。



可变长度部分，协议如下：



字节值
字节含义




0x01
无操作


0x01
无操作


0x0402
表示支持SACK


0x080a 0f20 3af8 2e4c b2ef
时间戳。Ts val=0x2e4c b2ef=253770488, ecr=0x0f20 3acf=776778479



剩下来的就是数据部分了。我们一行一行地看。



首地址
字节流
字符




0x0030
4854 5450 2f31 2e31 2032 3030
HTTP/1.1 200


0x0040
204f 4b0d 0a41 6363 6573 732d 436f 6e74
OK \r\n Access-Cont


0x0050
726f 6c2d 416c 6c6f 772d 4f72 6967 696e
rol-Allow-Origin


0x0060
3a20 2a0d 0a44 6174 653a 2054 6875 2c20
: * \r\n Date: Thu,


0x0070
3033 204a 616e 2032 3031 3920 3132 3a32
03 Jan 2019 12:2


0x0080
333a 3437 2047 4d54 0d0a 436f 6e74 656e
3:47 GMT \r\n Conten


0x0090
742d 4c65 6e67 7468 3a20 3438 0d0a 436f
t-Length: 48\r\n Co


0x00a0
6e74 656e 742d 5479 7065 3a20 7465 7874
ntent-Type: text


0x00b0
2f70 6c61 696e 3b20 6368 6172 7365 743d
/plain; charset=


0x00c0
7574 662d 380d 0a0d 0a7b 2264 6174 6122
utf-8\r\n\r\n{"data"


0x00d0
3a7b 2261 7265 6122 3a34 3837 3634 3133
:{"area":4876413


0x00e0
3535 3937 2e38 3432 3630 367d 2c22 6572
5597.842606},"er


0x00f0
7273 7472 223a 2222 7d
rstr":""}



把上表的最后一列连起来，就是：
HTTP/1.1 200 OK 
Access-Control-Allow-Origin: * 
Date: Thu, 03 Jan 2019 12:23:47 GMT 
Content-Length: 48 
Content-Type: text/plain; charset=utf-8 
{"data":{"area":48764135597.842606},"errstr":""}
Content-Length: 48，最后一行的长度即为48个字节。
最后，第七个包，字节流如下：
0x0000:  4500 0034 0000 4000 3606 649c ac17 ccdc
0x0010:  0a60 5cd4 db9b 1ba8 a59a 492f 6d03 e942
0x0020:  8010 100f 1eb9 0000 0101 080a 2e4c b314
0x0030:  0f20 3af8



字节值
字节含义




0x4
IP版本为ipv4


0x5
首部长度为5 * 4字节=20B


0x00
服务类型，现在基本都置为0


0x0034
总长度为52字节，即整个包的长度是52字节


0x0000
标识。同一个数据报的唯一标识。当IP数据报被拆分时，会复制到每一个数据中。


0x4000
3bit 标志 + 13bit 片偏移。3bit 标志对应 R、DF、MF。目前只有后两位有效，DF位：为1表示不分片，为0表示分片。MF：为1表示“更多的片”，为0表示这是最后一片。13bit 片位移：本分片在原先数据报文中相对首位的偏移位。（需要再乘以8 )


0x36
生存时间TTL。IP报文所允许通过的路由器的最大数量。每经过一个路由器，TTL减1，当为 0 时，路由器将该数据报丢弃。TTL 字段是由发送端初始设置一个 8 bit字段.推荐的初始值由分配数字 RFC 指定。发送 ICMP 回显应答时经常把 TTL 设为最大值 255。TTL可以防止数据报陷入路由循环。 此处为54.


0x06
协议类型。指出IP报文携带的数据使用的是哪种协议，以便目的主机的IP层能知道要将数据报上交到哪个进程。TCP 的协议号为6，UDP 的协议号为17。ICMP 的协议号为1，IGMP 的协议号为2。该 IP 报文携带的数据使用 TCP 协议，得到了验证。


0x649c
16bitIP首部校验和。


0xac17 ccdc
32bit源ip地址。


0x0a60 5cd4
32bit目的ip地址。



剩余的数据部分即为TCP协议相关的。TCP也是20B固定长度+可变长度部分。



字节值
字节含义




0xdb9b
16bit源端口。56219


0x1ba8
16bit目的端口7080


0xa59a 492f
32bit序列号。2778351919


0x6d03 e942
32bit确认号号。1828972866. 第六个包的序列号为1828972669, 加上数据长度197, 正好相等


0x8
4bit首部长度，以4byte为单位。共8*4=32字节。因此TCP报文的可选长度为32-20=12字节


0b000000
6bit保留位。目前置为0


0b010000
6bitTCP标志位。从左到右依次是紧急 URG、确认 ACK、推送 PSH、复位 RST、同步 SYN 、终止 FIN。ack有效


0x100f
滑动窗口大小，滑动窗口即tcp接收缓冲区的大小，用于tcp拥塞控制。4111


0x1eb9
16bit校验和。


0x0000
紧急指针。仅在 URG = 1时才有意义，它指出本报文段中的紧急数据的字节数。当 URG = 1 时，发送方 TCP 就把紧急数据插入到本报文段数据的最前面，而在紧急数据后面的数据仍是普通数据。



可变长度部分，协议如下：



字节值
字节含义




0x01
无操作


0x01
无操作


0x080a 2e4c b314 0f20 3af8
时间戳。Ts val=0x2e4c b314=776778516, ecr=0x0f20 3af8=253770488



至此，一次完整的http请求的报文就解析完了。感觉如何，是不是很亲切？
HTTP协议分析
上面我们把HTTP协议相关的数据给解构了，下面我将对照上面的数据拆解结果，一步步带你深入理解HTTP协议。
整体介绍
HTTP(Hypertext Transfer Protocol)超文本传输协议，是在互联网上进行通信时使用的一种协议。说得更形象一点：HTTP是现代互联网中使用的公共语言。它最著名的应用是用在浏览器的服务器间的通信。
HTTP属于应用层协议，底层是靠TCP进行可靠地信息传输。

HTTP在传输一段报文时，会以流的形式将报文数据的内容通过一条打开的TCP连接按序传输。TCP接到上层应用交给它的数据流之后，会按序将数据流打散成一个个的分段。再交到IP层，通过网络进行传输。另一端的接收方则相反，它们将接收到的分段按序组装好，交给上层HTTP协议进行处理。

编码
我们再来回顾一下：



url
值




原始url
/data?cmd=Fence2Area&meta={"caller":"test","TraceId":"test"}&request={"fence":[{"lng":10.2,"lat":10.2}, {"lng":10.2,"lat":8.2}, {"lng":8.2,"lat":8.2}, {"lng":8.2,"lat":10.2}],"coordtype":2}


编码后url
/data?cmd=Fence2Area&meta={%22caller%22:%22test%22,%22TraceId%22:%22test%22}&request={%22fence%22:[{%22lng%22:10.2,%22lat%22:10.2},%20{%22lng%22:10.2,%22lat%22:8.2},%20{%22lng%22:8.2,%22lat%22:8.2},%20{%22lng%22:8.2,%22lat%22:10.2}],%22coordtype%22:2}



在之前的报文拆解过程中，我们看到多了很多%22，其实，0x22是单引号"的ascii值，
一方面，URL描述的资源为了能通过其他各种协议传送，但是有些协议在传输过程中会剥去一些特定的字符；另一方面，URL还是可读的，所以那些不可打印的字符就不能在URL中使用了，比如空格；最后，URL还得是完整的，它需要支持所有语言的字符。
总之，基于很多原因，URL设计者将US-ASCII码和其转义序列集成到URL中，通过转义序列，就可以用US-ASCII字符集的有限子集对任意字符或数据进行编码了。
转义的方法：百分号(%)后跟着两个表示ASCII码的十六进制数。比如：

所以上面在浏览器发送给服务器的URL进行了非“安全字符”编码，也就不奇怪了吧？

在URL中，当上面的保留字符用在保留用途之外的场合时，需要对URL进行编码。
MIME类型
响应数据中，我们注意到有一个首部：
Content-Type: text/plain; charset=utf-8
互联网上有数千种不同的数据类型，HTTP给每种对象都打上了MIME(Multipurpose Internet Media Extension, 多用途因特网邮件扩展)标签，也就是响应数据中的Content-Type. MIME本来是用在邮件协议中的，后来被移植到了HTTP中。浏览器从服务器上取回了一个对象时，会去查看MIME类型，从而得知如何处理这种对象，是该展示图片，还是调用声卡播放声音。MIME通过斜杠来标识对象的主类型和其中的特定的子类型，下表展示了一些常见的类型，其中的实体主体是指body部分：

URI/URL/URN
URI(Uniform Resource Identifier, 统一资源标识符)表示服务器资源，URL(Uniform Resource Locator, 统一资源定位符)和URN(Uniform Resource Name, 统一资源名)是URI的具体实现。URI是一个通用的概念，由两个主要的子集URL和URN构成，URL通过位置、URN通过名字来标识资源。
URL定义了资源的位置，表示资源的实际地址，在使用URL的过程中，如果URL背后的资源发生了位置移动，访问者就找不到它了。这个时候就要用到URN了，它给定资源一个名字，无论它移动到哪里，都可以通过这个名字来访问到它，简直完美！
URL通常的格式是：
协议方案+服务器地址+具体的资源路径
协议方案(scheme)，如 http, ftp，告知web客户端怎样访问资源)；服务器地址，如 www.oreilly.com; 具体的资源路径，如 index.html.

HTTP方法
HTTP支持几种不同的请求方法，每种方法对服务器要求的动作不同，如下图是几种常见的方法：

HEAD方法只获取头部，不获取数据部分。通过头部可以获取比如资源的类型(Content-Type)、资源的长度(Content-Length)这些信息。这样，客户端可以获取即将请求资源的一些情况，可以做到心中有数。
POST用于向服务器发送数据，常见的是提交表单；PUT用于向服务器上的资源存储数据。
状态码
每条HTTP的响应报文都会带上一个三位数字的状态码和一条解释性的“原因短语”，通知客户端本次请求的状态，帮助客户端快速理解事务处理结果，最常见的是：
200 OK 
404 Not Found
500 Internal Server Error
我们平时使用浏览器的时候，很多的错误码其实是由浏览器处理的，我们感知不到。但是404 Not Found会穿透重重迷雾，来到我们面前，为何？那是因为他对我们爱的深沉啊！
客户端可以据此状态码，决定下一步的行动（如重定向等）。
三位数字的第一位表示分类：

报文格式
HTTP报文实际上是由一行行的字符串组成的，每行字符串的末尾用\r\n分隔，人类可以很方便的阅读。顺便说一句，不是所有的协议都对人类这么友好的，像thrift协议，直接甩一堆字节给你，告诉你说0x0001表示调用方法，诸如此类的，你只能对着一个十六进制的数据块一个个地去“解码”。不可能像HTTP协议这样，直接将字符编码，人类可以直接读懂。
举个简单的请求报文和响应报文的格式的例子：

实际上，请求报文也是可以有body（主体）部分的。请求报文是由请求行（request line）、请求头部（header）、空行、请求数据四个部分组成。唯一要注意的一点就是，请求报文即使body部分是空的，请求头部后的回车换行符也是必须要有的。

响应报文的格式和请求报文的格式类似：

请求报文、响应报文的起始行和响应头部里的字段都是文本化、结构化的。而请求body却可以包含任意二进制数据（如图片、视频、软件等），当然也可以包含文本。
有些首部是通用的，有些则是请求或者响应报文才会有的。



首部
属性
值
含义




Date
通用
Thu, 03 Jan 2019 12:23:47 GMT
报文构建的时间


Accept
请求报文
text/html,application/xhtml+xml,application/xm
客户端能接收的数据类型


Content-Type
通用
Content-Type: text/plain; charset=utf-8
报文中的body部分的数据类型。注意，若是请求报文中也有数据部分，也是需要此字段的



顺便提一下， 用telnet直连服务器的http端口，telnet命令会建立一条TCP通道，然后就可以通过这个通道直接发送HTTP请求数据，获取响应数据了。

HTTP协议进阶
代理
HTTP的代理服务器既是Web服务器，又是Web客户端。

使用代理可以“接触”到所有流过的HTTP流量，代理可以对其进行监视和修改。常见的就是对儿童过滤一些“成人”内容；网络工程师会利用代理服务器来提高安全性，它可以限制哪些应用层的协议数据可以通过，过滤“病毒”等数据；代理可以存储缓存的文件，直接返回给访问者，无需请求原始的服务器资源；对于访问慢速网络上的公共内容时，可以假扮服务器提供服务，从而提高访问速度；这被称为反向代理；可以作为内容路由器，如对付费用户，则将请求导到缓存服务器，提高访问速度；可以将页面的语言转换到与客户端相匹配，这称为内容转码器; 匿名代理会主动从HTTP报文中删除身份相关的信息，如User-Agent, Cookie等字段。
现实中，请求通过以下几种方式打到代理服务器上去：

报文每经过一个中间点（代理或网关），都需要在首部via字段的末尾插入一个可以代表本节点的独特的字符串，包含实现的协议版本和主机地址。注意图中的via字段。

请求和响应的报文传输路径通常都是一致的，只不过方向是相反的。因此，响应报文上的via字段表示的中间节点的顺序是刚好相反的。
缓存
当有很多请求访问同一个页面时，服务器会多次传输同一份数据，这些数据重复地在网络中传输着，消耗着大量带宽。如果将这些数据缓存下来，就可以提高响应速度，节省网络带宽了。
大部分缓存只有在客户端发起请求，并且副本已经比较旧的情况下才会对副本的新鲜度进行检测。最常用的请求首部是If-Modified-Since, 如果在xx时间(此时间即为If-Modified-Since的值)之后内容没有变化，服务器会回应一个304 Not Modified. 否则，服务器会正常响应，并返回原始的文件数据，而这个过程中被称为再验证命中。
再验证可能出现命中或未命中的情况。未命中时，服务器回复200 OK，并且返回完整的数据；命中时，服务器回复304 Not Modified; 还有一种情况，缓存被删除了，那么根据响应状态码，缓存服务器也会删除自己缓存的副本。
顺带提一句，若要在项目中使用缓存，就一定要关注缓存命中比例。若命中比例不高，就要重新考虑设置缓存的必要性了。
缓存服务器返回响应的时候，是基于已缓存的服务器响应的首部，再对一些首部字段做一些微调。比如向其中插入新鲜度信息（如Age, Expires首部等），而且通常会包含一个via首部来说明缓存是由一个缓存代理提供的。注意，这时不要修改Date字段，它表示原始服务器最初构建这条响应的日期。
HTTP通过文档过期机制和服务器再验证机制保持已缓存数据和服务器间的数据充分一致。
文档过期通过如下首部字段来表示缓存的有效期：

当上面两个字段暗示的过期时间已到，需要向服务器再次验证文档的新鲜度。如果这时缓存仍和服务器上的原始文档一致，缓存只需要更新头部的相关字段。如上表中提到的Expires字段等。
为了更好的节省网络流量，缓存服务器可以通过相关首部向原始服务器发送一个条件GET请求, 这样只有在缓存真正过期的情况下，才会返回原始的文档，否则只会返回相关的首部。条件GET请求会用到如下的字段：

cookie
cookie是服务器“贴在”客户端身上的标签，由客户端维护的状态片段，并且只会回送给合适的站点。
有两类cookie: 会话cookie、持久cookie. 会话cookie在退出浏览器后就被删除了；而持久cookie则保存在硬盘中，计算机重启后仍然存在。
服务器在给客户端的响应字段首部加上Set-cookie或Set-cookie2, 值为名字=值的列表，即可以包含多个字段。当下次浏览器再次访问到相同的网站时，会将这些字段通过Cookie带上。cookie中保留的内容是服务器给此客户端打的标签，方便服务进行追踪的识别码。浏览器会将cookie以特定的格式存储在特定的文件中。
浏览器只会向产生这条cookie的站点发生cookie. Set-cookie字段的值会包含domain这个字段，告知浏览器可以把这条cookie发送给给相关的匹配的站点。path字段也是相似的功能。如i浏览器收到如下的cookie:
Set-cookie: user="mary"; domain="stefno.com"
那么浏览器在访问任意以stefno.com结尾的站点都会发送：
Cookie: user="mary"
实体和编码
响应报文中的body部分传输的数据本质上都是二进制。我们从上面的报文数据也可以看出来，都是用十六进制数来表示，关键是怎么解释这块内容。如果Content-Type定义是text/plain, 那说明body内容就是文本，我们直接按文本编码来解释；如果Content-Type定义是image/png, 说明body部分是一幅图片，那我们就按图片的格式去解释数据。
Content-Length标示报文主体部分的数据长度大小，如果内容是压缩的，那它表示的就是压缩后的大小。另外，Content-Length在长连接的情况下，可以对多个报文进行正确地分段。所以，如果没有采用分块编码，响应数据中必须带上Content-Length字段。分块编码的情形中，数据被拆分成很多小块，每块都有大小说明。因此，任何带有主体部分的报文（请求或是响应）都应带上正确的Content-Length首部。
HTTP的早期版本采用关闭连接的方式来划定报文的结束。这带来的问题是显而易见的：客户端并不能分清是因为服务器正常结束还是中途崩溃了。这里，如果是客户端用关闭来表示请求报文主体部分的结束，是不可取的，因为关闭之后，就无法获取服务器的响应了。当然，客户端可以采用半关闭的方式，只关闭数据发送方向，但是很多服务器是不识别的，会把半关闭当成客户端要成服务器断开来处理。
HTTP报文在传输的过程中可能会遭到代理或是其他通信实体的无意修改，为了让接收方知道这种情况，服务器会对body部分作一个md5, 并把值放到Content-MD5这个字段中。但是，如果中间的代理即修改了报文主体，又修改了md5, 就不好检测了。因此规定代理是不能修改Content-MD5首部的。这样，客户端在收到数据后，先进行解码，再算出md5, 并与Content-MD5首部进行比较。这主要是防止代理对报文进行了无意的改动。
HTTP在发送内容之前需要对其进行编码，它是对报文主体进行的可逆变换。比如将报文用gzip格式进行压缩，减少传输时间。常见的编码类型如下：

当然，客户端为了避免服务器返回自己不能解码的数据，请求的时候，会在Accept-Encoding首部里带上自己支持的编码方式。如果不传输的话，默认可以接受任何编码方式。
上面提到的编码是内容编码，它只是在响应报文的主体报文将原始数据进行编码，改变的是内容的格式。还有另一种编码：传输编码。它与内容无关，它是为了改变报文数据在网络上传输的方式。传输编码是在HTTP 1.1中引入的一个新特性。
通常，服务器需要先生成数据，再进行传输，这时，可以计算数据的长度，并将其编码到Content-Length中。但是，有时，内容是动态生成的，服务器希望在数据生成之前就开始传输，这时，是没有办法知道数据大小的。这种情况下，就要用到传输编码来标注数据的结束的。
HTTP协议中通过如下两个首部来描述和控制传输编码：



字段
含义
典型值




Transfer-Encoding
发送方告知接收方，我方已经进行了何种传输编码
chuncked 分块编码


TE
请求方告知服务器可以用哪种传输编码
trailers, chuncked 接受分块编码，并且愿意接受在报文结尾上的拖挂



分块编码的报文形式是这样的：

每个分块包含一个长度值（十六进制，字节数）和该分块的数据。<CR><LF>用于区隔长度值和数据。长度值不包含分块中的任何<CR><LF>序列。最后一个分块，用长度值0来表示结束。注意报文首部包含一个Trailer: Content-MD5, 所以在紧跟着最后一个报文结束之后，就是一个拖挂。其他如，Content-Length, Trailer, Transfer-Encoding也可以作为拖挂。
内容编码和传输编码是可以结合起来使用的。

国际化支持
HTTP为了支持国际化的内容，客户端要告知服务器自己能理解的何种语言，以及浏览器上安装了何种字母表编码算法。这通过Accept-Charset和Accept-Language首部实现。
比如：
Accept-Language: fr, en;q=0.8
Accept-Charset: iso-8859-1, utf-8
表示：客户端接受法语(fr, 优先级默认为1.0）、英语（en, 优先级为0.8），支持iso-8859-1, utf-8两种字符集编码。服务器则会在Content-Type首部里放上charset.
本质上，HTTP报文的body部分存放的就是一串二进制码，我们先把二进制码转换成字符代码（如ascii是一个字节表示一个字符，而utf-8则表示一个字符的字节数不定，每个字符1~6个字节），之后，用字符代码去字符集中找到对应的元素。
比较常见的字符集是US-ASCII: 这个字符集是所有字符集的始祖，早在1968年就发布了标准。ASCII码的代码值从0到127, 只需要7个bit位就可以覆盖代码空间。HTTP报文的首部、URL使用的字符集就是ASCII码。可以再看下上文报文分析部分的acsii码集。
US-ASCII是把每个字符编码成固定的7位二进制值。UTF-8则是无固定的编码方案。第一个字节的高位用来表示编码后的字符所用的字节数（如果所用的字节数是5，则第一个字节前5bit都是1，第6bit是0），所需的后续的字节都含有6位的代码值，前两个bit位是用10标识。

举个例子，汉字“严”的Unicode编码为4E25(100111000100101), 共有15位，落在上表中的第三行，因此“严”的编码就需要三个字节。将100111000100101填入上表中的c位即可。因此，严的UTF-8编码是11100100 10111000 10100101，转换成十六进制就是E4B8A5. 比如我在谷歌搜索框里搜索“严”字，google发出的请求如下：
https://www.google.com.hk/search?q=%E4%B8%A5&oq=%E4%B8%A5&aqs=chrome..69i57j0l5.3802j0j4&sourceid=chrome&ie=UTF-8&gws_rd=cr
q=%E4%B8%A5这个就是搜索的词了。
重定向与负载均衡
Web内容通常分散地分布在很多地方，这可以防止“单点故障”，万一某个地方发生地震了，机房被毁了，那还有其他地方的机房可以提供服务。一般都会有所谓的“双活”，“多活”，所谓狡兔三窟嘛。
这样，用户的请求会根据负载均衡的原则，被重定向到它应该去的地方。
HTTP重定向
服务器收到客户端请求后，向客户端返回一条带有状态码302重定向的报文，告诉他们应该去其他的地方试试。web站点将重定向看成一种简单的负载均衡策略来使用，重定向服务器找到可用的负载最小的机器，由于服务器知道客户端的地址，理论上来说，可以做到最优的重定向选择。
当然，缺点也是显而易见的，由于客户端要发送两次请求，因此会增加耗时。
DNS重定向
DNS将几个IP地址关联到一个域上，采用算法决定返回的IP地址。可以是简单的轮转；也可以是更高级的算法，如返回负载最轻的服务器的IP地址，称为负载均衡算法；如果考虑地理位置，返回给客户端最近位置的地址，称为邻接路由算法；还有一种是绕过出现故障的地址，称为故障屏蔽算法。
DNS服务器总是会返回所有的IP地址，但是DNS客户端一般只会使用第一个IP地址，而且会缓存下来，之后会一直用这个地址。所以，DNS轮转通常不会平衡单个客户端的负载。但是，由于DNS服务器对于不同的请求，总是会返回轮转后的IP地址列表，因此，会把负载分散到多个客户端。
HTTP连接
HTTP连接是HTTP报文传输的关键通道。
并行连接
对于一个页面上同时出现多个对象的时候，如果浏览器并行地打开多个连接，同时去获取这些对象，多个连接的TCP握手时延可以进行重叠，速度会快起来。
如一个包含3张图片的页面，浏览器要发送4次HTTP请求来获取页面。1个用于顶层的HTML页面，3个用于图片。如果采用串行方式，那么连接时延会进行叠加。

采用并行连接之后：

但是并行连接也不绝对提升速度，如果一个页面有数百个内嵌对象，那要启动数百个连接，对服务器的性能也是非常大的挑战。所以，通常浏览器会限制并行连接的总数据在一个较小的值，通常是4个，而且服务端可以随意关闭客户端超量的连接。
另一方面，如果客户端网络带宽较小，每个连接都会去争抢有限的带宽，每个连接都会获取较小的速度，即每个对象都会以较小的速度去加载。这样，并行连接带来的速度提升就会比较小，甚至没有提升。
持久连接
HTTP keep-alive机制
我们知道HTTP请求是“请求-应答”模式，每次请求-应答都要新建一个连接，完成之后要断开连接。HTTP是无状态的，连接之间没有任何关系。
HTTP是应用层协议，TCP是传输层协议。HTTP底层仍然采用TCP进行传输数据。TCP为HTTP提供了一层可靠的比特传输通道。HTTP一般交换的数据都不大，而每次连接都要进行TCP三次握手，很大一部分时间都消耗在这上面，有时候甚至能达到50%。如果能复用连接，就可以减少由于TCP三次握手所带来的时延。
HTTP 1.1默认开启keep-alive机制，从上面抓到的包也可以看到。这样，数据传输完成之后保持TCP连接不断开，之后同域名下复用连接，继续用这个通道传输数据。服务器在响应一个请求后，可以保持这个连接keep-alive timeout的时间，在这个时间内没有请求，则关闭此连接；否则，重新开始倒计时keep-alive timeout时间。

HTTP有keep-alive机制，目的是可以在一个TCP
连接上传输多个HTTP事务，以此提高通信效率。底层的TCP其实也有keep-alive机制，它是为了探测TCP连接的活跃性。TCP层的keepalive可以在任何一方设置，可以是一端设置、两端同时设置或者两端都没有设置。新建socket的时候需要设置，从而使得协议栈调用相关函数tcp_set_keepalive，来激活连接的keep-alive属性。
当网络两端建立了TCP连接之后，闲置（双方没有任何数据流发送往来）时间超过tcp_keepalive_time后，服务器内核就会尝试向客户端发送侦测包，来判断TCP连接状况(有可能客户端崩溃、强制关闭了应用、主机不可达等等)。如果没有收到对方的回答(ack包)，则会在 tcp_keepalive_intvl后再次尝试发送侦测包，直到收到对方的ack,如果一直没有收到对方的ack,一共会尝试 tcp_keepalive_probes次，每次的间隔时间在这里分别是15s, 30s, 45s, 60s, 75s。如果尝试tcp_keepalive_probes次后,依然没有收到对方的ack包，则会丢弃该TCP连接。TCP连接默认闲置时间是2小时，一般设置为30分钟足够了。
管道化连接
在keep-alive的基础上，我们可以做地更进一步，在响应到达之前，我们将多条请求按序放入请求队列，服务端在收到请求后，必须按照顺序对应请求的响应。但由于网络环境非常复杂，因此即使请求是按顺序发送的，也不一定是按顺序到达服务端的。而且就算是服务端按序处理的，也不一定是按序返回给客户端，所以最好是在响应中附带一些可以标识请求的参数。
为了安全起见，管道化的连接只适合“幂等”的请求，一般我们认为：GET/HEAD/PUT/DELETE/TRACE/OPTIONS等方法都是幂等的。
小结
以上，就是所有HTTP的通信细节了，足够在日常开发 作中使用了。更多没有涉及的细节可以在用到的时候再去仔细研究。
文章看完了，不知道你对HTTP的理解有没有更上一层楼？欢迎一起交流探讨。

参考资料
【http长连接】https://www.cnblogs.com/cswuyg/p/3653263.html
【http/tcp keep alive】https://segmentfault.com/a/1190000012894416
【http/tcp keep alive】http://www.nowamagic.net/academy/detail/23350305
【http/tcp keep alive】https://laravel-china.org/articles/8020/on-the-keep-alive-and-tcp-keep-alive-in-the-http-protocol
【tcp keep alive】http://blog.51cto.com/zxtong/1788252
【http权威指南】https://book.douban.com/subject/10746113/
【HTTP状态码】https://www.cnblogs.com/starof/p/5035119.html
【HTTP协议】https://www.cnblogs.com/ranyonsue/p/5984001.html
【HTTP状态分类】http://www.runoob.com/http/http-status-codes.html
【url编码】http://www.ruanyifeng.com/blog/2010/02/url_encoding.html
https://www.cnblogs.com/qcrao-2018/p/10285348.html
**************************************************
利用Topshelf把.NET Core Generic Host管理的应用程序部署为Windows服务
背景
2019第一篇文章。
此文源于前公司在迁移项目到.NET Core的过程中，希望使用Generic Host来管理定时任务程序时，没法部署到Windows服务的问题，而且官方也没给出解决方案，只能关注一下官方issue #809 等他们方解决了。
官方文档只提供了一个《在 Windows 服务中托管 ASP.NET Core》的方案，可以使用Microsoft.AspNetCore.Hosting.WindowsServices类库来把Web应用部署为Windows服务。但是ASP.NET Core虽然是控制台程序，但是它本身是使用了含有HTTP管道的Web Host来负责应用程序的生命周期管理，用它来作为定时任务的话，会有很多不必要的工作负载，例如占用端口、增加了很多依赖等等。
官方意识到这个问题之后，在.NET Core 2.1版本新增了Generic Host通用主机，剥离了原来WebHost的Http管道相关的API，源码中可以发现Web Host已经基于Generic Host实现。它才是作为纯粹定时任务程序的最佳拍档。
但是由于Generic Host本身非常简单，用它运行的程序设置在注册为Windows服务启动之后会自动停止。研究很久之后才知道，想在Windows上启动服务，还是不能像Linux上那么简单——
于是尝试结合Topshelf来创建Windows服务，最终成功了。
实现方法

先实现IHostLifetime接口来接管应用程序的生命周期，其实就是用空的实现来替换掉默认的ConsoleLifetime，这样就可以在之后由Topshelf框架内部去管理生命周期。

    internal class TopshelfLifetime : IHostLifetime
    {
        public TopshelfLifetime(IApplicationLifetime applicationLifetime, IServiceProvider services)
        {
            ApplicationLifetime = applicationLifetime ?? throw new ArgumentNullException(nameof(applicationLifetime));
        }

        private IApplicationLifetime ApplicationLifetime { get; }

        public Task WaitForStartAsync(CancellationToken cancellationToken)
        {
            return Task.CompletedTask;
        }

        public Task StopAsync(CancellationToken cancellationToken)
        {
            return Task.CompletedTask;
        }
    }

然后实现IHostedService接口，把后台任务逻辑写到StartAsync方法中，参见官方文档《在 ASP.NET Core 中使用托管服务实现后台任务》，本文示例使用定时写入文本到一个文件来测试定时任务是否成功运行。

    internal class FileWriterService : IHostedService, IDisposable
    {
        private static string path = Path.Combine(AppDomain.CurrentDomain.BaseDirectory, @"test.txt");

        private Timer _timer;

        public Task StartAsync(CancellationToken cancellationToken)
        {
            if (cancellationToken.IsCancellationRequested) return Task.FromCanceled(cancellationToken);

            _timer = new Timer(
                (e) => WriteTimeToFile(),
                null,
                TimeSpan.Zero,
                TimeSpan.FromSeconds(10));

            return Task.CompletedTask;
        }

        public void WriteTimeToFile()
        {
            if (!File.Exists(path))
            {
                using (var sw = File.CreateText(path))
                {
                    sw.WriteLine(DateTime.Now);
                }
            }
            else
            {
                using (var sw = File.AppendText(path))
                {
                    sw.WriteLine(DateTime.Now);
                }
            }
        }

        public Task StopAsync(CancellationToken cancellationToken)
        {
            _timer?.Change(Timeout.Infinite, 0);

            return Task.CompletedTask;
        }

        public void Dispose()
        {
            _timer?.Dispose();
        }
    }

构建Generic Host，在ConfigureServices方法中注册TopshelfLifetime，并且注册一个托管服务FileWriterService，就能完成Generic Host的简单构建，当然完整的项目应该还包含配置、日志等等。最后，使用Topshelf来接管Generic Host，创建Windows服务。

    internal class Program
    {
        private static void Main(string[] args)
        {
            var builder = new HostBuilder()
                .ConfigureServices((hostContext, services) =>
                {
                    services.AddSingleton<IHostLifetime, TopshelfLifetime>();
                    services.AddHostedService<FileWriterService>();
                });

            HostFactory.Run(x =>
            {
                x.SetServiceName("GenericHostWindowsServiceWithTopshelf");
                x.SetDisplayName("Topshelf创建的Generic Host服务");
                x.SetDescription("运行Topshelf创建的Generic Host服务");

                x.Service<IHost>(s =>
                {
                    s.ConstructUsing(() => builder.Build());
                    s.WhenStarted(service =>
                    {
                        service.Start();
                    });
                    s.WhenStopped(service =>
                    {
                        service.StopAsync();
                    });
                });
            });
        }
    }

最后发布应用程序，并安装到Windows服务。

以管理员权限开启终端，执行命令：
  dotnet publish -c release -r win-x64
  
  cd path-to-project/bin/release/netcoreapp2.1/win-x64/publish

  ./project-name install

  net start GenericHostWindowsServiceWithTopshelf


这样这个Windows服务就启动了！查看输出文件，可以看到定时写入成功，服务也一直没关闭~

示例代码
https://github.com/ElderJames/GenericHostWindowsServiceWithTopshelf
参考链接
官方文档《.NET 通用主机》
官方文档《在 ASP.NET Core 中使用托管服务实现后台任务》

https://www.cnblogs.com/ElderJames/p/Using-Topshelf-To-Deploy-Net-Core-Generic-Host-App-To-Windows-Services.html
**************************************************
ML.NET 0.9特性简介
ML.NET 0.9已于上周发布，距离上次0.8版本的发布只有一个多月，此次增加的新特性主要包括特征贡献计算，模型可解释性增强，ONNX转换对GPU的支持，Visual Studio ML.NET项目模板预览，以及API改进。
特征贡献计算
特征贡献计算(Feature Contribution Calculation)通过决定每个特征对模型分数的贡献，从而显示哪些特征在对特别个体的数据样本的模型预测最有影响力。
当你面临历史数据中有许多特征时而又想选择使用最重要的特征时，特征贡献计算显得十分重要。因为使用太多的特征(尤其是包含对模型没有影响的特征)会减少模型的性能与准确性。因此，使用特征贡献计算你可以从原始特征集中识别最有影响力的正向与负向的贡献。
示例代码：
// 创建特征贡献计算器
// 对已有训练模型参数的所有特征进行计算贡献

var featureContributionCalculator = mlContext.Model.Explainability.FeatureContributionCalculation(model.Model, model.FeatureColumn, numPositiveContributions: 11, normalize: false);

// FeatureContributionCalculatingEstimator可被用作管道中的一个步骤
// 被FeatureContributionCalculatingEstimator保存的特征将在FeatureContribution列中

var pipeline = mlContext.Model.Explainability.FeatureContributionCalculation(model.Model, model.FeatureColumn, numPositiveContributions: 11)
    .Append(mlContext.Regression.Trainers.OrdinaryLeastSquares(featureColumn: "FeatureContributions"));
输出结果可下：
The output of the above code is:

  Label   Score   BiggestFeature         Value   Weight   Contribution

  24.00   27.74   RoomsPerDwelling        6.58    98.55   39.95
  21.60   23.85   RoomsPerDwelling        6.42    98.55   39.01
  34.70   29.29   RoomsPerDwelling        7.19    98.55   43.65
  33.40   27.17   RoomsPerDwelling        7.00    98.55   42.52
对于特征选取的模型可解释性的增强
除了特征贡献计算之外，排列特征重要性(PFI)与广义加性模型(GAM)也有加强。

排列特征重要性支持大多数学习任务：回归，二元分类，多元分类与排序。
排列特征重要性允许你在特征重要性分数上计算置信区间，以便可以得到更好的平均值估计。
广义加性模型支持特征贡献计算，以便你可以迅速看到哪些特征驱动个体的预测。

增加对ONNX转换的GPU支持

在ML.NET 0.9中通过集成高性能的ONNX运行时库添加了使用激活GPU的CUDA 10.0运行ONNX模型的功能。ONNX模型的GPU支持现在已经可以在Windows 64位系统上使用，不久之后将支持Linux与Mac系统。
新的Visual Studio ML.NET项目模板预览
Visual Studio项目模板现在推出了支持ML.NET的预览版本。下载地址

模板涵盖以下场景：

ML.NET控制台应用程序
ML.NET模型类库

其它API的改进
简化文本数据加载
ML.NET 0.9以前你需要显示的标明列名：
var mlContext = new MLContext();

var reader = mlContext.Data.CreateTextReader(new[] {
        new TextLoader.Column("IsOver50K", DataKind.BL, 0),
        new TextLoader.Column("Workclass", DataKind.TX, 1)
    },hasHeader: true
);
var dataView = reader.Read(dataPath);
现在你可以直接使用泛型：
var mlContext = new MLContext();

var dataView = mlContext.Data.ReadFromTextFile<InspectedRow>(dataPath, hasHeader: true);

private class InspectedRow
{
    [LoadColumn(0)]
    public bool IsOver50K { get; set; }
    [LoadColumn(1)]
    public string Workclass { get; set; }
}
获取预测置信因子
通过Calibrator Estimators，除了在评估模型质量时可以获得分数列之外，还可以得到置信因子。
例如，你可以获得每个预测值的概率：
Score - 0.458968    Probability 0.4670409
Score - 0.7022135   Probability 0.3912723
Score 1.138822      Probability 0.8703266
新的键-值匹配估测器及转换
新特性替换了TermLookupTransform，同时提供了指定值之间匹配的新方法。你可以指定键列与值列的匹配关系，但需保证两者数量一致。
其它的改进与变化

允许ML.NET在Windows Nano容器及Windows机器上运行，而无需安装Visual C++运行时。
在包含模型信息的DataView构造器中提供元数据支持，比如被编码为元数据的评估指标可以通过代码解析出来，由此能够使用任何工具进行可视化。


https://www.cnblogs.com/kenwoo/p/10284906.html
**************************************************
开发框架数据库配置的几种应用场景
在我的开发框架系列中，底层数据库的配置处理都是差不多的，框架整体支持SQLServer、DB2、MySql、ODP.NET（Oracle）、PostgreSQL、SQLite、SqlEx等数据库的，往往客户在使用框架的时候会问，框架支持哪些数据库、各种数据库配置信息、如何实现数据库分库处理、如何同时支持SQLServer和Oracle等数据库支持、如何实现数据库连接字符串加密等问题，本篇随笔逐一进行介绍。
1、框架支持的数据库介绍
框架底层数据库访问采用了微软企业库实现，因此在处理多种数据库访问的时候，能够提供统一的访问处理操作，同时对不同的数据库支持操作也是非常不错的。下图是框架底层数据库的支持情况。

采用了微软企业库Enterprise Library作为我们底层的数据库访问模块后，对于多种数据库的访问操作，就会统一采用这个企业库的数据库访问对象，操作起来非常一致，为了对不同数据库的常规增删改查等一些操作进行进一步的封装，以达到简化代码的目的，因此我们可以为每个不同的数据库定义一个数据访问操作基类，以便实现一些不同数据库差异性的处理，但是它们还是有一个共同的数据访问基类。
采用不同的数据库，我们需要为不同数据库的访问层进行生成处理，如为SQLServer数据的表生成相关的数据访问层DALSQL，里面放置各个表对象的内容，不过由于采用了相关的继承类处理和基于数据库的代码生成，需要调整的代码很少。

这样整合多种数据库支持的底层后，整个数据访问的架构设计如下所示。

 
2、各种数据库配置信息
对于默认支持的SQLServer数据库，它的连接字符串如下所示。

<?xml version="1.0"?>
<configuration>
  <configSections>
    <section name="dataConfiguration" type="Microsoft.Practices.EnterpriseLibrary.Data.Configuration.DatabaseSettings, Microsoft.Practices.EnterpriseLibrary.Data"/>
  </configSections>
  <connectionStrings>
    <!--SQLServer数据库的连接字符串-->
    <add name="sqlserver" providerName="System.Data.SqlClient" connectionString="Persist Security Info=False;Data Source=(local);Initial Catalog=WinFramework;Integrated Security=SSPI"/>
  </connectionStrings>
  <dataConfiguration defaultDatabase="sqlserver">
  </dataConfiguration>
</configuration>

上面的sqlserver数据库连接信息是采用信任模式配置的，如果我们基于局域网，那么需要配置对应的IP或者sa用户名和密码的方式，配置信息如下所示。

<add name="sqlserver2" providerName="System.Data.SqlClient" connectionString="Data Source=192.168.1.10;Initial Catalog=CRM;Persist Security Info=True;User ID=sa;Password=123456"/>

不过对于一些扩展支持的数据库，我们还需要添加一些映射处理，如对于MySQL的支持，我们需要添加连接字符串：

  <!--MySQL数据库的连接字符串-->
    <add name="mysql" providerName="MySql.Data.MySqlClient" connectionString="Server=localhost;Database=WinFramework;Uid=root;Pwd=123456;"/>

还需要添加ProviderMappings的支持，如下所示的XML。

  <dataConfiguration defaultDatabase="mysql">
    <providerMappings>
      <add databaseType="EntLibContrib.Data.MySql.MySqlDatabase, EntLibContrib.Data.MySql" name="MySql.Data.MySqlClient" />
    </providerMappings>
  </dataConfiguration>

下面我列出所有不同数据库的连接字符串以及映射关系的一个完整版本，供参考。

<?xml version="1.0"?>
<configuration>
  <configSections>
    <section name="dataConfiguration" type="Microsoft.Practices.EnterpriseLibrary.Data.Configuration.DatabaseSettings, Microsoft.Practices.EnterpriseLibrary.Data"/>
  </configSections>
  <connectionStrings>
    <!--Sqlserver数据库的连接字符串-->
    <add name="sqlserver" providerName="System.Data.SqlClient" connectionString="Persist Security Info=False;Data Source=(local);Initial Catalog=WinFramework;Integrated Security=SSPI"/>

    <!--PostgreSQL数据库的连接字符串-->
    <add name="npgsql" providerName="Npgsql" connectionString="Server=localhost;Port=5432;Database=postgres;User Id=postgres;Password=123456"/>
    <!--MySQL数据库的连接字符串-->
    <add name="mysql" providerName="MySql.Data.MySqlClient" connectionString="Server=localhost;Database=WinFramework;Uid=root;Pwd=root;"/>
    <!--路径符号|DataDirectory|代表当前运行目录-->
    <add name="access" providerName="System.Data.OleDb" connectionString="Provider=Microsoft.Jet.OLEDB.4.0;Data Source=|DataDirectory|\WinFramework.mdb;User ID=Admin;Jet OLEDB:Database Password=;" />
    <!--sqlite数据库字符串，路径符号|DataDirectory|代表当前运行目录-->
    <add name="sqlite"  providerName="System.Data.SQLite" connectionString="Data Source=|DataDirectory|\WinFramework.db;Version=3;" />
    <!--Oracle数据库的连接字符串-->
    <add name="oracle" providerName="System.Data.OracleClient" connectionString="Data Source=orcl;User ID=win;Password=win"/>
    <!--达梦数据库的连接字符串-->
    <add name="Dm" providerName="Dm" connectionString="Server=localhost;User ID=SYSDBA;PWD=SYSDBA;Database=WINFRAMEWORK;" />
    <!--IBM DB2数据库的连接字符串-->
    <add name="db2" providerName="IBM.Data.DB2"    connectionString="database=whc;uid=whc;pwd=123456"/>
    <!--采用OdpNet方式的Oracle数据库的连接字符串-->
    <add name="oracle2"    providerName="Oracle.DataAccess.Client"    connectionString="Data Source=orcl;User id=win;Password=win;" />
  </connectionStrings>
  <dataConfiguration defaultDatabase="sqlserver">
    <providerMappings>
      <add databaseType="EntLibContrib.Data.PostgreSql.NpgsqlDatabase, EntLibContrib.Data.PostgreSql" name="Npgsql" />
      <add databaseType="EntLibContrib.Data.MySql.MySqlDatabase, EntLibContrib.Data.MySql" name="MySql.Data.MySqlClient" />
      <add databaseType="EntLibContrib.Data.SQLite.SQLiteDatabase, EntLibContrib.Data.SqLite" name="System.Data.SQLite" />
      <add databaseType="EntLibContrib.Data.Dm.DmDatabase, EntLibContrib.Data.Dm" name="Dm" />
      <add databaseType="EntLibContrib.Data.DB2.DB2Database, EntLibContrib.Data.DB2" name="IBM.Data.DB2" />
      <add databaseType="EntLibContrib.Data.OdpNet.OracleDatabase, EntLibContrib.Data.OdpNet" name="Oracle.DataAccess.Client" />
    </providerMappings>
  </dataConfiguration>
  <appSettings>
    <!--组件的数据库类型：access、sqlserver、sqlite、oracle等，默认为sqlserver可不写-->
    <add key="ComponentDbType" value="sqlserver"/>
  </appSettings>
  <startup useLegacyV2RuntimeActivationPolicy="true">
    <supportedRuntime version="v4.0" sku=".NETFramework,Version=v4.0"/>
    <supportedRuntime version="v2.0.50727"/>
  </startup>
</configuration>

里面包括了支持的各种数据库的连接字符串的写法。
 
3、如何实现数据库分库处理、如何同时支持SQLServer和Oracle等数据库支持 
由于整个框架的设计抽象了多种数据库的处理模型，因此数据库分库处理实现也是比较方便的，数据库的分库处理和同时支持SQLServer和Oracle等数据库的操作是类似的，他们都是对多个数据库（包括不同各类型）进行访问处理。
之前我在随笔《Winform开发框架中实现多种数据库类型切换以及分拆数据库的支持》这里也介绍了具体的实现处理，其实我们使用的同时支持多数据库的操作代码是比较简单的，我们可以在代码里面通过调用BLL层类的接口SetConfitName来指定特定的数据库，如下代码所示。

            //指定业务类的数据库配置
            BLLFactory<Asset>.Instance.SetConfigName("workflow");  
            BLLFactory<StoreAddress>.Instance.SetConfigName("workflow"); 

有时候，可能BLL对象有可能出现相同的情况，但是需要访问不同库里面的表对象，那么我们可以在使用后恢复默认的配置信息。

            BLLFactory<DictData>.Instance.SetConfigName("workflow");//使用业务库
            Dictionary<string, string> dict = BLLFactory<DictData>.Instance.GetDictByDictType(dictTypeName);
            BLLFactory<DictData>.Instance.SetConfigName(null);//恢复默认

以上代码就是先访问workflow配置的数据库信息，获取字典信息后恢复默认的数据库信息。
上面那种方式是对于同种类型数据库的接口切换，如果不同的数据库类型，如一个是SQLServer，一个是Oracle，那么就可以多指定一个参数即可，如下代码所示。

            //指定使用oracle类型的数据库配置
            BLLFactory<DictData>.Instance.SetConfigName("workflow", "oracle");//使用业务库
            Dictionary<string, string> dict = BLLFactory<DictData>.Instance.GetDictByDictType(dictTypeName);
            BLLFactory<DictData>.Instance.SetConfigName(null);//恢复默认

虽然我们一般使用一个库，但是如果是分库，或者要同时支持多个数据库类型，基本上处理还是很方便的。
如果对于通用类型的数据库处理，我们可以使用公用类库里面的CommonDAL类进行处理。这个类库可以很方便的处理视图、存储过程、或者常规的接口查询操作，不需要和具体的实体类绑定的接口。
它的定义如下所示。

 
4、数据库字符串加密处理
很多情况下，我们为了部署应用，需要公开数据库连接字符串信息，但是我们又不想让使用者很容易的获取到我们的连接字符串里面的用户名和密码敏感信息，这时候连接字符串加密就是比较必要的了。
处理方式就是我们创建一个工具，使用自己知道的加解密规则来处理连接字符串的加解密处理。

处理的过程大概如下所示。
1）找到app.config文件，打开内容编辑。2）找到数据库（如SQLServer）连接字符串的connectionString字符串，如下所示。    <add name="sqlserver" providerName="System.Data.SqlClient" connectionString="Persist Security Info=False;Data Source=(local);Initial Catalog=CRM;Integrated Security=SSPI"/>3）提取里面的connectionString字符串部分，放到上面的加密软件里面进行加密，然后把加密内容替换connectionString字符串，变为如下所示的配置信息。    <add name="sqlserver" providerName="System.Data.SqlClient" connectionString="9Fs/vPhm24CYa0mXCLAMYOJmbBHq/qQAjdbVdbeOhS5L0d8WGhHUR3iIyFZydEV8cPmlPHfDTnwJZMr9xkMAxuNtPKUsIdKTjlWInpf+Vc+UD2gtYIE3FnvL06KcHzX+"/>
4）保存文件，配置加密字符串完成。
这个配置信息在框架的处理的时候有对应的解密处理规则，可以正常解析加密字符串即可。
如果要了解或者修改其中的对应解密处理操作，可以定位扩展公用类库里面数据库访问层，如下所示。

定位到对应的数据库访问类，然后找到下面的对应函数了解即可。

 
https://www.cnblogs.com/wuhuacong/p/10283050.html
**************************************************
MapServer Tutorial——MapServer7.2.1教程学习——第一节用例实践：Example1.1 A map with single layer
MapServer Tutorial——MapServer7.2.1教程学习——第一节用例实践：Example1.1 A map with single layer
一、前言
　　开始MapServer用例实践之旅，做项目算是可以比喻为考试，但是考试之前，还是以做练习题模拟考为主。下面实践一下官网的第一个例子：Example1.1 A map with single layer（官网地址：https://www.mapserver.org/tutorial/example1-1.html#example1-1）
二、简介
　　1.打开案例相关介绍
　　　　1.1.MapServer能够创建图像并投影到本地目录或者直接的投影到你所使用请求的浏览器上面。本用例你可以通过浏览器直接打开：http://localhost/cgi-bin/mapserv?map=/ms4w/apps/tutorial/htdocs/example1-1.map&layer=states&mode=map查看。
　　　　　　注意：这个位置的地址是按照官网上面翻译的地址，如果是根据自己的个人安装环境以及建站配置等设置的，需要修改。
　　　　1.2.当前URL分为两个部分
　　　　　　第一部分："http://localhost/cgi-bin/mapserv?"部分，调用MapServer CGI进程。如果你在浏览器中打开http://localhost/cgi-bin/mapserv?，会出现“No query information to decode. QUERY_STRING is set, but empty”。
　　　　　　第二部分：URL中的三个参数 map、layer、mode
　　　　　　　　map=/ms4w/apps/tutorial/htdocs/example1-1.map：此处表示告诉MapServer的CGI进程去解析哪一个map文件。可以是绝对路径，也可以是相对路径（相对路径是针对mapserv.exe而言的）。
　　　　　　　　layer=states：此处告诉MapServer“打开”layer状态，回调时，我们命名layer对象为“states”
　　　　　　　　mode=map：告诉MapServer对mapfile文件的输出格式，这里是告诉MapServer直接将图像投影到浏览器，无需先在服务器端创建零时的图像。
　　　　　　注意：mapserver模式cgi变量采用的值不是map。例如，如果使用mode=browse，mapserver将把映像转储到服务器上的临时目录。浏览模式现在不起作用，但稍后我们将再次实践。
　　2.mapfile的文件结构
　　　　官网用例“Example1.1 A map with single layer”的mapfile文件结构请查看此链接：https://www.mapserver.org/tutorial/example1-1-map.html#example1-1-map
　　　　
　　　　MapFile文件介绍请查看此链接：https://www.mapserver.org/mapfile/index.html#mapfile
　　　　　　后续再添加一篇文章，对MapFile做详细介绍。
　　　　Mapefile是MapServer的基础配置机制（个人理解就是告诉MapServer的站点是如何运行的）。它有对象组成，如：LAYERT。每个对象有自己的关键之，并可以包含其他对象。它包含的对象有一定的层次结构，如：LAYER对象中包含CLASS，CLASS是属于LAYER的。
　　　　当前是一个非常简单的文件结构，当学习完其他用例时，你会了解更复杂的mapfile层次结构。
　　　　我们定义mapfile中的内容时，以对象名称开头，END结尾；#表示注释。
　　　　目前的层次结构为：
　　　　　　MAP
　　　　　　　　|----LAYER
　　　　　　　　　　|----CLASS
　　　　　　　　　　　　|----STYLE
　　3.Mapfile中的对象解释
　　　　3.1Map对象
　　　　　　MAP：每个mapfile均以MAP开头，END结尾。只有这样格式的文件才会被识别为mapfile文件。
　　　　　　IMAGETYPE：图片类型，imagetype为mapserver的CGI定义输出图片格式。当前样例使用PNG作为输出格式（老版本用GIF）。如果需要使用GIF，在编译源码时，需要开启GIF，WBMP或 JPEG 支持（在cmd里面输入mapserv -v可以查看输出的格式支持）。
　　　　　　　　　　   　　当然也能够指定输出其他格式，如：PDF、SWF、GeoTIFF等。只要编译的时候在OUTPUTFORMAT加上相关的支持即可。输出支持详见：https://mapserver.org/documentation.html#output
　　　　　　EXTENT：此参数指定地图的输出范围-初始地图的边界框。范围值的格式如：<Lower Left X> <Lower Left Y> <Upper Right X> <Upper Right Y>
　　　　　　　　　　　每个值用空格分开。这需要与数据使用相同的单位，或者，如果指定了不同的输出投影，则需要与输出投影使用相同的单位。
 　　　　　　　　   　　在这个例子中，我们的数据是地理投影的，所以单位是十进制的。
　　　　　　　　   　　你可以使用ogrinfo（官网：http://gdal.org/ogrinfo.html）提供的工具，其作为GDAL/OGR二级制包的一部分，作为shapefile数据集的一部分（或者其他支持向量格式数据集）。
　　　　　　　　　　　使用cmd命令定位到.shp文件所在目录，输入：ogrinfo -al -so states_ugl.shp 查看 states_ugl.shp 文件信息（红色部分根据实际查看文件信息填写）
　　　　　　　　　　　运行此命令的先决条件是，你安装了MapServer，同时环境变量中添加了GDAL
　　　　　　　　　　　
　　　　　　　　   　　当然，你也可以使用ArcView或者其他开元GIS软件查看，比如QGIS、Thuban 等。请随意更改范围的值，以便更好地了解范围如何更改地图。
　　　　　　SIZE：表示MapServer生成map图片的大小，但是为像素。当前例子是宽400像素，高300像素。在mapfile中修改这个值，可以查看在地图中的变化。
　　　　　　SHAPEPATH：图层数据的路径。可以是绝对路径，也可以是mapfile的相对路径（如：../data 或 E:\SvnWorkspace\LY_WEB_GIS\branches\Documents\ms4w-mapserver-for-wimdows\release-1911-x64-gdal-2-3-3-mapserver-7-2-1\apps\Example1.1\data）。
　　　　　　　　　　　　　此路径不需要通过web访问，通常根本不需要对其访问，除非你愿意提供给别人下载你的原始数据。在web浏览器上面，它没有任何作用，所以不要考虑提供下载此源文件的URL。
　　　　　　　　　　    　　只需要保证你的应用程序池可以访问到此shape文件（在Unix上面，是“nobdy”或“Apache”用户组），并对其读取权限。
　　　　　　IMAGECOLOR：地图的背景颜色。使用RGB值组成。
　　　　3.2 LAYER对象
　　　　　　LAYER：map对象图层的开始标签，详见：https://www.mapserver.org/mapfile/layer.html#layer。你可以指定多个layer对象。
　　　　　　NAME：NAME是LAYER对象的唯一标识符。MapServer通过NAME控制LAYER的开关。这个案例中LAYER的STATUS为默认值，所以NAME是无法控制工作的。后续案例中再做详细介绍。
　　　　　　DATA：数据的名称（当前案例中是shape文件的数据名称）。MapServer通过OGR（GDAL库的一部分）库支持ESRI的shapefile格式以及以外的矢量数据解析。你可以通过访问GDAL数据支持学习MapServer所支持的不同的矢量数据格式（http://gdal.org/ogr_formats.html）。
　　　　　　　　　  然后官网上面也有相关的数据支持文档guide to using vector data for MapServer.
　　　　　　TYPE：数据是什么类型的？如果是矢量数据，你可以指定为POLYGON（多边形），LINE（线）（即使是POLYLINE在技术上也可以使用LINE），或者POINT（点）。你也可以指定为RASTER（栅格）或ANNOTATION（标注）数据。当前案例是POLYGON（多边形）。
　　　　　　STATUS：层级（layers）是通过他们的STATUS来设置开关的。DEFAULT 状态默认是打开的。层级（layers）的开关控制是通过URL中LAYER的名称参数控制的。
　　　　3.3 CLASS 对象
　　　　　　CLASS：在LAYER对象中，以CLASS标签开始，END结尾。你可以在一个layer中指定多个CLASS。
　　　　　　NAME：CLASS对象的唯一标识符。一个layer对象可以有多个class对象，就像一个MAP对象可以有多个layer对象一样。MapServer通过CLASS对象的NAME标记命名CLASS等对象，所以最好给每个CLASS对象有一个适当的名称描述。
　　　　3.4 STYLE 对象
　　　　　　STYLE：一个CLASS对象中可以定义多个style对象。可以通过一个style覆盖或重载其他style。
　　　　　　CLOLR：多边形的填充颜色。如果是线形（TYPE值为LINE），则表示线的颜色。COLOR是一个RGB格式颜色值。
　　　　　　OUTLINECOLOR：多边形的边线颜色。RGB颜色值格式。默认情况下，MapServer不会绘制多边形（TYPE值为POLYGON）的边线。如果你想查看多边形的边界，你需要在mapfile中定义OUTLINECOLOR参数。
　　最后，你可以在mapfile中修改相关参数值，以便让你更加轻松的学习相关关键之的用法。
三、根据用例创建自己实际运行的站点应用
　　1. 观察URL根据自己的实际情况加以调整
　　　　1.1 新建一个自己的 Example1.1 A map with single layer 站点
　　　　　　根据map参数的相对路径是基于mapserv.exe的相对路径所决定的。我建议将站点建立在E:\SvnWorkspace\LY_WEB_GIS\branches\Documents\ms4w-mapserver-for-wimdows\release-1911-x64-gdal-2-3-3-mapserver-7-2-1\apps目录下。
　　　　　　这样发布站点时，URL中的map参数可以为相对路径。mapfile中SHAPEPATH也使用相对路径比较方便。
　　　　　　
 
 
　　　　　　后续所有的案例均放在apps文件夹下面，每个案例的命名规则为Example + section + 编号 + . + 案例编号。所以第一节第一个案例的目录名称为：Example1.1
　　　　　　在cmd中输入：cd /d E:\SvnWorkspace\LY_WEB_GIS\branches\Documents\ms4w-mapserver-for-wimdows\release-1911-x64-gdal-2-3-3-mapserver-7-2-1\apps
　　　　　　在cmd中输入：md Example1.1
　　　　　　在cmd中输入：cd  Example1.1
　　　　　　在cmd中输入：cd.>web.config
　　　　　　以上操作是创建相关文件夹以及web.config文件。
　　　　　　在web.config文件呢中天下如下内容，配置FastCGI指向。（详情请参考《MapServer Configuring with IIS》）　　　　　　

1 <?xml version="1.0" encoding="UTF-8"?>
2 <configuration>
3     <system.webServer>
4         <handlers>
5             <add name="MapServerFastCgi" path="*" verb="*" type="" modules="FastCgiModule" scriptProcessor="E:\SvnWorkspace\LY_WEB_GIS\branches\Documents\ms4w-mapserver-for-wimdows\release-1911-x64-gdal-2-3-3-mapserver-7-2-1\bin\mapserv.exe" resourceType="Unspecified" requireAccess="Script" allowPathInfo="false" preCondition=""  />
6         </handlers>
7         <caching enabled="true" enableKernelCache="true" />
8     </system.webServer>
9 </configuration>

 
　　　　　　其中“scriptProcessor”的实际路径根据你安装的MapServer填写。
 
　　　　　　站点的物理文件路径为：E:\SvnWorkspace\LY_WEB_GIS\branches\Documents\ms4w-mapserver-for-wimdows\release-1911-x64-gdal-2-3-3-mapserver-7-2-1\apps\Example1.1
　　　　　　端口为：8011
　　　　　　应用程序池名称为：Example1.1
 
　　　　1.2 配置当前 Example1.1 站点
　　　　　　其中，当前站点需要运行，涉及到：mapfile文件、shape文件等存放。
　　　　　　在cmd中输入：cd /d E:\SvnWorkspace\LY_WEB_GIS\branches\Documents\ms4w-mapserver-for-wimdows\release-1911-x64-gdal-2-3-3-mapserver-7-2-1\apps\Example1.1
　　　　　　在cmd中输入：md data
　　　　　　在cmd中输入：md logs
　　　　　　在cmd中输入：cd.>example1_1.map
　　　　　　创建相关文件夹和数据存放文件夹，日志记录文件夹，以及Example1.1站点运行时调用MapServer的mapfile文件。
　　　　　　将tutorial\data文件夹里面的states_ugl.dbf、states_ugl.shp、states_ugl.shx拷贝到“E:\SvnWorkspace\LY_WEB_GIS\branches\Documents\ms4w-mapserver-for-wimdows\release-1911-x64-gdal-2-3-3-mapserver-7-2-1\apps\Example1.1\data”文件夹下面。
　　　　　　输入文件详见：https://www.mapserver.org/input/vector/format_types.html。这里解释了上个文件的类型。
　　　　　　修改example1_1.map文件，内容如下：
 

 1 MAP
 2   IMAGETYPE      PNG
 3   EXTENT         -97.238976 41.619778 -82.122902 49.385620
 4   SIZE           400 300
 5   SHAPEPATH      "./data" #可以使exmaple1_1.map的相对路径，也可以是绝对路径 E:\SvnWorkspace\LY_WEB_GIS\branches\Documents\ms4w-mapserver-for-wimdows\release-1911-x64-gdal-2-3-3-mapserver-7-2-1\apps\Example1.1\data
 6   IMAGECOLOR     255 255 255
 7 
 8   LAYER 
 9     NAME         states
10     DATA         states_ugl
11     STATUS       OFF
12     TYPE         POLYGON
13 
14     CLASS
15       NAME       "The Upper Great Lakes States"
16 
17       STYLE
18         COLOR        232 232 232
19         OUTLINECOLOR 32 32 32
20       END
21     END
22   END 
23   DEBUG 5 
24   CONFIG "MS_ERRORFILE" "logs\ms.log"
25 END 

 　　　　配置web站点程序池对日志文件读写权限
　　　　 在cmd中输入：icacls "E:\SvnWorkspace\LY_WEB_GIS\branches\Documents\ms4w-mapserver-for-wimdows\release-1911-x64-gdal-2-3-3-mapserver-7-2-1\apps\Example1.1\logs" /grant "IIS AppPool\Example1.1":(OI)(CI)RW
 　　　　
　　　　测试站点能否访问，在浏览器中输入：http://localhost:8011/mapserv?map=../apps/Example1.1/example1_1.map&layer=states&mode=map
　　　　
 　　　  map=../apps/Example1.1/example1_1.map  表示 MapServer 与 mapfile （example1_1.map）的相对路径
　　　　mapfile （example1_1.map）中 SHAPEPATH      "./data" 也是使用的相对路径。
　　　　两处均使用了相对路径。这样URL显得比较安全，毕竟没有路径盘符信息了。
后记
　　在写此案例的过程中，遇到了一些相对路径的问题。修修改改，总算完成，其中也有很多不懂的知识，但是就像做题一样，做错了，就能马上知道自己错了。然后找答案，查询知识点。就能一一解决。
　　下一篇按照大纲接续写。
 
https://www.cnblogs.com/eshinex/p/10276449.html
**************************************************
004.Ceph块设备基础使用
一 基础准备


参考《002.Ceph安装部署》文档部署一个基础集群；
新增节点主机名及IP在deploy节点添加解析：



  1 [root@deploy ~]# echo "172.24.8.75 cephclient" >>/etc/hosts



配置国内yum源：



  1 [root@cephclient ~]# yum -y update
  2 [root@cephclient ~]# rm /etc/yum.repos.d/* -rf
  3 [root@cephclient ~]# wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo
  4 [root@cephclient ~]# yum -y install epel-release
  5 [root@cephclient ~]# mv /etc/yum.repos.d/epel.repo /etc/yum.repos.d/epel.repo.backup
  6 [root@cephclient ~]# mv /etc/yum.repos.d/epel-testing.repo /etc/yum.repos.d/epel-testing.repo.backup
  7 [root@cephclient ~]# wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo

 
二 块设备

2.1 添加普通用户

  1 [root@cephclient ~]# useradd -d /home/cephuser -m cephuser
  2 [root@cephclient ~]# echo "cephuser" | passwd --stdin cephuser	#cephclient节点创建cephuser用户
  3 [root@cephclient ~]# echo "cephuser ALL = (root) NOPASSWD:ALL" > /etc/sudoers.d/cephuser
  4 [root@cephclient ~]# chmod 0440 /etc/sudoers.d/cephuser
  5 [root@deploy ~]# su - manager
  6 [manager@deploy ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub cephuser@172.24.8.75

 
2.2 安装ceph-client

  1 [root@deploy ~]# su - manager
  2 [manager@deploy ~]$ cd my-cluster/
  3 [manager@deploy my-cluster]$ vi ~/.ssh/config
  4 Host node1
  5    Hostname node1
  6    User cephuser
  7 Host node2
  8    Hostname node2
  9    User cephuser
 10 Host node3
 11    Hostname node3
 12    User cephuser
 13 Host cephclient
 14    Hostname cephclient						#新增cephclient节点信息
 15    User cephuser
 16 [manager@deploy my-cluster]$ ceph-deploy install cephclient	#安装Ceph

 
注意：若使用ceph-deploy部署的时候出现安装包无法下载，可在部署时候指定ceph.repo为国内源：

  1 ceph-deploy install cephclient --repo-url=https://mirrors.aliyun.com/ceph/rpm-mimic/el7/ --gpg-url=https://mirrors.aliyun.com/ceph/keys/release.asc


  1 [manager@deploy my-cluster]$ ceph-deploy admin cephclient

提示：为方便后期deploy节点管理cephclient，在CLI中使用命令中简化相关key的输出，可将key复制至相应节点。ceph-deploy 工具会把密钥环复制到/etc/ceph目录，要确保此密钥环文件有读权限（如 sudo chmod +r /etc/ceph/ceph.client.admin.keyring ）。
2.3 创建pool

  1 [manager@deploy my-cluster]$ ssh node1 sudo ceph osd pool create mytestpool 64

2.4 初始化pool

  1 [root@cephclient ~]# ceph osd lspools
  2 [root@cephclient ~]# rbd pool init mytestpool

 
2.5 创建块设备

  1 [root@cephclient ~]# rbd create mytestpool/mytestimages --size 4096 --image-feature layering

2.6 确认验证

  1 [root@cephclient ~]# rbd ls mytestpool
  2 mytestimages
  3 [root@cephclient ~]# rbd showmapped
  4 id pool       image        snap device
  5 0  mytestpool mytestimages -    /dev/rbd0
  6 [root@cephclient ~]# rbd info mytestpool/mytestimages

 

2.7 将image映射为块设备

  1 [root@cephclient ~]# rbd map mytestpool/mytestimages --name client.admin
  2 /dev/rbd0

 
2.8 格式化设备

  1 [root@cephclient ~]# mkfs.ext4 /dev/rbd/mytestpool/mytestimages
  2 [root@cephclient ~]# lsblk

 

2.9 挂载并测试

  1 [root@cephclient ~]# sudo mkdir /mnt/ceph-block-device
  2 [root@cephclient ~]# sudo mount /dev/rbd/mytestpool/mytestimages /mnt/ceph-block-device/
  3 [root@cephclient ~]# cd /mnt/ceph-block-device/
  4 [root@cephclient ceph-block-device]# echo 'This is my test file!' >> test.txt
  5 [root@cephclient ceph-block-device]# ls
  6 lost+found  test.txt

 
2.10 自动map

  1 [root@cephclient ~]# vim /etc/ceph/rbdmap
  2 # RbdDevice     Parameters
  3 #poolname/imagename id=client,keyring=/etc/ceph/ceph.client.keyring
  4 mytestpool/mytestimages id=admin,keyring=/etc/ceph/ceph.client.admin.keyring

 
2.11 开机挂载

  1 [root@cephclient ~]# vi /etc/fstab
  2 #……
  3 /dev/rbd/mytestpool/mytestimages    /mnt/ceph-block-device  ext4    defaults,noatime,_netdev    0 0

 
2.12 rbdmap开机启动

  1 [root@cephclient ~]# systemctl enable rbdmap.service
  2 [root@cephclient ~]# df -hT				#查看验证

 

提示：若出现开机后依旧无法自动挂载，rbdmap也异常，可如下操作：

  1 [root@cephclient ~]# vi /usr/lib/systemd/system/rbdmap.service
  2 [Unit]
  3 Description=Map RBD devices
  4 WantedBy=multi-user.target			#需要新增此行
  5 #……

 
https://www.cnblogs.com/itzgr/p/10285882.html
**************************************************
ASP.NET MVC5多语言切换快速实现方案
功能
实现动态切换语言,Demo 做了三种语言库可以切换,包括资源文件的定义,实体对象属性设置,后台代码Controller,IAuthorizationFilter,HtmlHelper的是实现,做法比较简单,配合我之前发布的# MVC Scaffolding SmartCode-Engine 更新 模板中新增了多语言资源文件的生成功能,发现我的这个框架和目前很流行的ABP框架是类似更有点像收费版的Asp.net Zero,只是我做的更加轻量级,更方便,更快速,可惜Asp.net Core 下的Scaffolding这块扩展不想MVC5那么容易.这块还需要研究,下一步就准备升级到asp.net core.

Github download Demo
 
具体实现方法


定义实体类通过Display属性定义Name ResourceType,需要读取的语言库资源文件







生成资源文件通过mvc scaffolding扩展工具会自动生成对应实体对象的3个资源文件默认中文,繁体,英文繁体需要自己翻译,英文内容根据字段名定义后已大写字母分割 DateTime 现实 Date Time

页面功能按钮语言资源文件库









 前后端代码实现语言切换功能


-选择切换语言

 



2.Js代码

/* multiple lang dropdown */
$('#dropdownlang-dropdown-menu').on('click', 'a', function () {
  const lang = this.text;
  const flag = this.firstElementChild.className;
  const culture = this.firstElementChild.getAttribute("culture");
  $('#dropdownlang').children()[0].className = flag;
  $('#dropdownlang').children()[1].innerHTML = lang;
  localStorage.setItem('lang-text', lang);
  localStorage.setItem('lang-css', flag);
  localStorage.setItem('lang-culture', culture);
  $.get('/Account/SetCulture?lang=' + culture).then(res => {
    if (res.success) {
      location.reload();
    }

  });
});
$(function () {
  const lang = localStorage.getItem('lang-text');
  const css = localStorage.getItem('lang-css');
  const culture = localStorage.getItem('lang-culture');
    scripttag = document.createElement("script");
    scripttag.type = "text/javascript";
    scripttag.src = src;
    document.body.appendChild(scripttag);
    $.parser.parse();
 
 
  };
  if (lang && css && culture) {
    $('#dropdownlang').children()[0].className = css;
    $('#dropdownlang').children()[1].innerHTML = lang;

  }
});

后端代码



[HttpGet]
    public ActionResult SetCulture(string lang) {
      //这里设置CultureInfo是多余的
      switch (lang.Trim())
      {
        case "en":
          CultureInfo.CurrentCulture = new CultureInfo("en-US");
          CultureInfo.CurrentUICulture = new CultureInfo("en-US");
          break;
        case "cn":
          CultureInfo.CurrentCulture = new CultureInfo("zh-CN");
          CultureInfo.CurrentUICulture = new CultureInfo("zh-CN");
          break;
        case "tw":
          CultureInfo.CurrentCulture = new CultureInfo("zh-TW");
          CultureInfo.CurrentUICulture = new CultureInfo("zh-TW");
          break;
      }
     //这里设置CultureInfo是多余的
      var cookie = new HttpCookie("culture", lang)
      {
        Expires = DateTime.Now.AddYears(1)
      };
      Response.Cookies.Add(cookie);
      return Json(new { success = true }, JsonRequestBehavior.AllowGet);

    }

CultureFilter 这是关键 这是没有使用RouteData,通过修改url来保存当前语言要改的地方很多还要修改路由规则,所以我就用Cookies来保存



public class CultureFilter : IAuthorizationFilter
  {
    private readonly string defaultCulture;

    public CultureFilter()
    {
      this.defaultCulture = "cn";
    }

    public void OnAuthorization(AuthorizationContext filterContext)
    {
      var culture = filterContext.HttpContext.Request.Cookies["culture"];
      var lang = defaultCulture;
      if (culture != null && culture.Value != null)
      {
        lang = culture.Value;
        filterContext.HttpContext.Response.Cookies.Set(culture);
      }
      switch (lang.Trim())
      {
        case "en":
          CultureInfo.CurrentCulture = new CultureInfo("en-US");
          CultureInfo.CurrentUICulture = new CultureInfo("en-US");
          //Thread.CurrentThread.CurrentUICulture = new CultureInfo("en-US");
          //Thread.CurrentThread.CurrentCulture = new CultureInfo("en-US");
          break;
        case "cn":
          CultureInfo.CurrentCulture = new CultureInfo("zh-CN");
          CultureInfo.CurrentUICulture = new CultureInfo("zh-CN");
          //Thread.CurrentThread.CurrentUICulture = new CultureInfo("zh-CN");
          //Thread.CurrentThread.CurrentCulture = new CultureInfo("zh-CN");
          break;
        case "tw":
          CultureInfo.CurrentCulture = new CultureInfo("zh-TW");
          CultureInfo.CurrentUICulture = new CultureInfo("zh-TW");
          //Thread.CurrentThread.CurrentUICulture = new CultureInfo("zh-TW");
          //Thread.CurrentThread.CurrentCulture = new CultureInfo("zh-TW");
          break;
      }
    }
  }

HtmlHelper 代码实现语言的输出



/// <summary>
    /// 多语言切换
    /// </summary>
    /// <param name="helper"></param>
    /// <param name="name"></param>
    /// <returns></returns>
    public static HtmlString L(this HtmlHelper helper, string name) {

      var resource = new System.Resources.ResourceManager(typeof(WebApp.resource.Global));
      var text = resource.GetString(name);
      return new HtmlString(text ?? name);
    }
    /// <summary>
    /// 前端easyui或是其它js相关的比如提示信息也需要转换必须在这里加载不同的语言文件
    /// </summary>
    public static HtmlString LangScriptTag(this HtmlHelper helper,string defaultsrc) {
      var src = defaultsrc;
      var lang = CultureInfo.CurrentCulture.Name;
      switch (lang)
      {
        case "en-US":
          src = "/Scripts/easyui/locale/easyui-lang-en.js";
          break;
        case "zh-CN":
          src = "/Scripts/easyui/locale/easyui-lang-zh_CN.js";
          break;
        case "zh-TW":
          src = "/Scripts/easyui/locale/easyui-lang-zh_TW.js";
          break;
        default:
          src = defaultsrc;
          break;
      }
      return new HtmlString($"<script src=\"{ src }\"></script>");
    }

//html代码
<div class="btn-group btn-group-sm">
<button onclick="append()" class="btn btn-default"> <i class="fa fa-plus"></i> @Html.L("Add") </button>
</div>
<div class="btn-group btn-group-sm">
<button onclick="removeit()" class="btn btn-default"> <i class="fa fa-trash-o"></i> @Html.L("Delete") </button>
</div>
<div class="btn-group btn-group-sm">
<button onclick="accept()" class="btn btn-default"> <i class="fa fa-floppy-o"></i> @Html.L("Save") </button>
</div>

 



https://www.cnblogs.com/neozhu/p/10285874.html
**************************************************
Android 实现卡片翻转的动画（翻牌动画）

Android 实现卡片翻转的动画（翻牌动画）
需求描述
　　点击卡片，卡片翻转过来显示内容。　　点击左边的卡片，将卡片翻转显示右边的图片结果。
功能实现
　　因为要翻转所以使用动画来完成翻转的动画。动画分为两部分，一部分是左边的布局以中心垂直线从左向右旋转，旋转 180 度之后隐藏，另一部分是右边的布局以中心垂直线从右向左旋转，旋转 180 度之后显示。　　这种动画涉及到播放顺序的问题，所以动画使用 Animator 属性动画实现。 布局 
    <FrameLayout
        android:id="@+id/activity_main_result_layout_fl"
        android:layout_width="@dimen/activity_main_result_width"
        android:layout_height="@dimen/activity_main_result_height"
        android:layout_gravity="center"
        android:layout_marginTop="@dimen/activity_main_result_margin_top">

        <ImageView
            android:id="@+id/activity_main_result_iv"
            android:layout_width="match_parent"
            android:layout_height="match_parent"
            android:src="@mipmap/result" />

        <TextView
            android:id="@+id/activity_main_result_tv"
            android:layout_width="match_parent"
            android:layout_height="match_parent"
            android:gravity="center"
            android:textColor="@android:color/white"
            android:textSize="@dimen/activity_main_text_size"
            android:visibility="gone" />

    </FrameLayout>

 动画文件 


rotate_in_anim .xml
<?xml version="1.0" encoding="utf-8"?>
<set xmlns:android="http://schemas.android.com/apk/res/android">
  <!--消失-->
  <objectAnimator
      android:duration="0"
      android:propertyName="alpha"
      android:valueFrom="1.0"
      android:valueTo="0.0" />

  <!--旋转-->
  <objectAnimator
      android:duration="1000"
      android:propertyName="rotationY"
      android:valueFrom="-180"
      android:valueTo="0" />

  <!--出现-->
  <objectAnimator
      android:duration="0"
      android:propertyName="alpha"
      android:startOffset="500"
      android:valueFrom="0.0"
      android:valueTo="1.0" />
</set>



rotate_out_anim.xml
<?xml version="1.0" encoding="utf-8"?>
<set xmlns:android="http://schemas.android.com/apk/res/android">
  <!--旋转-->
  <objectAnimator
      android:duration="1000"
      android:propertyName="rotationY"
      android:valueFrom="0"
      android:valueTo="180" />

  <!--消失-->
  <objectAnimator
      android:duration="0"
      android:propertyName="alpha"
      android:startOffset="500"
      android:valueFrom="1.0"
      android:valueTo="0.0" />
</set>



 播放动画 
    tv_result.setVisibility(View.VISIBLE);
    tv_sure.setVisibility(View.VISIBLE);
    AnimatorSet inAnimator = (AnimatorSet) AnimatorInflater.loadAnimator(this, R.animator.rotate_in_anim);
    AnimatorSet outAnimator = (AnimatorSet) AnimatorInflater.loadAnimator(this, R.animator.rotate_out_anim);
    int distance = 16000;
    float scale = getResources().getDisplayMetrics().density * distance;
    iv_result.setCameraDistance(scale); //设置镜头距离
    tv_result.setCameraDistance(scale); //设置镜头距离
    outAnimator.setTarget(iv_result);
    inAnimator.setTarget(tv_result);
    outAnimator.start();
    inAnimator.start();
    outAnimator.addListener(new AnimatorListenerAdapter() {
        @Override
        public void onAnimationEnd(Animator animation) {
            super.onAnimationEnd(animation);
            iv_result.setVisibility(View.GONE);
            iv_result.setAlpha(1.0f);
            iv_result.setRotationY(0.0f);
        }
    });

　　注意：动画的实现方式是使用了属性动画 Animator 实现的，如果动画需要再次显示，那么在动画结束之后就需要将控件的属性值设置为初始值，因为属性动画会修改控件的属性值为动画结束时的属性值。
参考文章
http://lishuaishuai.iteye.com/blog/2297056

https://www.cnblogs.com/zhangmiao14/p/10285862.html
**************************************************
分布式系统关注点——「无状态」详解

如果这是第二次看到我的文章，欢迎右侧扫码订阅我哟~  👉
本文长度为2728字，建议阅读8分钟。
坚持原创，每一篇都是用心之作～

 
 
前面聊完的2个章节「数据一致性」和「高可用」其实本质是一个通过提升复杂度让整体更完善的方式。
 
接下去我们开始聊一些让系统更简单，更容易维护的东西——「易伸缩」，首当其冲的第一篇文章就是「stateless」，也叫「无状态」。
 
z哥带你先来认识一下「状态」是什么。
 
 
一、初识「状态」
之前在「负载均衡」的第四篇（分布式系统关注点——做了「负载均衡」就可以随便加机器了吗？）中提到过一个例子，我们再翻出来一下。
 

开发Z哥对运维Y弟喊：“Y弟，现在系统好卡，刚上了一波活动，赶紧帮我加几台机器上去顶一下。”
 
Y弟回复说：“没问题，分分钟搞定”。
 
然后就发现数据库的压力迅速上升，DBA就吼了：“Z哥，你丫的搞什么呢？数据库要被你弄垮了”。
 
然后客服那边接框也爆炸了，越来越多的用户说刚登陆后没多久，操作着就退出了，接着登陆，又退出了，到底还做不做生意了。

 
这个案例中的问题，产生的根本原因是因为系统中存在着大量「有状态」的业务处理过程。
 
 
二、「有状态」和「无状态」
 N.Wirth曾经在它1984年出版的书中将程序的定义经典的概括为：程序=数据结构+算法。（这个概括也是这本书的书名）
 
这是一个很有意思的启发，受它的影响，z哥认为程序做的事情本质就是“数据的移动和组合”，以此来达到我们所期望的结果。而如何移动、如何组合是由“算法”来定的，所以z哥延伸出一个新的定义：数据+算法=成果。
 
通过程序处理所得到的“成果”其实和你平时生活中完成的任何事情所得到的“成果”是一样的。任何一个“成果”都是你通过一系列的“行动”将最开始的“原料”进行加工、转化，最终得到你所期望的“成果”。
 

 
比如，你将常温的水，通过“倒入水壶”、“通电加热”等工作后变成了100度的水，就是这样一个过程。
 
正如烧水的例子，大多数时候得到一个“成果”往往需要好几道“行动”才能完成。

这个时候如果想降低这几道“行动”总的成本（如：时间）该怎么办呢？
自然就是提炼出反复要做的事情，让其只做一次。而这个事情在程序中，就是将一部分“数据”放到一个「暂存区」（一般就是本地内存），以提供给相关的“行动”共用。 

但是如此一来，就导致了需要增加一道关系，以表示每一个“行动”与哪一个「暂存区」关联。因为在程序里，“行动”可能是「多线程」的。
 
这时，这个“行动”就变成「有状态」的了。

 

题外话：共用同一个「暂存区」的多个“行动”所处的环境经常被称作「上下文」。

 
 
我们再来深入聊聊「有状态」。
 
「暂存区」里存的是「数据」，所以可以理解为“有数据”就等价于“有状态”。
 
「数据」在程序中的作用范围分为「局部」和「全局」（对应局部变量和全局变量），因此「状态」其实也可以分为两种，一种是局部的「会话状态」，一种是全局的「资源状态」。
 

题外话：因为有些服务端不单单负责运算，还会提供其自身范围内的「数据」出去，这些「数据」属于服务端完整的一部分，被称作「资源」。所以，理论上「资源」可以被每个「会话」来使用，因此是全局的状态。

 
本文聊的「有状态」都指的是「会话状态」。
 
 
与「有状态」相反的是「无状态」，「无状态」意味着每次“加工”的所需的“原料”全部由外界提供，服务端内部不做任何的「暂存区」。并且请求可以提交到服务端的任意副本节点上，处理结果都是完全一样的。
 
有一类方法天生是「无状态」，就是负责表达移动和组合的“算法”。因为它的本质就是：


接收“原料”（入参）


“加工”并返回“成果”（出参）


 
 
为什么网上主流的观点都在说要将方法多做成「无状态」的呢？
 
因为我们更习惯于编写「有状态」的代码，但是「有状态」不利于系统的易伸缩性和可维护性。
 
在分布式系统中，「有状态」意味着一个用户的请求必须被提交到保存有其相关状态信息的服务器上，否则这些请求可能无法被理解，导致服务器端无法对用户请求进行自由调度（例如双11的时候临时加再多的机器都没用）。
 
同时也导致了容错性不好，倘若保有用户信息的服务器宕机，那么该用户最近的所有交互操作将无法被透明地移送至备用服务器上，除非该服务器时刻与主服务器同步全部用户的状态信息。
 
这两个问题在负载均衡的第四篇（分布式系统关注点——做了「负载均衡」就可以随便加机器了吗？）中也有提到。
 
 
但是如果想获得更好的伸缩性，就需要尽量将「有状态」的处理机制改造成「无状态」的处理机制。
 
 
三、「无状态」化处理
将「有状态」的处理过程改造成「无状态」的，思路比较简单，内容不多。
 
首先，状态信息前置，丰富入参，将处理需要的数据尽可能都通过上游的客户端放到入参中传过来。

当然，这个方案的弊端也很明显：网络数据包的大小会更大一些。
 
 
另外，客户端与服务端的交互中如果涉及到多次交互，则需要来回传递后续服务端处理中所需的数据，以避免需要在服务端暂存。

▲橙色请求，绿色响应
 
这些改造的目的都是为了尽量少出现类似下面的代码。
 

func(){
    return i++;
}

 
而是变成：
 

func(i){
    return i+1;
}

 
 
要更好的做好这个「无状态」化的工作，依赖于你在架构设计或者项目设计中的合理分层。
 
尽量将会话状态相关的处理上浮到最前面的层，因为只有最前面的层才与系统使用者接触，如此一来，其它的下层就可以将「无状态」作为一个普遍性的标准去做。
 
与此同时，由于会话状态集中在最前面的层，所以哪怕真的状态丢失了，重建状态的成本相对也小很多。
 
比如三层架构的话，保证BLL和DAL都不要有状态，代码的可维护性大大提高。
 
如果是分布式系统的话，保证那些被服务化的程序都不要有状态。除了能提高可维护性，也大大有利于做灰度发布、A/B测试。
 

题外话：在这里，提到做分层的目的是为了说明，只有将IO密集型程序和CPU密集型程序分离，才是通往「无状态」真正的出路。一旦分离后，CPU密集型的程序自然就是「无状态」了。
 
如此也能更好的做「弹性扩容」。因为常见的需要「弹性扩容」的场景一般指的就是CPU负荷过大的时候。

 
 
最后，如果前面的都不合适，可以将共享存储作为降级预案来运用，如远程缓存、数据库等。然后当状态丢失的时候可以从这些共享存储中恢复。
 
所以，最理想的状态存放点。要么在最前端，要么在最底层的存储层。

 
 
四、总结
任何事物都是有两面性的，正如前面提到的，我们并不是要所有的业务处理都改造成「无状态」，而只是挑其中的一部分。最终还是看“价值”，看“性价比”。
 
比如，将一个以“状态”为核心的即时聊天工具的所有处理过程都改造成「无状态」的，就有点得不偿失了。
 
 

 
相关文章：


分布式系统关注点——初识「高可用」


分布式系统关注点——仅需这一篇，吃透「负载均衡」妥妥的


分布式系统关注点——「负载均衡」到底该如何实施？


分布式系统关注点——做了「负载均衡」就可以随便加机器了吗？这三招来帮你！


分布式系统关注点——99%的人都能看懂的「熔断」以及最佳实践


分布式系统关注点——想通关「限流」？只要这一篇


分布式系统关注点——让你的系统“坚挺不倒”的最后一个大招——「降级」


分布式系统关注点——99%的人都能看懂的「补偿」以及最佳实践


 

 
作者：Zachary
出处：https://www.cnblogs.com/Zachary-Fan/p/stateless.html
 
如果你喜欢这篇文章，可以点一下右下角的「推荐」。
这样可以给我一点反馈。: )
谢谢你的举手之劳。
 

▶关于作者：张帆（Zachary，个人微信号：Zachary-ZF）。坚持用心打磨每一篇高质量原创。欢迎扫描右侧的二维码~。
定期发表原创内容：架构设计丨分布式系统丨产品丨运营丨一些思考。








 
https://www.cnblogs.com/Zachary-Fan/p/stateless.html
**************************************************
jdk1.8之线程中断
在Core Java中有这样一句话："没有任何语言方面的需求要求一个被中断的程序应该终止。中断一个线程只是为了引起该线程的注意，被中断线程可以决定如何应对中断 "
线程中断不会使线程立即退出，而是给线程发送一个通知，告知目标线程有人希望你退出。至于目标线程接收到通知后如何处理，则完全由目标线程自行决定。
线程中断有关的三个方法

void Thread.interrupt();//中断线程
boolean Thread.isInterrupted()//判断是否中断
static boolean Thread.interrupted()//判断是否中断，并清除当前中断状态
Thread.interrupt()方法是一个实例方法，它通知目标线程中断，也就是设置中断标志位。中断标志位表示当前线程已经被中断了。
Thread.isInterrupted()方法也是实例方法，它判断当前线程是否有被中断（通过检查中断标志位）。

静态方法Thread.interrupted()也是用来判断当前线程的中断状态，但同时会清除当前线程的中断标志位状态。
运行中的线程不会因为interrupt()而中断，因为它仅仅是一个信号（status）
    public static void main(String[] intsmaze) throws InterruptedException {
        Thread t1=new Thread()
        {
            public void run()
            {
                while(true){ }
            }
        };     
        t1.start();
        Thread.sleep(2000);
        t1.interrupt();
    }
这个程序虽然对t1进程了中断，但是在t1中并没有中断处理的逻辑，因此即使t1线程被置上了中断状态，但是这个中断不会发生任何作用。
如果希望t1在中断后退出，必须为他增加相应的中断处理代码，如下
    public static void main(String[] intsmaze) throws InterruptedException {
        Thread t1=new Thread()
        {
            public void run()
            {
                while(true)
                {
                    if(Thread.currentThread().isInterrupted())//判断当前线程是否中断。
                    {
                        System.out.println("intsmaze Interrupt");
                        break;
                    }
                }
            }
        };
        t1.start();
        Thread.sleep(2000);
        t1.interrupt();
    }
等待中的线程（wait(long),sleep(long),join(long)收到中断信号会抛出InterruptedException
public static native void sleep(long millis) throws InterruptedException;会抛出一个中断异常。当线程在休眠sleep时，如果被中断就会产生该异常，此时它会清楚中断标志，如果不加处理，那么在下一次循环开始时，就无法捕获这个中断。
如果注释掉catch中的Thread.currentThread().interrupt();我们可以发现，程序一直运行，线程没有停止；反之放开该注释，则发现程序运行结束了。
    public static void main(String[] intsmaze) throws InterruptedException {
        Thread t1=new Thread()
        {
            public void run()
            {
                while(true)
                {
                    if(Thread.currentThread().isInterrupted())
                    {
                        System.out.println("intsmaze Interrupt");
                        break;
                    }               
                    try {
                        Thread.sleep(6000);
                    } catch (InterruptedException e) {
                        System.out.println("Interrupt when intsmaze sleep");
                        Thread.currentThread().interrupt();//设置中断状态
                    }
                }
            }
        };
        
        t1.start();
        Thread.sleep(2000);
        t1.interrupt();
    }
BLOCKED
如果线程在等待锁，对线程对象调用interrupt()只是会设置线程的中断标志位，线程依然会处于BLOCKED状态，也就是说，interrupt()并不能使一个在等待锁的线程真正”中断”。通过前面的代码可以看到，中断是通过循环不判进行Thread.currentThread().isInterrupted()判断的。
在使用synchronized关键字获取锁的过程中不响应中断请求，这是synchronized的局限性。如果想在等待获取锁的过程中能响应中断，应该使用显式锁，Lock接口，它支持以响应中断的方式获取锁。
NEW/TERMINATE
如果线程尚未启动(NEW)，或者已经结束(TERMINATED)，则调用interrupt()对它没有任何效果，中断标志位也不会被设置。比如说，以下代码的输出都是false。
IO操作
如果线程在等待IO操作，尤其是网络IO，则会有一些特殊的处理。
如果IO通道是可中断的，即实现了InterruptibleChannel接口，则线程的中断标志位会被设置，同时，线程会收到异常ClosedByInterruptException。
如果线程阻塞于Selector调用，则线程的中断标志位会被设置，同时，阻塞的调用会立即返回。
我们重点介绍另一种情况，InputStream的read调用，该操作是不可中断的，如果流中没有数据，read会阻塞 (但线程状态依然是RUNNABLE)，且不响应interrupt()，与synchronized类似，调用interrupt()只会设置线程的中断标志，而不会真正”中断”它，我们看段代码。
public class InterruptReadDemo {
    private static class A extends Thread {
        @Override
        public void run() {
            while(!Thread.currentThread().isInterrupted()){
                try {
                    System.out.println(System.in.read());
                } catch (IOException e) {
                    e.printStackTrace();
                }    
            }
            System.out.println("exit");
        }
    }
    public static void main(String[] args) throws InterruptedException {
        A t = new A();
        t.start();
        Thread.sleep(100);
        t.interrupt();
    }
}
线程t启动后调用System.in.read()从标准输入读入一个字符，不要输入任何字符，我们会看到，调用interrupt()不会中断read()，线程会一直运行。
拿两年前的笔记出来冒个泡

https://www.cnblogs.com/intsmaze/p/10285850.html
**************************************************
视频编解码基础概念
错误难免，逐渐完善。
1. 概述
音视频领域由早期使用的模拟化技术逐渐发展为现在的数字化技术。数字化的主要好处有：可靠性高、能够消除传输及存储损耗，便于计算机处理及网络传输等。
数字化后，音视频处理就进入了计算机技术领域，音视频处理本质上就是对计算机数据的处理。
图像信息经采集后生成的原始视频数据，数据量非常大，对于某些采集后直接本地播放的应用场合，不需要考虑压缩技术。
但现实中更多的应用场合，涉及视频的传输与存储，传输网络与存储设备无法容忍原始视频数据的巨大数据量，必须将原始视频数据经过编码压缩后，再进行传输与存储。
本文仅考虑视频压缩技术，不考虑音频。
2. 视频压缩原理
2.1 熵与冗余
引自参考资料[1]

在所有的实际节目素材中，存在着两种类型的信号分量：即异常的、不可预见的信号分量和可以预见的信号分量。
异常分量称为熵，它是信号中的真正信息。其余部分称为冗余，因为它不是必需的信息。
冗余可以是空间性的，如在图象的大片区域中，邻近象素几乎具有相同的数值。冗余也可以是时间性的， 例如连续图象之间的相似部分。
在所有的压缩系统编码器中都是将熵与冗余相分离，只有熵被编码和传输，而在解码器中再从编码器的发送的信号中计算出冗余。

2.2 帧内编码
帧内编码是空间域编码，利用图像空间性冗余度进行图像压缩，处理的是一幅独立的图像，不会跨越多幅图像。
空间域压缩依赖于一幅图像中相邻像素间的相似性和图案区的主要空间域频率。
JPEG标准用于静止图像(即图片)，只使用了空间域压缩，只使用帧内编码。
2.3 帧间编码
帧间编码是时间域编码，是利用一组连续图像间的时间性冗余度进行图像压缩。如果某帧图像可被解码器使用，那么解码器只须利用两帧图像的差异即可得到下一帧图像。
比如运动平缓的几帧图像的相似性大，差异性小，而运动剧烈的几幅图像则相似性小，差异性大。当得到一帧完整的图像信息后，可以利用与后一帧图像的差异值推算得到后一帧图像，这样就实现了数据量的压缩。
时间域编码依赖于连续图像帧间的相似性，尽可能利用已接收处理的图像信息来“预测”生成当前图像。
MPEG标准用于运动图像(即视频)，会使用空间域编码和时间域编码，因此是帧内编码和帧间编码结合使用。
2.4 运动矢量
一组连续图像记录了目标的运动。运动矢量用于衡量两帧图像间目标的运动程度，运动矢量由水平位移量和垂直位移量二者构成。
2.5 运动补偿
目标的运动降低了图像间的相似性，增加了差异数据量。而运动补偿则通过运行矢量来降低图像间的差异数据量。
下图为运动补偿的示意图。当某一目标运动时，其位置会变化但形状颜色等基本不变。编码器则可利用运动矢量减低图像差值，解码器根据图像差值中的运动适量移动目标到合适的位置即可。
假设图中是理想情况，目标除移动位置外其他任何属性无任何变化，则两幅图像间的差值仅包含运动矢量这一数据量。显然运动补偿可以显著减少图像差值数据量。

2.6 双向预测
先看示意图：

连续的三幅图像中，目标块有垂直位置上的移动，背景块无位置移动。我们考虑如何取得当前帧图像(画面N)。
画面N中，目标向上移动后，露出背景块。
画面N-1中，因为背景块被目标块遮挡住了，因此没有背景块相关信息。
画面N+1中，完整包含背景块的数据，因此画面N可以从画面N-1中取得背景块。
如何可以得到画面N呢？解码器可以先解码得到画面N-1和画面N+1，通过画面N-1中的目标块数据结合运动矢量即可得到画面N中的目标块数据，通过画面N+1中的背景块数据则可得到画面N中的背景块数据。
三幅画面的解码顺序为：N-1, N+1, N。三幅画面的显示顺序为：N-1, N, N+1。画面N通过其前一幅画面N-1和后一幅画面N+1推算(预测，predicted)得到，因此这种方式称为双向预测(或前面预测、双向参考)。
2.7 I帧，IDR帧，P帧，B帧
I帧
I帧(Intra-coded picture, 帧内编码帧，常称为关键帧)包含一幅完整的图像信息，属于帧内编码图像，不含运动矢量，在解码时不需要参考其他帧图像。
因此在I帧图像处可以切换频道，而不会导致图像丢失或无法解码。I帧图像用于阻止误差的累积和扩散。
在闭合式GOP中，每个GOP的第一个帧一定是I帧，且当前GOP的数据不会参考前后GOP的数据。
IDR帧
IDR帧(Instantaneous Decoding Refresh picture, 即时解码刷新帧)是一种特殊的I帧。当解码器解码到IDR帧时，
会将DPB(Decoded Picture Buffer，指前后向参考帧列表)清空，将已解码的数据全部输出或抛弃，然后开始一次
全新的解码序列。IDR帧之后的图像不会参考IDR帧之前的图像。
P帧
P帧(Predictive-coded picture, 预测编码图像帧)是帧间编码帧，利用之前的I帧或P帧进行预测编码。
B帧
B帧(Bi-directionally predicted picture, 双向预测编码图像帧)是帧间编码帧，利用之前和(或)之后的I帧或P帧进行双向预测编码。
B帧不可以作为参考帧。
2.8 GOP
GOP(Group Of Pictures, 图像组)是一组连续的图像，由一个I帧和多个B/P帧组成，是编解码器存取的基本单位。
GOP结构常用的两个参数M和N，M指定GOP中首个P帧和I帧之间的距离，N指定一个GOP的大小。
例如M=1，N=15，GOP结构为：IPBBPBBPBBPBBPB
GOP有两种：闭合式GOP和开放式GOP
闭合式GOP
闭合式GOP只需要参考本GOP内的图像即可，不需参考前后GOP的数据。这种模式决定了，闭合式GOP的显示顺序总是以I帧开始以P帧结束
开放式GOP
开放式GOP中的B帧解码时可能要用到其前一个GOP或后一个GOP的某些帧。码流里面包含B帧的时候才会出现开放式GOP。
开放式GOP和闭合式GOP中I帧、P帧、B帧的依赖关系如下图所示：

2.9 DTS和PTS
DTS(Decoding Time Stamp, 解码时间戳)，表示packet的解码时间。
PTS(Presentation Time Stamp, 显示时间戳)，表示packet解码后数据的显示时间。
音频中DTS和PTS是相同的。视频中由于B帧需要双向预测，B帧依赖于其前和其后的帧，因此含B帧的视频解码顺序与显示顺序不同，即DTS与PTS不同。当然，不含B帧的视频，其DTS和PTS是相同的。
下图以一个开放式GOP示意图为例，说明视频流的解码顺序和显示顺序

采集顺序指图像传感器采集原始信号得到图像帧的顺序。
编码顺序指编码器编码后图像帧的顺序。存储到磁盘的本地视频文件中图像帧的顺序与编码顺序相同。
传输顺序指编码后的流在网络中传输过程中图像帧的顺序。
解码顺序指解码器解码图像帧的顺序。
显示顺序指图像帧在显示器上显示的顺序。
采集顺序与显示顺序相同。编码顺序、传输顺序和解码顺序相同。
图中“B[1]”帧依赖于“I[0]”帧和“P[3]”帧，因此“P[3]”帧必须比“B[1]”帧先解码。这就导致了解码顺序和显示顺序的不一致，后显示的帧需要先解码。
3. 参考资料
[1] 泰克Tektronic, MPEG基础和协议分析指南
[2] 视频直播的理论知识，https://www.jianshu.com/p/04b5b1e4ff27
[3] open GOP & close GOP, https://www.jianshu.com/p/d30c051b4106
[4] I帧/B帧/P帧/GOP, https://blog.csdn.net/abcsunl/article/details/68190136
[5] FFmpeg音视频同步原理与实现, https://www.jianshu.com/p/3578e794f6b5
[6] FFmpeg 音视频同步, https://www.jianshu.com/p/27279255f67e
4. 修改记录
2018-12-08 V1.0 初稿

https://www.cnblogs.com/leisure_chn/p/10285829.html
**************************************************
“别更新了，学不动了” 之：全栈开发者 2019 应该学些什么？
 

转载请注明出处：葡萄城官网，葡萄城为开发者提供专业的开发工具、解决方案和服务，赋能开发者。
原文转载自 公众号 infoqchina

 
对于什么是全栈开发者并没有一个明确的定义。但是，有一件事是肯定的：2019 年对全栈开发者的需求量很大。在本文中，我将向你概述一些趋势，你可以尝试根据这些趋势来确定你可能要投入的时间。
简单地说，全栈开发者就是可以构建完整应用程序的人。他们了解前端和后端技术、工具和服务，并结合所有这些技能开发出可以在生产环境中运行的东西。
这是美国全栈开发者在 2019 年的工资走势：

人生苦短，所以尽量少做无用功。如果你希望保持最新状态并成为全栈开发者，以下是你需要了解并考虑列入学习计划的 2019 年技术趋势。
 
前端
基础
HTML、CSS 和 JavaScript 是必须掌握的，你还需要学习 React、Vue 或 Angular 等前端框架或库。但是，你应该选择哪一个？对于一个真正的全栈开发者，你可以在 2019 年选择这三个框架中的任何一个。
来自React 16 的更新
你需要了解 React 的基础知识及其基于单向数据流架构的组件。今年我们看到了 React 16 的大量更新和 2019 年即将发布的一些小版本更新。
新的生命周期方法；




React 16.6 中的 Suspense for Code Splitting（已发布）；


带有 React Hooks 的 16.x 版本（2019 年第一季度）；


带有并发模式的 16.x 版本（2019 年第二季度）；


带有 Suspense for Data Fetching 的 16.x 版本（2019 年中）。




这意味着你需要知道如何使用 React.lazy() 和 < React.Suspense> 进行代码拆分，使用 React.memo 进行优化，并时刻关注新功能，如 React Hooks，它可能会给 React 生态系统带来重大改变。
我们现在还有标准化的 React Context API，你应该对它有一个基本的了解。
React 生态系统将在 2019 年继续发展和演化。它这不仅限于 Web，在移动、物联网和 AR/VR 等不同平台上移植和使用 React 的能力将使其变得越来越重要，并在 2019 年领先于其他 2 个库。
Vue 3.0
2018 年，Vue 持续获得开发者的青睐，2019 年将会继续增长……但它是否足以超越其他两大玩家？我们拭目以待。
Vue 生态系统正在不断发展，而且，随着 Vue 3.0 的发布极其改进的 Vue CLI，2019 年的开发者体验将比以往更好。
开发者可以使用 Vue Native 进行跨平台开发（就像 React Native 那样），我们已经很接近 React 那样的大型生态系统，但还是有一大段距离。
Vue 有一个非常有趣的趋势，它将在 2019 年继续增长：阿里巴巴、百度、腾讯、小米和 DJI 等中国科技巨头更喜欢 Vue。预计中国市场将继续保持快速增长，因为 Vue 是一个独立的开源库，与西方的大型科技巨头无关。
Angular Ivy 和 Angular Elements
新的渲染引擎 Ivy 即将推出，性能将会得到大幅提升。
Ivy 将成为 Angular 渲染引擎的第三个化身，它的目标是成为更小、更快、更简单的编译器。
Angular Elements 将使我们能够在 Angular 以外的其他环境中使用 Angular 组件。简单地说就是你可以构建可以被添加到不使用 Angular 的 HTML 页面中的组件，有点像 Web 组件。现在，我可以使用 Angular 创建世界上最好的组件，并将它交给我的朋友，她将它用在她的 React 应用程序中！
2019 年，Angular 将继续做他们擅长的事情：提供一个功能齐全的框架，用于构建丰富的 Web 应用程序。
Angular、Vue、React——更小更快
总的来说，2019 年将看到这 3 个前端库的发展。如前所述，你只要掌握其中一个，就已经为进入新的一年做好了准备。预计在 2019 年，这些库都会发生微小的变化，提高渲染速度并缩小库的体积……但它们都不会带来任何重大改进来压倒其他库。
CLI 将会风靡
你必须使用 babel、webpack、eslint、测试库和其他工具搭建项目脚手架的日子已经一去不复返了。我的意思是，我们仍然可以这么做，但 CLI 确实让这种体验变得更好了。




Angular CLI；


Create React App 2；


Vue CLI。




2019 年，我们将在 CLI 中看到越来越多的改进体验。
状态管理




Vue 将继续使用 Vuex 进行状态管理。


Angular 将继续主要使用 RxJS。


随着新的 Context API 的问世和 GraphQL + Apollo 的普及，React 今年则遭遇了一点危机。很长一段时间以来，Redux 第一次被认为不是状态管理的明智选择。你仍然需要学习 Redux，因为你可以从 Redux 中学到一些有用的计算机科学原理，如事件溯源和 CQRS。




新的 Context API、Redux 和 GraphQL
Apollo 内置的离线客户端缓存将使 Apollo + GraphQL 在 2019 年成为 Redux 的一个重要替代品（当然，从技术上讲，可以同时使用它们）。新的 Context API 问世了，很多人称它为 Redux 终结者。
2019 年，你需要了解它们三者，了解它们的工作原理，以及它们可以用来解决哪些问题。但如果从就业方面来看，学习 Redux 仍然是一个很好的选择。
服务器端渲染
服务器端渲染在 JavaScript 领域仍然是一个待解决的问题。我们知道，单页应用程序和客户端渲染很容易让项目出现代码膨胀，而且需要向客户端发送太多的 JavaScript 代码，而且可能会影响你的 SEO（但可能没有你想象的那么多）。
有一些方法可以解决这个问题，例如：PRPL 模式、prerender.io，或者你可以这么想，其实谷歌机器人在抓取单页应用程序时没有那么糟糕。
目前，如果要进行服务器端渲染，可以使用：




用于 React 的 Next.js；


用于 Vue 的 Nuxt.js；


用于 Angular 的 Angular Universal。




静态页面正在重新刮起一阵流行风，你可以看看 JAM Stack：
https://www.netlify.com/blog/2017/06/06/jamstack-vs-isomorphic-server-side-rendering/
它的主要思想是：预构建标记（静态页面），通过利用服务器的 API 在客户端成为动态单页面应用程序。这将在 2019 年真正改变服务器端渲染，我预测会有更多人使用像 GatsbyJS 这样的工具，而不是自己构建复杂的服务器端渲染逻辑。
 
Web组件
浏览器采用的 Web 组件终于离我们想要的标准越来越近了。2019 年，我们将看到更多关于 Web 组件的讨论，但它仍然不会在 2019 年达到临界点。你可以密切地关注它们，但不需要花费大量时间在掌握如何构建 Web 组件上。
你可以了解 React、Angular、Vue 和普通 HTML 的组件，但很难说 Web 组件会在什么时候得到大规模采用并为我们带来主要的好处。
性能
每个人都喜欢谈论性能。2019 年，代码拆分可能会成为标准实践，更多新的优化图像格式（如 WebP）将会发挥越来越重要的作用。
人们仍然会讨厌谷歌的 AMP。
你应该学习并为 2019 年做好准备的是：




针对 Angular、React、Vue 的特定优化；


代码拆分；


Tree Shanking；


只传输必要的 JavaScript 代码；


更加关注你正在使用的 NPM 库，并最大限度地减少库的大小；


制定性能预算；


通过使用 CDN 和浏览器优先级工具更好地确定资源优先级。




PWA
渐进式 Web 应用程序在 2019 年仍然会很热门，但它最复杂的功能可能不会流行起来（即推送通知）。
大多数情况下，你将使用 HTTPS、App Shell 和 Service Worker 来获得一些额外的脱机功能、安全性和性能。你应该学习如何构建 PWA，并使用像 Lighthouse 这样的工具来测试它。
Safari 最终为 PWA 添加了一些支持，实现渐进式 Web 应用程序功能可能会更容易一些。但说到底，你需要先学会使用 manifest.json 文件和 Service Worker。谷歌正在这方面努力推进，但不要指望在 2019 年会看到任何突破。
 
后端
别担心！2019 年的后端世界并不会像前端世界那样疯狂。
HTTPS 无处不在
需要将用户输入的数据发送到服务器的网站必须使用 HTTPS。如果你没有使用 HTTPS，谷歌将会惩罚你。幸运的是，HTTPS Everywhere 或 Gaddy 让迁移到 HTTPS 变得更容易。
REST 与 GraphQL
RESTful API 在 2019 年还会存在，你需要学习如何实现和设计这些 API。你应该学会使用 Node.js 和 Express.js 来创建 API 服务器，在 2019 年，这两个框架的组合仍然会占主导地位。
现在出现了很多有关 GraphQL 的炒作，但它还不是可以赢得所有市场的大赢家。了解 GraphQL 可以解决哪些问题，以及如何在 RESTful API 中用它来进行路由优化。这将是 2019 年最重要的趋势：不是如何单独使用 GraphQL，而是如何在极少数情况下使用 GraphQL 优化一些 RESTful API 路由。
HTTP2
HTTP2 变得越来越普遍，你需要知道如何使用这个协议来优化内容的传输。此外，HTTP3 正在开发当中，你可以关注它，但它并不是你在 2019 年需要过分关注的东西。
基础设施即服务
需要自己构建和管理服务器的场景越来越少，以下是 2019 年的主要选择。




Digital Ocean——用于简单的服务器。


Heroku——用于简单和集成的服务器和部署。


Now——用于超级简单的部署。


Firebase——用于托管基础设施和数据库。


AWS——几乎任何你想要的东西，你可以永远不需要考虑自己管理服务器。




你需要学习 SQL

Firebase、AWS 等托管数据库将继续增长，但你还是需要学习 SQL。2019 年，像 PostgreSQL 这样的数据库将继续发展，而像 MongoDB 这样的 NoSQL 数据库似乎会有所下降。你可能需要了解每种方案的优点和缺点，因为在数据库领域并没有可以解决所有问题的完美解决方案。
不要把搜索给忘了
搜索可能不是绝对必要的，但它是 Web 的重要组成部分。2019 年，全栈开发者可以试着了解下面两个平台：




Elasticsearch；


Algolia Search；




你可能需要学习 Redis
了解使用 Redis 作为缓存以及内存存储的工作原理。缓存和内存存储是 2019 年需要学习的重要概念，可以用它们来优化你的系统。Redis 是理解这些概念的一个很好的起点。
 
测试 
学习三种测试类型
很多人都在讨论这个话题，但为了简单问题，可以将测试分解为三种类型：




单元测试：给定输入，测试输出，用于测试单个函数或类。


集成测试：测试流程或组件是否按预期运行（包括副作用）。


端到端测试：测试用户的实际行为，不仅仅是测试一个简单的功能。




保持简单
测试框架有很多选择，但下面是 2019 年最好的两个组合：




Jest（https://jestjs.io/）


Mocha + Chai + Sinon + Istanbul




将 Jest 视为一体化的测试框架，就不需要像第二个选项那样添加其他工具和库。如果你想要简单些，只需使用 Jest。如果你想要更多可定制性和模块化，请选择 Mocha。
如果你还了解这些，那是锦上添花：Mock、Spy、存根和快照测试。
适当的端到端测试就可以了
进行端到端测试需要公司投入大量的成本，所以在你的职业生涯中有可能会也有可能不会遇到这种测试。但不管怎样，在 2019 年，你最好可以学习这些框架，或至少可以了解一下：




Cypress；


Nightwatch；


Protractor，适合 Angular 爱好者。




 
移动开发
跟移动开发说再见？

移动开发在 2019 年的日子可能会有点难过。应用程序的下载量不像过去那么多，而且最热门的下载要么是游戏，要么是大型科技公司的应用程序。2019 年，移动端 Web 浏览量将超过原生移动应用程序。因此，对于全栈开发者和移动开发者而言，他们应该将更多的关注点放在移动设备 Web 应用程序上（例如使用 PWA）。
iOS 和 Android 仍然是企业所需要的重要开发技能，但在过去几年中对它们的需求一直在下降，似乎出现了从原生移动开发到 React Native 引领的混合开发（或接近原生）的重大转变。如果你看一下上面的图表，React Native 已经取代了 Swift，它是原生 iOS 开发的主要编程语言。
以下是你需要关注的开发技术：
React Native 在 2018 年遭遇了一些挫折，一些大公司在博文说他们正在放弃它。但这些公司都曾经尝试将 React Native 添加到他们现有的 iOS 或 Android 代码库中。如果你是这方面的新手，对于你来说，它仍然是 2019 年的一个很好的选择。它将会继续增长下去。
Flutter 在 2018 年非常火爆，但现在判断它在 2019 年将会怎样发展还为时过早。你需要关注它，但到目前为止，它并没有带来比 React Native 更显著的优势。
Ionic 和 NativeScript 的使用将在 2019 年逐渐减少，除非你正在使用 Angular，否则你不应该关注它们。
所以，在 2019 年，请继续关注 React Native。
 
工具
你应该使用的 NPM 包




Prettier——让你可以专注于你正在写的代码，而不是去关心代码的格式；
https://prettier.io/


eslint——保持代码整洁；
https://eslint.org/


date-fns——moment.js 的轻量级替代品；
https://date-fns.org/


lodash——主要用于 throttle() 和 debounce() 函数；
https://lodash.com/


rambda——如果你真的喜欢函数式编程。
https://ramdajs.com/




 
JavaScript
JavaScript 是饱受争议的编程语言之一。2018 年，静态类型在 JavaScript 的动态类型领域变得越来越受欢迎。那么竞争者有哪些？




TypeScript：可以编译为 JavaScript 的 JavaScript 超集。


Flow：JavaScript 的静态类型检查器。


Reason：利用了 JavaScript 和 OCaml 生态系统的类型语言。


PureScript：一种强类型语言，可以编译为 JavaScript，使用 Haskell 开发。


Elm：纯粹的函数式编程语言，可以编译成 JavaScript。




关于静态与动态类型语言的讨论由来已久，不会很快就得出结论。以上这些都不会取代 JavaScript 作为 Web 主要编程语言的主导地位。但是，Angular 和 Vue 都采用了 TypeScript，并将其作为开发者社区的标准，因此，TypeScript 可能会继续增长，并超越上述其他语言。
你需要学习 TypeScript 的基础知识及其原理（以及静态类型的好处），但要注意，它并非写出好代码的唯一方法。要写出好代码，可以先关注如何写出好的单元测试。
模块捆绑器
Webpack 4 和 Parcel 是 2019 年的主要工具。它们都朝着降低复杂性和更多“为用户着想”的方向发展，很多前端库都提供了 CLI。学习这两个工具，但请记住，CLI 在项目开始时帮你消除掉最初 80％的复杂性。如果要发布 NPM 包，请使用 Rollup。
 
计算机科学基础
数据结构 + 算法
讨论技术趋势的文章很少会提到计算机科学基础知识。但这可能是最重要的主题，而且我可以非常自信地说，这个趋势具有 99.99999％的准确率：如果你想在 2019 年和未来几年成为一个全栈开发者，计算机科学基础是非常重要的。计算机科学基本原理不怎么会发生变化，并且已经存在了很长时间，不会像开发库那样，一旦有新东西出来就变得过时了。
容器和 serverless
容器为我们提供了与几年前完全不同的架构，其中的一个主要的想法是 serverless。serverless 并不是说不需要服务器了，而是说有人为你管理服务器（基础设施），你可以专注于自己的应用程序逻辑，无需担心扩展性等问题。
serverless 的流行始于 2017 年，并持续到了 2018 年。2019 年，我们将看到一些相同的常见用例，比如 AWS API Gateway 与 AWS Lambda 的结合，供前端应用程序代码调用。
在降低成本的同时提高性能是一个好主意，如果冷启动问题在 2019 年可以得到解决，那么它将变得越来越流行。
平台即服务 / 后端即服务
亚马逊、谷歌和 Azure 将在 2019 年争夺服务器市场，它们当中的每一个都提供了全托管的服务。
AppSync、Amplify、App Services、App Engine 等服务将继续发展，但由于程序员很难放弃如此多的控制权（除非是小型的个人项目），所以它们并不会真正有大起色。
2019 年，Azure 将主导企业市场，AWS 将主导一般的开发者市场，而谷歌将主导机器学习市场。
机器学习
2019 年，你需要学习并了解如何通过以下 API 使用机器学习模型：




Google Cloud AI；


亚马逊机器学习；


Azure 机器学习；




除了之前列出的平台即服务和后端即服务，还会有更多的服务出现，这些大公司提供的机器学习 API 和模型将在 2019 年成为一个更重要的趋势。你应该学会在未来的项目中使用其中一些（不用担心，它们使用起来没有那么难，就像使用大多数其他 API 一样）。
2019 年，我们将可以看到机器学习 API 在 Web 上的应用，而不是从头开始构建自己的机器学习模型。因为与上述大型科技巨头不同，大多数人或公司无法为机器学习提供足够的资源或数据。
WebAssembly
WebAssembly 集将继续缓慢改进，但仍然只有一小部分开发者会使用它（主要用于游戏、图像处理）。你可以先了解它，在几年后等它成为主流时你就是这方面的专家了。
以上是我的个人意见，不管怎样，学习新东西绝不是一个坏主意。
但不论技术风向如何变化，葡萄城一直都秉承着为开发者着想，赋能开发者的企业理念，同时为开发者提供技术领先、功能可靠的一站式开发工具、解决方案及技术服务。
 
https://www.cnblogs.com/powertoolsteam/p/10283365.html
**************************************************
SpringBoot 2.x（九）：遇到跨域不再慌
什么是跨域
首先，我们需要了解一下一个URL是怎么组成的：

// 协议 + 域名（子域名 + 主域名） + 端口号 + 资源地址
http: + // + www.baidu.com + :8080/

只要协议，子域名，主域名，端口号这四项组成部分中有一项不同，就可以认为是不同的域，不同的域之间互相访问资源，就被称之为跨域。
随着前后端分离开发的越来越普及，会经常遇到跨域的问题，当我们在浏览器中看到这样的错误时，就需要意识到遇到了跨域：

解决跨域的几种方案
首先，我们使用vue-cli来快速构建一个前端项目，然后使用axios来向后台发送ajax请求。然后在控制台中打印出返回信息。这里就不再多做赘述，后面我会单独写一篇文章来讲一下如何使用vue-cli快速创建一个vue项目。

这里不再讲解使用jsonp的方式来解决跨域，因为jsonp方式只能通过get请求方式来传递参数，而且有一些不便之处。

下面的几种方式都是通过跨域访问技术CORS(Cross-Origin Resource Sharing）来解决的。具体的实现原理我们不做深入的探究，这节课的目的是解决跨域问题~
方法一：注解
在Spring Boot 中给我们提供了一个注解@CrossOrigin来实现跨域，这个注解可以实现方法级别的细粒度的跨域控制。我们可以在类或者方添加该注解，如果在类上添加该注解，该类下的所有接口都可以通过跨域访问，如果在方法上添加注解，那么仅仅只限于加注解的方法可以访问。
@RestController
@RequestMapping("/user")
@CrossOrigin
public class UserController {
   @Autowired
    private UserService userService;

   @RequestMapping("/findAll")
    public Object findAll(){
        return userService.list();
    }
}
现在再去测试一下：

Bingo，成功！
方法二：实现WebMvcConfigurer
这里可以通过实现WebMvcConfigurer接口中的addCorsMappings()方法来实现跨域。
@Configuration
class CORSConfiguration implements WebMvcConfigurer {
    @Override
    public void addCorsMappings(CorsRegistry registry) {
        registry.addMapping("/**");
    }
}
下面我们将刚刚加上的注解给注释掉，看看能不能访问到这个接口：

不出我们所料，果然还是可以的~
方法三：Filter
我们可以通过实现Fiter接口在请求中添加一些Header来解决跨域的问题：
@Component
public class CORSFilter implements Filter {

    @Override
    public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain)
            throws IOException, ServletException {
        HttpServletResponse res = (HttpServletResponse) response;
        res.addHeader("Access-Control-Allow-Credentials", "true");
        res.addHeader("Access-Control-Allow-Origin", "*");
        res.addHeader("Access-Control-Allow-Methods", "GET, POST, DELETE, PUT");
        res.addHeader("Access-Control-Allow-Headers", "Content-Type,X-CAF-Authorization-Token,sessionToken,X-TOKEN");
        if (((HttpServletRequest) request).getMethod().equals("OPTIONS")) {
            response.getWriter().println("ok");
            return;
        }
        chain.doFilter(request, response);
    }
    @Override
    public void destroy() {
    }
    @Override
    public void init(FilterConfig filterConfig) throws ServletException {
    }
}
不出意外的话，应该也可以在控制台获取到返回信息。
Nginx配置解决跨域问题
如果我们在项目中使用了Nginx，可以在Nginx中添加以下的配置来解决跨域（原理其实和Filter类似，只不过把活儿丢给了运维🤔）
location / {
   add_header Access-Control-Allow-Origin *;
   add_header Access-Control-Allow-Headers X-Requested-With;
   add_header Access-Control-Allow-Methods GET,POST,PUT,DELETE,OPTIONS;

   if ($request_method = 'OPTIONS') {
     return 204;
   }
}
原创文章，才疏学浅，如有不对之处，万望告知！
公众号

您的推荐就是对我最大的支持！

https://www.cnblogs.com/viyoung/p/10285786.html
**************************************************
Java String：重要到别人只能当老二的字符串类

字符串，是Java中最重要的类。这句肯定的推断不是Java之父詹姆斯·高斯林说的，而是沉默王二说的，因此你不必怀疑它的准确性。
关于字符串，有很多的面试题，但我总觉得理论知识绕来绕去没多大意思。你比如说：String cmower = new String("沉默王二");定义了几个对象?
我总觉得问我这样的问题，就好像是在拷问我：“既然你家买了冰箱，你难道不应该知道冰箱制冷的原理？”
再说，为什么要用String cmower = new String("沉默王二");而不是String cmower = "沉默王二";？
我劝各位面试官不要再缠住这样的问题不放了，切记“学以致用”。理论知识如果一直是在绕弯弯，那真的毫无价值。如果要我来做面试官，我想要问的问题是：“你平常是怎么判断两个字符串相等的？是用equals()还是==？”
前言就说这么多。接下来，我们来探讨几个实用的知识点。
01 字符串是不可变的
我们来看一下String类的定义：
public final class String    implements java.io.Serializable, Comparable<String>, CharSequence {}
可以发现，String类是final类型的，因此不能被继承。
如果类可以被继承，那么就会破坏类的不可变性机制。因为子类可以覆盖父类的方法，并且可以改变父类的成员变量值，一旦子类以父类的形式出现时，就不能保证类是不可变的。
String类的不可变性有什么好处呢？
1）作为HashMap的键。
因为字符串是不可变的，因此它在创建的时候哈希码（hash code）就计算好了。这也就意味着每次在使用一个字符串的哈希码的时候不用重新计算一次，这样更加高效，很适合作为HashMap中的键。
2）线程安全。
同一个字符串对象可以被多个线程共享，如果访问频繁的话，可以省略同步和锁等待的时间，从而提升性能。
3）字符串常量池的需要。
稍后介绍。
特别要注意的是，String类的所有方法都没有改变字符串本身的值，都是返回了一个新的对象。
02 字符串常量池
在Java中，常用的创建字符串的方式有两种：
String cmower = "沉默王二";String cmowsan = new String("沉默王三");
cmower使用双引号，cmowsan使用new关键字，它们有什么区别呢？
答案如下：
String cmower = "沉默王二";String cmower1 = "沉默王二";System.out.println(cmower == cmower1); // 输出trueString cmowsan = new String("沉默王三");String cmowsan1 = new String("沉默王三");System.out.println(cmowsan == cmowsan1); // 输出false
双引号创建的相同字符串使用==判断时结果为true，而new关键字创建的相同字符串使用==判断时结果为false。
这是为什么呢？
String在Java中使用过于频繁，为了避免在系统中产生大量的String对象，Java的设计者引入了“字符串常量池”的概念。
当使用双引号创建一个字符串时，首先会检查字符串常量池中是否有相同的字符串对象，如果有，则直接从常量池中取出对象引用；如果没有，则新建字符串对象，并将其放入字符串常量池中，并返回对象引用。
这也就是说，"沉默王二"是放在字符串常量池中的，cmower和cmower1两个字符串对象引用是相同的。
而new关键字创建的字符串对象是不涉及字符串常量池的，直接放在堆中，也就是说，虽然cmowsan和cmowsan1都叫沉默王三，但不一个人。
强烈建议：不要使用new关键字的形式创建字符串对象。
03 +号和StringBuilder
由于字符串是不可变的，因此字符串在进行拼接的时候会创建新的字符串对象。大家都知道，内存是一定的，因此对象创建多了就会影响系统性能。
StringBuilder正是为了解决字符串拼接产生太多中间对象的问题而提供的一个类，可以通过append()方法把字符串添加到已有序列的末尾，非常高效。
那么有人在进行字符串拼接的时候，就会产生疑惑：“我到底是用+号还是StringBuilder？”
我们先来看这样一段代码：
String chenmo = "沉默";String wanger = "王二";System.out.println(chenmo + wanger);
这段代码是怎么编译的呢？可以使用JAD（Java反编译工具）来看一看。
String s = "\u5A0C\u5910\u7CAF";String s1 = "\u941C\u5B29\u7C29";System.out.println((new StringBuilder()).append(s).append(s1).toString());
你是不是看到了StringBuilder的影子？
没错，使用+号进行字符串拼接的时候，Java编译器实际是通过StringBuilder类来完成的。
难道可以使用+号来随意拼接字符串？反正Java编译器已经自动地为我们优化了。
但事实并非如此，来看这样一段代码：
String cmowers = "";for (int i = 0; i < 9; i++) {    cmowers += "沉默王二";}System.out.println(cmowers);
闭上眼睛先想一想，Java编译器会怎么做？我们期望的结果是在循环外部就创建StringBuilder，Java编译器能如我们所愿吗？
JAD反编译后的结果如下：
String s = "";for(int i = 0; i < 10; i++)    s = (new StringBuilder()).append(s).append("\u5A0C\u5910\u7CAF\u941C\u5B29\u7C29").toString();System.out.println(s);
这么看来，StringBuilder是在for循环内部创建的，也就是说会创建10次。天呐，这可不是我们期望的结果！我们只希望StringBuilder创建一次。
没办法，Java编译器是做不到的，只能靠我们自己：
StringBuilder cmowers = new StringBuilder();for (int i = 0; i < 9; i++) {    cmowers.append("沉默王二");}System.out.println(cmowers);
强烈建议：如果只是三四个字符串的拼接，尽管使用+号操作符，别想什么性能优化（举个例子，你离目的地只有100米，你是打算打个出租车，还是自己步行走过去？）；如果遇到多于四个字符串的拼接，或者需要用到循环来拼接，那就选择StringBuilder。
在我年轻的时候，我还会犯这样一个错误：
StringBuilder cmowers = new StringBuilder();for (int i = 0; i < 9; i++) {    cmowers.append("沉默王二" + "和他的读者朋友们");}System.out.println(cmowers);
我去，竟然在append()方法的内部使用+号！因为这个错误，我差点没被领导打死。你可要小心点。
04 关于concat()
除了使用+号和StringBuilder对字符串进行拼接，还可以使用String类的concat()方法。
concat()方法只不过是String类的一个方法而已，为什么我要单独拎出来说呢？
因为之前我要在JSP页面的EL表达式中拼接字符串，刚开始想到的是用+号操作符，但EL表达式不是Java，+号操作符是不能拼接字符串的。我当时竟然没想起来用concat()！
重新铭记一下：
${item.username.concat('-').concat(item.realname)}
05 关于intern()
关于字符串的性能问题，我常在一些技术文章中看到这样的建议：“如果一个字符串使用的频率非常高，建议使用String.intern()将其缓存。”
但我并不建议你这么做，因为这个方法要显式的调用，这样很麻烦；况且，在代码编写阶段，怎么可能知道哪个字符串使用频率很高呢？
06 关于StringUtils
据我的编程经验来看，字符串的操作往往需要用到一个工具类，那就是org.apache.commons.lang3.StringUtils（null安全的，也就是说，StringUtils类的方法可以接受为null的字符串，但不会抛出NullPointerException）。
不过，我最常用的方法就那么几个：


方法等价



IsEmpty(String str)
str == null || str.length == 0


isBlank(String str)
str == null || str.length == 0 || str.trim().length == 0


join(Object[] arrey)
把数组中的元素连接成一个字符串返回



 
 
推荐阅读：
Java异常处理：给程序罩一层保险Java集合类：我其实没那么简单
https://www.cnblogs.com/qing-gee/p/10277688.html
**************************************************
004.Ceph块设备基础使用
一 基础准备


参考《002.Ceph安装部署》文档部署一个基础集群；
新增节点主机名及IP在deploy节点添加解析：



  1 [root@deploy ~]# echo "172.24.8.75 cephclient" >>/etc/hosts



配置国内yum源：



  1 [root@cephclient ~]# yum -y update
  2 [root@cephclient ~]# rm /etc/yum.repos.d/* -rf
  3 [root@cephclient ~]# wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo
  4 [root@cephclient ~]# yum -y install epel-release
  5 [root@cephclient ~]# mv /etc/yum.repos.d/epel.repo /etc/yum.repos.d/epel.repo.backup
  6 [root@cephclient ~]# mv /etc/yum.repos.d/epel-testing.repo /etc/yum.repos.d/epel-testing.repo.backup
  7 [root@cephclient ~]# wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo

 
二 块设备

2.1 添加普通用户

  1 [root@cephclient ~]# useradd -d /home/cephuser -m cephuser
  2 [root@cephclient ~]# echo "cephuser" | passwd --stdin cephuser	#cephclient节点创建cephuser用户
  3 [root@cephclient ~]# echo "cephuser ALL = (root) NOPASSWD:ALL" > /etc/sudoers.d/cephuser
  4 [root@cephclient ~]# chmod 0440 /etc/sudoers.d/cephuser
  5 [root@deploy ~]# su - manager
  6 [manager@deploy ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub cephuser@172.24.8.75

 
2.2 安装ceph-client

  1 [root@deploy ~]# su - manager
  2 [manager@deploy ~]$ cd my-cluster/
  3 [manager@deploy my-cluster]$ vi ~/.ssh/config
  4 Host node1
  5    Hostname node1
  6    User cephuser
  7 Host node2
  8    Hostname node2
  9    User cephuser
 10 Host node3
 11    Hostname node3
 12    User cephuser
 13 Host cephclient
 14    Hostname cephclient						#新增cephclient节点信息
 15    User cephuser
 16 [manager@deploy my-cluster]$ ceph-deploy install cephclient	#安装Ceph

 
注意：若使用ceph-deploy部署的时候出现安装包无法下载，可在部署时候指定ceph.repo为国内源：

  1 ceph-deploy install cephclient --repo-url=https://mirrors.aliyun.com/ceph/rpm-mimic/el7/ --gpg-url=https://mirrors.aliyun.com/ceph/keys/release.asc


  1 [manager@deploy my-cluster]$ ceph-deploy admin cephclient

提示：为方便后期deploy节点管理cephclient，在CLI中使用命令中简化相关key的输出，可将key复制至相应节点。ceph-deploy 工具会把密钥环复制到/etc/ceph目录，要确保此密钥环文件有读权限（如 sudo chmod +r /etc/ceph/ceph.client.admin.keyring ）。
2.3 创建pool

  1 [manager@deploy my-cluster]$ ssh node1 sudo ceph osd pool create mytestpool 64

2.4 初始化pool

  1 [root@cephclient ~]# ceph osd lspools
  2 [root@cephclient ~]# rbd pool init mytestpool

 
2.5 创建块设备

  1 [root@cephclient ~]# rbd create mytestpool/mytestimages --size 4096 --image-feature layering

2.6 确认验证

  1 [root@cephclient ~]# rbd ls mytestpool
  2 mytestimages
  3 [root@cephclient ~]# rbd showmapped
  4 id pool       image        snap device
  5 0  mytestpool mytestimages -    /dev/rbd0
  6 [root@cephclient ~]# rbd info mytestpool/mytestimages

 

2.7 将image映射为块设备

  1 [root@cephclient ~]# rbd map mytestpool/mytestimages --name client.admin
  2 /dev/rbd0

 
2.8 格式化设备

  1 [root@cephclient ~]# mkfs.ext4 /dev/rbd/mytestpool/mytestimages
  2 [root@cephclient ~]# lsblk

 

2.9 挂载并测试

  1 [root@cephclient ~]# sudo mkdir /mnt/ceph-block-device
  2 [root@cephclient ~]# sudo mount /dev/rbd/mytestpool/mytestimages /mnt/ceph-block-device/
  3 [root@cephclient ~]# cd /mnt/ceph-block-device/
  4 [root@cephclient ceph-block-device]# echo 'This is my test file!' >> test.txt
  5 [root@cephclient ceph-block-device]# ls
  6 lost+found  test.txt

 
2.10 自动map

  1 [root@cephclient ~]# vim /etc/ceph/rbdmap
  2 # RbdDevice     Parameters
  3 #poolname/imagename id=client,keyring=/etc/ceph/ceph.client.keyring
  4 mytestpool/mytestimages id=admin,keyring=/etc/ceph/ceph.client.admin.keyring

 
2.11 开机挂载

  1 [root@cephclient ~]# vi /etc/fstab
  2 #……
  3 /dev/rbd/mytestpool/mytestimages    /mnt/ceph-block-device  ext4    defaults,noatime,_netdev    0 0

 
2.12 rbdmap开机启动

  1 [root@cephclient ~]# systemctl enable rbdmap.service
  2 [root@cephclient ~]# df -hT				#查看验证

 

提示：若出现开机后依旧无法自动挂载，rbdmap也异常，可如下操作：

  1 [root@cephclient ~]# vi /usr/lib/systemd/system/rbdmap.service
  2 [Unit]
  3 Description=Map RBD devices
  4 WantedBy=multi-user.target			#需要新增此行
  5 #……

 
https://www.cnblogs.com/itzgr/p/10285882.html
**************************************************
ASP.NET MVC5多语言切换快速实现方案
功能
实现动态切换语言,Demo 做了三种语言库可以切换,包括资源文件的定义,实体对象属性设置,后台代码Controller,IAuthorizationFilter,HtmlHelper的是实现,做法比较简单,配合我之前发布的# MVC Scaffolding SmartCode-Engine 更新 模板中新增了多语言资源文件的生成功能,发现我的这个框架和目前很流行的ABP框架是类似更有点像收费版的Asp.net Zero,只是我做的更加轻量级,更方便,更快速,可惜Asp.net Core 下的Scaffolding这块扩展不想MVC5那么容易.这块还需要研究,下一步就准备升级到asp.net core.

Github download Demo
 
具体实现方法


定义实体类通过Display属性定义Name ResourceType,需要读取的语言库资源文件







生成资源文件通过mvc scaffolding扩展工具会自动生成对应实体对象的3个资源文件默认中文,繁体,英文繁体需要自己翻译,英文内容根据字段名定义后已大写字母分割 DateTime 现实 Date Time

页面功能按钮语言资源文件库









 前后端代码实现语言切换功能


-选择切换语言

 



2.Js代码

/* multiple lang dropdown */
$('#dropdownlang-dropdown-menu').on('click', 'a', function () {
  const lang = this.text;
  const flag = this.firstElementChild.className;
  const culture = this.firstElementChild.getAttribute("culture");
  $('#dropdownlang').children()[0].className = flag;
  $('#dropdownlang').children()[1].innerHTML = lang;
  localStorage.setItem('lang-text', lang);
  localStorage.setItem('lang-css', flag);
  localStorage.setItem('lang-culture', culture);
  $.get('/Account/SetCulture?lang=' + culture).then(res => {
    if (res.success) {
      location.reload();
    }

  });
});
$(function () {
  const lang = localStorage.getItem('lang-text');
  const css = localStorage.getItem('lang-css');
  const culture = localStorage.getItem('lang-culture');
    scripttag = document.createElement("script");
    scripttag.type = "text/javascript";
    scripttag.src = src;
    document.body.appendChild(scripttag);
    $.parser.parse();
 
 
  };
  if (lang && css && culture) {
    $('#dropdownlang').children()[0].className = css;
    $('#dropdownlang').children()[1].innerHTML = lang;

  }
});

后端代码



[HttpGet]
    public ActionResult SetCulture(string lang) {
      //这里设置CultureInfo是多余的
      switch (lang.Trim())
      {
        case "en":
          CultureInfo.CurrentCulture = new CultureInfo("en-US");
          CultureInfo.CurrentUICulture = new CultureInfo("en-US");
          break;
        case "cn":
          CultureInfo.CurrentCulture = new CultureInfo("zh-CN");
          CultureInfo.CurrentUICulture = new CultureInfo("zh-CN");
          break;
        case "tw":
          CultureInfo.CurrentCulture = new CultureInfo("zh-TW");
          CultureInfo.CurrentUICulture = new CultureInfo("zh-TW");
          break;
      }
     //这里设置CultureInfo是多余的
      var cookie = new HttpCookie("culture", lang)
      {
        Expires = DateTime.Now.AddYears(1)
      };
      Response.Cookies.Add(cookie);
      return Json(new { success = true }, JsonRequestBehavior.AllowGet);

    }

CultureFilter 这是关键 这是没有使用RouteData,通过修改url来保存当前语言要改的地方很多还要修改路由规则,所以我就用Cookies来保存



public class CultureFilter : IAuthorizationFilter
  {
    private readonly string defaultCulture;

    public CultureFilter()
    {
      this.defaultCulture = "cn";
    }

    public void OnAuthorization(AuthorizationContext filterContext)
    {
      var culture = filterContext.HttpContext.Request.Cookies["culture"];
      var lang = defaultCulture;
      if (culture != null && culture.Value != null)
      {
        lang = culture.Value;
        filterContext.HttpContext.Response.Cookies.Set(culture);
      }
      switch (lang.Trim())
      {
        case "en":
          CultureInfo.CurrentCulture = new CultureInfo("en-US");
          CultureInfo.CurrentUICulture = new CultureInfo("en-US");
          //Thread.CurrentThread.CurrentUICulture = new CultureInfo("en-US");
          //Thread.CurrentThread.CurrentCulture = new CultureInfo("en-US");
          break;
        case "cn":
          CultureInfo.CurrentCulture = new CultureInfo("zh-CN");
          CultureInfo.CurrentUICulture = new CultureInfo("zh-CN");
          //Thread.CurrentThread.CurrentUICulture = new CultureInfo("zh-CN");
          //Thread.CurrentThread.CurrentCulture = new CultureInfo("zh-CN");
          break;
        case "tw":
          CultureInfo.CurrentCulture = new CultureInfo("zh-TW");
          CultureInfo.CurrentUICulture = new CultureInfo("zh-TW");
          //Thread.CurrentThread.CurrentUICulture = new CultureInfo("zh-TW");
          //Thread.CurrentThread.CurrentCulture = new CultureInfo("zh-TW");
          break;
      }
    }
  }

HtmlHelper 代码实现语言的输出



/// <summary>
    /// 多语言切换
    /// </summary>
    /// <param name="helper"></param>
    /// <param name="name"></param>
    /// <returns></returns>
    public static HtmlString L(this HtmlHelper helper, string name) {

      var resource = new System.Resources.ResourceManager(typeof(WebApp.resource.Global));
      var text = resource.GetString(name);
      return new HtmlString(text ?? name);
    }
    /// <summary>
    /// 前端easyui或是其它js相关的比如提示信息也需要转换必须在这里加载不同的语言文件
    /// </summary>
    public static HtmlString LangScriptTag(this HtmlHelper helper,string defaultsrc) {
      var src = defaultsrc;
      var lang = CultureInfo.CurrentCulture.Name;
      switch (lang)
      {
        case "en-US":
          src = "/Scripts/easyui/locale/easyui-lang-en.js";
          break;
        case "zh-CN":
          src = "/Scripts/easyui/locale/easyui-lang-zh_CN.js";
          break;
        case "zh-TW":
          src = "/Scripts/easyui/locale/easyui-lang-zh_TW.js";
          break;
        default:
          src = defaultsrc;
          break;
      }
      return new HtmlString($"<script src=\"{ src }\"></script>");
    }

//html代码
<div class="btn-group btn-group-sm">
<button onclick="append()" class="btn btn-default"> <i class="fa fa-plus"></i> @Html.L("Add") </button>
</div>
<div class="btn-group btn-group-sm">
<button onclick="removeit()" class="btn btn-default"> <i class="fa fa-trash-o"></i> @Html.L("Delete") </button>
</div>
<div class="btn-group btn-group-sm">
<button onclick="accept()" class="btn btn-default"> <i class="fa fa-floppy-o"></i> @Html.L("Save") </button>
</div>

 



https://www.cnblogs.com/neozhu/p/10285874.html
**************************************************
Android 实现卡片翻转的动画（翻牌动画）

Android 实现卡片翻转的动画（翻牌动画）
需求描述
　　点击卡片，卡片翻转过来显示内容。　　点击左边的卡片，将卡片翻转显示右边的图片结果。
功能实现
　　因为要翻转所以使用动画来完成翻转的动画。动画分为两部分，一部分是左边的布局以中心垂直线从左向右旋转，旋转 180 度之后隐藏，另一部分是右边的布局以中心垂直线从右向左旋转，旋转 180 度之后显示。　　这种动画涉及到播放顺序的问题，所以动画使用 Animator 属性动画实现。 布局 
    <FrameLayout
        android:id="@+id/activity_main_result_layout_fl"
        android:layout_width="@dimen/activity_main_result_width"
        android:layout_height="@dimen/activity_main_result_height"
        android:layout_gravity="center"
        android:layout_marginTop="@dimen/activity_main_result_margin_top">

        <ImageView
            android:id="@+id/activity_main_result_iv"
            android:layout_width="match_parent"
            android:layout_height="match_parent"
            android:src="@mipmap/result" />

        <TextView
            android:id="@+id/activity_main_result_tv"
            android:layout_width="match_parent"
            android:layout_height="match_parent"
            android:gravity="center"
            android:textColor="@android:color/white"
            android:textSize="@dimen/activity_main_text_size"
            android:visibility="gone" />

    </FrameLayout>

 动画文件 


rotate_in_anim .xml
<?xml version="1.0" encoding="utf-8"?>
<set xmlns:android="http://schemas.android.com/apk/res/android">
  <!--消失-->
  <objectAnimator
      android:duration="0"
      android:propertyName="alpha"
      android:valueFrom="1.0"
      android:valueTo="0.0" />

  <!--旋转-->
  <objectAnimator
      android:duration="1000"
      android:propertyName="rotationY"
      android:valueFrom="-180"
      android:valueTo="0" />

  <!--出现-->
  <objectAnimator
      android:duration="0"
      android:propertyName="alpha"
      android:startOffset="500"
      android:valueFrom="0.0"
      android:valueTo="1.0" />
</set>



rotate_out_anim.xml
<?xml version="1.0" encoding="utf-8"?>
<set xmlns:android="http://schemas.android.com/apk/res/android">
  <!--旋转-->
  <objectAnimator
      android:duration="1000"
      android:propertyName="rotationY"
      android:valueFrom="0"
      android:valueTo="180" />

  <!--消失-->
  <objectAnimator
      android:duration="0"
      android:propertyName="alpha"
      android:startOffset="500"
      android:valueFrom="1.0"
      android:valueTo="0.0" />
</set>



 播放动画 
    tv_result.setVisibility(View.VISIBLE);
    tv_sure.setVisibility(View.VISIBLE);
    AnimatorSet inAnimator = (AnimatorSet) AnimatorInflater.loadAnimator(this, R.animator.rotate_in_anim);
    AnimatorSet outAnimator = (AnimatorSet) AnimatorInflater.loadAnimator(this, R.animator.rotate_out_anim);
    int distance = 16000;
    float scale = getResources().getDisplayMetrics().density * distance;
    iv_result.setCameraDistance(scale); //设置镜头距离
    tv_result.setCameraDistance(scale); //设置镜头距离
    outAnimator.setTarget(iv_result);
    inAnimator.setTarget(tv_result);
    outAnimator.start();
    inAnimator.start();
    outAnimator.addListener(new AnimatorListenerAdapter() {
        @Override
        public void onAnimationEnd(Animator animation) {
            super.onAnimationEnd(animation);
            iv_result.setVisibility(View.GONE);
            iv_result.setAlpha(1.0f);
            iv_result.setRotationY(0.0f);
        }
    });

　　注意：动画的实现方式是使用了属性动画 Animator 实现的，如果动画需要再次显示，那么在动画结束之后就需要将控件的属性值设置为初始值，因为属性动画会修改控件的属性值为动画结束时的属性值。
参考文章
http://lishuaishuai.iteye.com/blog/2297056

https://www.cnblogs.com/zhangmiao14/p/10285862.html
**************************************************
2018年年终总结
  2018年最大的感受是人生太艰难了。
  1. 首先说说2018年的成果，做了如下事情：
 　1. 在北京定居买房了
    2. 跳槽到了某厂商做语音助手
    3. 和女友约定明年结婚
  2. 2018年做的遗憾的事情: 
  　1. 因为买房背负了高额的负债，每月房贷还的很艰难
  　2.工作加班非常的多，工作没有明显的产出，但身体健康下降的非常明显，严重影响了家庭的和谐
  　3.本身的成长速度严重低于预期
  3. 2018年回顾之买房
     买房是每个在北京工作的人绕不去的坎，通过家里的支持也好，自己的努力也好，能够上车的尽量上车。但是买房，不应该是生活的全部。只是现在的房贷还的很艰难，都快成为生活的全部了，心累。这个到底如何权衡，看个人自己的取舍。
     之前的想法很简单，在北京找一个同职业的的女朋友，两个人一起努力，大概率有机会在北京定居的，多久买房是时间的问题。从现在来看，这个目标算是提前实现了。然而这中间出来很多的变故，导致现在负债太高，又是出乎意料的。这件事情得到了两个教训，第一个事情，做事情一定要想明白了，自己想要什么，太过贪婪，超过自己的承受极限很容易就把人压垮了。第二个，一段时间的重点最好聚焦，比如今年我工作很忙，买房的事情前期基本交给了女朋友，导致她一个人承受了太多压力，出现很多的争吵。她认为我没有担当，我认为她不够理解我，导致了太多了争吵。两个人最重要的是目标一致，大家做的事情互相符合对方预期，家庭才能和睦。如果今年一开始就能达成一致，今年会好过很多。
  4.2018年回顾之跳槽到某互厂商做语音助手
    坦诚一点来讲今年是工作以来，最为痛苦的一年。一方面是和mentor沟通有很严重的问题，他也许自负聪明，或者是本身真的很厉害，或者本身也没有想明白，说话感觉老是只说一半，剩下一半完全是靠你自己领悟。做了大量工作，尝试了大量错误，每天从早上九点工作到晚上十点，做的工作却连日报里面都没有资格写入。一方面mentor 本身脾气也很急躁，说话也非常伤人。我经常做事做到怀疑人生。大家都是打工的，何必呢。
    一开始女友安慰我说，mentor 本身是有问题的，我承认，然后这个事实对解决问题没有什么帮助，除了换组，我只能改变自己。他经常吐槽我做事很慢，那么首先一点我得把能做的事情，更高效率的做完，更熟练的使用基本工具，基本命令，每次做事一次比一次快，这个需要经常总结，把常用命令记住，而不是每次去网上查，这一块新的一年要继续加强。第二他吐槽我代码很丑，基本语法不熟悉，工程能力差，那么我就把基本的语法再仔细看看，代码更加模块化，目前的代码我觉得最大的问题应该是变量的命名还有很大问题，这一块在code review 方面老是被打回, 其次是代码不够简洁，对基本的API还是不够了解，代码很冗余，估计新的一年还要继续优化。第三做了很多业务逻辑方面的优化，感觉对自己其实是没有任何提升的。
    在新的工作岗位呆了一年，做的很辛苦，有两点感受是很明显的，团队氛围是很重要的，团队氛围好，沟通顺畅才有更大概率把事情做好。现在这个团队大家都充满了戾气，沟通就一副你是傻逼的样子，这种状态把事情做好, 难度是很高的。以前我以为自己足够厉害，做事出彩完全是个人能力的因素，现在发现和上家公司的领导对我的帮助也是有很大关系的，跟对人真的太重要了，做什么反而还是其次。第二个就是做事情要么不做，要做就要做的足够出彩，这个时代，只要你足够出彩，任何人都很难限制你的发展，一件事情做到80%的程度，和没做是没什么区别的，今年十月以前做的的事情都不值得一提，最后两个月的事情让我从C到B了，给了我很大的信心。
    今年年初的目标是工作绩效要争取到B, 虽然最后的结果还没有出，但是大概率是能达到的。至于明年的计划，我想了很久，在这个团队我会有前途吗，在这个团队我会有成长吗，新的一年和mentor 关系可能沟通更顺畅吗。我内心是不抱指望的。互联网寒冬的到来，我也需要做好足够的准备。
   5.职业规划
     在曾经的一篇博客当中，我谈到以前的职业规划，希望专注算法，跟随自己的兴趣，同时贴合业务，让业务有落地的场景。时至今日，回首当初的选择，因为之前一直没有靠近过业务，所以对业务部门有天然的憧憬，面对当初的选择，放弃了纯算法的机会，放弃了看好自己的老大，固执的扎进了自己心仪的业务部门语音助手。现在每天和很多业务，产品经理打交道，做的工作越来越横向，以前和别人的沟通时候一直很难听懂别人的意思，如今慢慢能听懂了一部分，然而发现这个并非是我的兴趣，我本人的兴趣明显是更偏向算法一点，希望做的事情更加聚焦一点，业务部门天天一堆鸡毛蒜皮，扯皮拉筋的事情，太让人疲惫了。当然最疲惫的还是和团队老大关系不好，别人根本看不上你，不会带着你飞。
     到底什么是合适个人的职业规划，见仁见智了。目前在我看来最重要的：
     1 还是自己需要不断努力，提高面试的基本能力，保证自己想走的时候，可以去到更加心仪的公司，这个是一切的前提，保证实现职业规划的关键。
     2 我说了这么多团队氛围的事情，归根到底还是要自己做事足够优秀，这样老大才会带你飞。不管多什么傻逼的老大，你做事足够出成果了，都很难否定你。当然和领导关系好，对于把事情做好帮助也是巨大的。
     3 表达能力很重要，包括听懂别人的话，以及正面回应别人的能力，老大和你离的这么远，怎么发现你这个人牛逼，就是日常或者周会的一两次机会，这一两次机会，你把握住了，就把握住了。
     4 具体的方向，我会更加注重nlp 算法 + 工程能力方面，在这个部门呆了一年，我发现工程能力其实比算法重要太多了，很多应届生和想要转行的读了若干篇paper, 跑了一些demo, 就敢自称精通深度学习了，真要落地了，一地鸡毛，我之前也是这个状态，深感惭愧。
   6.新的一年计划
     1. 希望继续往模型复现方向这个方向深入下去，今年计划保底要复现四个模型，包括tensorflow 版本以及paddlePaddle 版本，训练以及预测模块，保证训练速度以及预测速度不低于竞品的50%
     2. 今年六月前 leetcode medium 400题 刷两遍
     3. 技术类博客更新6篇以上
     4. 参加两场线下分享
     5. 一周锻炼两次以上身体
   最后新的一年祝大家新年快乐，任何时候都不要放弃努力，大家一起加油。
https://www.cnblogs.com/ModifyRong/p/10285343.html
**************************************************
Linux-Kconfig总结与分析
使用Kconfig时,需要注意的地方

1.在Kconfig中定义的配置宏,前缀都没有"CONFIG_",只有编译内核时,自动生成autoconf.h才会出现前缀.


2.如果XX_defconfig板卡配置文件中定义新的宏=y时,而在Kconfig并没有声明它,则内核编译出来的autoconf.h里也不会定义它的.


3.如果XX_defconfig板卡配置文件中没有设置CONFIG_MODULES=y,则编译make modules时将会失败,而make menuconfig时, Kconfig中的tristate参数也会被读写为bool参数(也就是不能设置m)

 
CONFIG宏变量参数

bool:      表示该CONFIG宏只能选择y(编译内核)或者n(不编译),不能选择m(编译为模块)
tristate:  表示该CONFIG宏可以设置y/m/n三种模式(tristate)
string:    表示该CONFIG宏可以设为一串字符,比如#define CONFIG_XXX "config test"
hex:       表示该CONFIG宏可以设为一个十六进制,比如#define CONFIG_XXX 0x1234
int:         表示该CONFIG宏可以设为一个整数,比如#define CONFIG_XXX 1234

常用参数

default y:  表示默认是勾上的,当然也可以写为default m或者default n
help:           帮助提示信息
depends on:依赖项,比如depends on XXX 表示当前宏需要CONFIG_ XXX宏打开的前提下,才能设置它 (注意依赖项的config参数只有bool或tristate才有效)
select :      反依赖项,和depends on刚好相反,比如 selecton XXX表示当前宏如果是y或者m,则会自动设置XXX=y或者m(注意参数只有bool或tristate才有效)
choice:      会生成一个单选框,里面通过多选一方式选择config,需要注意choice中的config参数只能bool或tristate
prompt:     提示信息,如果对于choice而言,则会用来当做一个单选框入口点的标签
range :      设置用户输入的数据范围,比如range 0 100表示数据只能位于0~100
menuconfig: menuconfig XXX和config XXX类似,唯一不同的是该选项除了能设置y/m/n外,还可以实现菜单效果(能回车进入该项内部)

示例1-创建复选框(多选多)-探索宏变量参数接下来我们来试试如何给不同宏设置不同参数,以MY_SYMBOL1~ MY_SYMBOL5为例设置MY_SYMBOL3~ MY_SYMBOL5依赖于MY_SYMBOL2修改内核顶层Kconfig文件,添加内容:

menuconfig MY_SYMBOL_TEST    #生成一个菜单宏项
bool "MY_SYMBOL_TEST"
default y

config MY_SYMBOL1
bool "my symbol is bool"
default y
depends on MY_SYMBOL_TEST
config MY_SYMBOL2
tristate "my symbo2 is tristate"
default m
depends on MY_SYMBOL_TEST    

config MY_SYMBOL3
string "my symbo3 is string"
default "test symbo3"
depends on MY_SYMBOL2 && MY_SYMBOL_TEST

config MY_SYMBOL4
hex "my symbo4 is hex"
range 0 0x2000                  #设置hex区间
default 0x1234
depends on MY_SYMBOL2 && MY_SYMBOL_TEST

config MY_SYMBOL5
int "my symbo5 is int" 
range 0 2000                     #设置int区间
default 1234
depends on MY_SYMBOL2 && MY_SYMBOL_TEST

效果如下所示:

如上图所示,可以看到我们设置my symbol5超出区间[0,2000]时,直接数据报错
编译内核后,查看自动生成的autoconf.h,定义如下:  




其中上面的CONFIG_MY_SYMBOL2_MODULE宏是因为我们在Kconfig设置它为default m,所以CONFIG_MY_SYMBOL2_MODULE是个模块宏.
 
示例2-通过choice创建单选框(多选一) 

choice
prompt "choice example"    #作为该单选框入口点的标签
default y
default MY_SYMBOL3         #默认选择MY_SYMBOL3配置项

config MY_SYMBOL1
bool "my symbol1 is bool"
help
MY_SYMBOL1 example

config MY_SYMBOL2
bool "my symbo2 is bool"
help
MY_SYMBOL2 example

config MY_SYMBOL3
tristate "my symbo3 is tristate"
help
MY_SYMBOL3 example
endchoice

编译内核后,查看autoconf.h,如下图所示,可以看到对于choice单选框来说,tristate属性其实并没有module功能,只有y/n

 
未完,后续再遇到不懂的再总结~
https://www.cnblogs.com/lifexy/p/10292742.html
**************************************************
使用Airtest超快速开发App爬虫
想开发网页爬虫，发现被反爬了？想对 App 抓包，发现数据被加密了？不要担心，使用 Airtest 开发 App 爬虫，只要人眼能看到，你就能抓到，最快只需要2分钟，兼容 Unity3D、Cocos2dx-*、Android 原生 App、iOS App、Windows Mobile……。
Airtest是网易开发的手机UI界面自动化测试工具，它原本的目的是通过所见即所得，截图点击等等功能，简化手机App图形界面测试代码编写工作。
爬虫开发本着天下工具为我所用，能让我获取数据的工具都能用来开发爬虫这一信念，决定使用Airtest来开发手机App爬虫。
安装和使用
由于本文的目的是介绍如何使用Airtest来开发App爬虫，那么Airtest作为测试开发工具的方法介绍将会一带而过，仅仅说明如何安装并进行基本的操作。
安装Airtest
从Airtest官网：https://airtest.netease.com下载Airtest，然后像安装普通软件一样安装即可。安装过程没有什么需要特别说明的地方。Airtest已经帮你打包好了开发需要的全部环境，所以安装完成Airtest以后就能够直接使用了。
Airtest运行以后的界面如下图所示。

连接手机
以Android手机为例，由于Airtest会通过adb命令安装两个辅助App到手机上，再用adb命令通过控制这两个辅助App进而控制手机，因此首先需要确保手机的adb调试功能是打开的，并允许通过adb命令安装App到手机上。
启动Airtest以后，把Android手机连接到电脑上，点击下图方框中的refresh ADB：

此时在Airtest界面右上角应该能够看到手机的信息，如下图所示。

点击connect按钮，此时可以在界面上看到手机的界面，并且当你手动操作手机屏幕时，Airtest中的手机画面实时更新。如下图所示。

对于某些手机，例如小米，在第一次使用Airtest时，请注意手机上将会弹出提示，询问你是否允许安装App，此时需要点击允许按钮。
打开微信
先通过一个简单的例子，来看看如何快速上手Airtest，稍后再来详解。
例如我现在想使用电脑控制手机，打开微信。
此时，点击下图中方框框住的touch按钮：

此时，把鼠标移动到Airtest右边的手机屏幕区域，鼠标会变成十字型。在微信图标的左上角按下鼠标左键不放，并拖到微信右下角松开鼠标。此时请注意中间代码区域发生了什么变化，如下图所示。

好了。以上就是你需要使用电脑打开微信所要进行的全部操作。
点击上方工具栏中的三角形图标，运行代码，如下图所示。

代码运行完成以后，微信被打开了。
界面介绍
在有了一个直观的使用以后，我们再来介绍一下Airtest的界面，将会更加有针对性。
Airtest的界面如下图所示。

这里，我把Airtest分成了A-F6个区域，他们的功能如下：

A区：常用操作功能区
B区：Python代码编写区
C区：运行日志区
D区：手机屏幕区
E区：App页面布局信息查看区
F区：工具栏

A区是常用的基于图像识别的屏幕操作功能，例如：

touch: 点击屏幕元素
swipe: 滑动屏幕
exists: 判断屏幕元素是否存在
text: 在输入框中输入文字
snashot: 截图
……

一般来说，是点击A区里面的某一个功能，然后在D区屏幕上进行框选操作，B区就会自动生成相应的操作代码。
B区用来显示和编写Python代码。在多数情况下，不需要手动写代码，因为代码会根据你在手机屏幕上面的操作自动生成。只有一些需要特别定制化的动作才需要修改代码。
D区显示了手机屏幕，当你操作手机真机时，这个屏幕会实时刷新。你也可以直接在D区屏幕上使用鼠标操作手机，你的操作动作会被自动在真机上执行。
F区是一些常用工具，从左到右，依次为：

新建项目
打开项目
保存项目
运行代码
停止代码
查看运行报告

其中1-5很好理解，那么什么是查看运行报告呢？
当你至少运行了一次以后，点击这个功能，会自动给你打开一个网页。网页如下图所示，这是你的代码的运行报告，详细到每一步操作了什么元素。

通过截图功能操作手机虽然方便，但是截图涉及到分辨率的问题，代码不能在不同的手机上通用。所以对于A区的功能，做点简单操作即可，不用深入了解。
更高级的功能，需要通过E区实现。
基于App布局信息操作手机
初始化代码
App的布局信息就像网页的HTML一样，保存了App上面各个元素的相对位置和各个参数。对于一个App而言，在不同分辨率的手机上，可能相同的元素有着不同的坐标点，但是这个元素的属性参数一般是不会变的。因此，如果使用元素的属性参数来寻找并控制这个元素，就能实现在不同分辨率手机上的精确定位。
App的布局信息的格式与App的开发环境有关。点击F区的下拉菜单，可以看到这里能够指定不同的App开发环境。其中的Unity、Cocos-*等等一般是做游戏用的，Android是安卓原生App，iOS是苹果的App……如下图所示。

以手机版知乎为例，由于它是Android原生的App，所以在F区下拉菜单选择Android，此时注意B区弹出提示，询问你是否要插入poco初始代码到当前输入光标的位置，点击Yes，如下图所示。

此时，B区自动插入了一段代码，如下图所示。

定位并点击
现在，点击E区的锁形图标，如下图所示。

锁形图标激活以后，你再操作D区的屏幕，点击知乎App下面的知乎两个字，会发现屏幕上被点击的App并不会打开。但E区和C区却发生了变化，如下图所示。

其中E区显示的树状结构就是当前屏幕的布局信息，这与Chrome开发者工具里面显示的HTML结构如出一辙。C区显示的是当前被我点中的元素的信息。
请注意在这些元素信息中，有一个text属性，它的值为知乎。那么，这个属性就可以作为一个定位元素，于是可以在B区编写代码：
poco(text="知乎").click()
写完代码以后运行程序，可以看到知乎App被打开了。如下图所示。


注意，如果你发现手机真机显示的界面与Airtest屏幕显示的手机界面不一致，可能是因为Airtest的屏幕被你锁定了。在F区点一下锁形图标，取消锁定，Airtest中的手机屏幕就会更新了。

定位并输入
打开知乎以后，我想使用知乎的搜索功能，那么继续，把锁形图标激活，然后点击知乎顶部的搜索框，如下图所示：

继续看C区显示的搜索框属性，可以看到这里有一个name属性，它的值是com.zhihu.android:id/input，还有一个text属性，它的值为蔡徐坤任 NBA 新春贺岁大使。能不能像前面打开知乎一样，使用text这个属性呢？也行，也不行。说它行，是因为你这么做确实现在能工作；说它不行，因为这是知乎的热门搜索关键词，随时会改变。你今天使用这一句话成功了，明天热门关键词变化了，那么你的代码就无法使用了。所以此时需要使用name这个属性。
常见的基本上不会变化的属性包含但不限于：name type resourceId package。
另外还有一点，知乎首页的这个搜索框，实际上是不能输入内容的，当你点击以后，会跳转到另一个页面，如下图所示。

因此你需要先点击一下这个输入框，跳转到真正的搜索界面：
poco(name="com.zhihu.android:id/input").click()
在真正的搜索界面如下图所示。

可以看到，name属性的值依然是com.zhihu.android:id/input，此时就可以输入内容了。
输入内容使用的方法为set_text，用法为：
poco(name="com.zhihu.android:id/input").set_text('古剑奇谭三')
定位并筛选
输入了搜索关键词以后，再来看看当前页面，搜索出现了三个结果：



通过对比这三个结果的属性信息，发现他们的name属性都是相同的，而text不同。如果像下面这样写点击动作：
poco(name='com.zhihu.android:id/magi_title').click()
那么默认就会点击第一个搜索结果。
如果我想点击第二个搜索结果怎么办呢？可以这样写代码：
poco(name='com.zhihu.android:id/magi_title', text='古剑奇谭（电视剧）').click()
或者你也可以像列表一样使用索引定位：
poco(name='com.zhihu.android:id/magi_title')[1].click()
这两种写法的前提，都是我们已经知道了每个结果分别是什么。假设现在我就想搜索古剑奇谭三，但我不知道搜索结果是第几项，又应该怎么办呢？此时还可以使用正则表达式：
poco(name='com.zhihu.android:id/magi_title', textMatches='^古剑奇谭三.*$').click()
滑动屏幕
进入搜索结果以后，需要查看下面的各种问题，此时就需要不断向上滑动屏幕。这里有一点需要特别注意，Airtest只能获取当前屏幕上的元素布局信息，不在屏幕上的内容是无法获取的。这一点和Selenium是不一样的。
滑动屏幕使用的命令为swipe，滑动屏幕需要使用坐标信息。但这种坐标和屏幕分辨率无关。这里的坐标定义为：(x, y)，其中x为横坐标，y为纵坐标。屏幕左上角为(0, 0)，屏幕右下角为(1, 1)，从左向右，横坐标从0逐渐增大到1，从上到下，纵坐标从0逐渐增大到1。
现在我要把屏幕向上滑动，那么在真机上面，我是先按住屏幕下方，然后把屏幕向上滑动，所以代码可以这样写：

# poco.swipe(起点坐标，终点左边)
poco.swipe([0.5, 0.8], [0.5, 0.2])
方向示意图如下图所示：

在一般情况下：

向上滑动，只需要改动纵坐标，且起点值大于终点值
向下滑动，只需要改动纵坐标，且起点值小于终点值
向左滑动，只需要改动横坐标，且起点值大于终点值
向右滑动，只需要改动横坐标，且起点值小于终点值

在爬虫开发中，涉及到的Airtest操作基本上已经介绍完毕。
单独使用Python控制手机
在Airtest操作手机虽然方便，但是不可能在每一台电脑上都安装Airtest吧。所以需要想办法把代码从Airtest这个程序中分离出来。
Airtest基于Python的一个开源库Poco开发，而在Airtest的B区写的Python代码，实际上就是Poco的代码。所以只要安装Poco库，就可以在Python中直接控制手机。
安装Poco库的命令为：
pip install pocoui
这个库依赖的东西有点多，安装稍稍慢一些。安装完成以后，我们把代码复制到PyCharm中，如下图所示。

运行这段代码，如果是Linux或者macOS的用户，请注意看运行结果是不是有报错，提示adb没有运行权限。这是因为随Poco安装的adb没有运行权限，需要给它添加权限，在终端执行命令：
# chmod +x 报错信息中给出的adb地址

chmod +x /Users/kingname/.local/share/virtualenvs/ZhihuSpider/lib/python3.7/site-packages/airtest/core/android/static/adb/mac/adb(实际执行时请换成你的地址)
命令运行完成以后再次执行代码，可以看到代码运行成功，手机被成功控制了，如下图所示。

如何获取屏幕文字
由于Airtest的编辑器中的代码运行后无法正常打印出中文，因此后面的代码都直接在PyCharm中执行。
既然要做爬虫，就需要获取手机上的文字内容。回到搜索页面，我想知道“古剑奇谭”三这个关键字能搜索出多少条结果，每条结果有多少个讨论，如下图所示：

此时我们需要做两件事情：

分别查看每一个搜索结果
获取屏幕上的文字

E区的树状结构如下图所示：

每一个搜索结果的标题作为text属性的值，在name='com.zhihu.android:id/magi_title'对应的元素中；每一个搜索结果的讨论数作为text属性的值，在name='com.zhihu.android:id/magi_count'对应的元素中。
最直接的做法就是分别获取三个标题和三个讨论数，然后把它们合并在一起：
title_obj_list = poco(name='com.zhihu.android:id/magi_title')
title_list = [title.get_text() for title in title_obj_list]

discuss_obj_list = poco(name='com.zhihu.android:id/magi_count')
discuss_list = [discuss.get_text() for discuss in discuss_obj_list]

for title, discuss in zip(title_list, discuss_list):
    print(title, discuss)
运行效果如下图所示：

但是这种做法实际上是很危险的，假设会有某一个很生僻的搜索结果，只有标题没有讨论数，那么这样分开抓取再组合的做法，就会导致最后匹配错位。所以合理的做法是先抓大再抓小。每一组标题和讨论数，他们都有自己的父节点，如下图箭头所指向的三个android.widget.LinearLayout:

那么现在，使用先抓大再抓小的技巧，先把每一组结果的父节点抓下来，再到每一个结果里面分别获取标题和讨论数。
然而这个父节点又怎么获取呢？如下图所示，这个父节点每一个属性值都没有什么特殊的，写任何一个都有可能与别的节点撞上。

此时，最简单的办法，就是在E区，双击父节点。定位代码就会自动添加，如下图所示。

这个定位代码看起来非常复杂，但实际上它的内在逻辑非常简单，就是从顶层一层一层往下找而已。
自动生成的定位代码如下：
poco("android.widget.LinearLayout").offspring("com.zhihu.android:id/action_bar_root").offspring("com.zhihu.android:id/parent_fragment_content_id").offspring("android.support.v7.widget.RecyclerView").child("android.widget.LinearLayout")[0]
在这个自动生成的定位代码中，我们看到了offspring、child这两种方法。其中child代表子节点，offspring代表孙节点、孙节点的子节点、孙节点的孙节点……。简言之，使用child只会在子节点中搜索需要的内容，而使用offspring会像文件夹递归一样把里面的所有节点都遍历一次，直到找到符合条件的属性为止。显然，offspring速度会比child慢。
实际上，我们可以对这个定位代码做一些精简：
poco("com.zhihu.android:id/parent_fragment_content_id").offspring("android.support.v7.widget.RecyclerView").child("android.widget.LinearLayout")[0]
这个精简的方法，与从Chrome复制的XPath中进行精简是一样的逻辑，根本原则就是找到“独一无二”的属性值，然后用这个属性值来进行定位。
由于我点击的是第一个搜索结果，所以定位代码的最后有一个[0]。现在由于需要获得所有搜索结果的内容，所以应该去掉[0]而使用for循环展开，然后获取里面的内容：
result_obj = poco("com.zhihu.android:id/parent_fragment_content_id").offspring("android.support.v7.widget.RecyclerView").child("android.widget.LinearLayout")
for result in result_obj:
    title = result.child(name='com.zhihu.android:id/magi_title').get_text()
    count = result.child(name='com.zhihu.android:id/magi_count').get_text()
    print(title, count)
运行效果如下图所示。

控制多台手机
当我们在电脑上插入多个Android手机时，执行命令：
adb devices -l
运行效果如下图所示。

每个手机都会被列出来。在最左边的编号就是手机串号。使用这个串号可以指定多个手机：
from airtest.core.api import auto_setup
from airtest.core.android import Android
from poco.drivers.android.uiautomation import AndroidUiautomationPoco
auto_setup(__file__)

device_1 = Android('76efadf3a7ce4')
device_2 = Android('adfasdfasf23')
device_3 = Android('adifu39ernla')

poco_1 = AndroidUiautomationPoco(device_1, use_airtest_input=True, screenshot_each_action=False)
poco_2 = AndroidUiautomationPoco(device_2, use_airtest_input=True, screenshot_each_action=False)
poco_3 = AndroidUiautomationPoco(device_3, use_airtest_input=True, screenshot_each_action=False)
通过这种方式，在一台电脑上使用USBHub，连上二三十台手机是完全没有问题的。
搭建手机爬虫集群
一台电脑可以连接三十台手机，那么如果有很多电脑和很多手机，就可以实现手机爬虫集群，其运行效果如下图所示。

关于如何搭建爬虫集群，已经超出本文的范围了。如果大家有兴趣，可以阅读我的书：Python爬虫开发 从入门到实战第十章对于如何搭建手机爬虫集群有详细的说明和注意事项。
如果对我的书有兴趣，请关注我的微信公众号与我交流。

https://www.cnblogs.com/xieqiankun/p/use_airtest.html
**************************************************
设计模式之代理模式(结构型)
第一章
1.1 模式定义
代理模式：代理模式就是引入一个代理对象，通过代理对象实现对原对象的引用。代理模式是一种对象结构型。
1.2 代理模式包含如下角色

Subject：抽象主题角色
Proxy：代理主题角色
RealSubject：真实主题角色


1.3 模式例子
public class Proxy implements Subject
{
    private RealSubject realSubject = new RealSubject();
    public void preRequest()
    {…...}
    public void request()
    {
        preRequest();
        realSubject.request();
        postRequest();
    }
    public void postRequest()
    {……}
} 

1.4 模式类型
来自：《设计模式》一书归纳分类


远程(Remote)代理：为一个位于不同的地址空间的对象提供一个本地的代理对象，这个不同的地址空间可以是在同一台主机中，也可是在另一台主机中，远程代理又叫做大使(Ambassador)。
虚拟(Virtual)代理：如果需要创建一个资源消耗较大的对象，先创建一个消耗相对较小的对象来表示，真实对象只在需要时才会被真正创建。
Copy-on-Write代理：它是虚拟代理的一种，把复制（克隆）操作延迟到只有在客户端真正需要时才执行。一般来说，对象的深克隆是一个开销较大的操作，Copy-on-Write代理可以让这个操作延迟，只有对象被用到的时候才被克隆。
保护(Protect or Access)代理：控制对一个对象的访问，可以给不同的用户提供不同级别的使用权限。
缓冲(Cache)代理：为某一个目标操作的结果提供临时的存储空间，以便多个客户端可以共享这些结果。
防火墙(Firewall)代理：保护目标不让恶意用户接近。
同步化(Synchronization)代理：使几个用户能够同时使用一个对象而没有冲突。
智能引用(Smart Reference)代理：当一个对象被引用时，提供一些额外的操作，如将此对象被调用的次数记录下来等。


下面介绍一下静态代理和动态代理

代理模式分为静态代理和动态代理 • 静态代理:静态代理就是编译阶段就生成代理类来完成对代理对象的一系列操作。
• 动态代理:动态代理是指在运行时动态生成代理类。即，代理类的字节码将在运行时生成并载入当前代理的 ClassLoader。

第二章 静态代理
静态代理：静态代理就是编译阶段就生成代理类来完成对代理对象的一系列操作。
主题接口：
public   interface Subject  {    
    abstract   public   void  request(); 
}   
目标对象：
public   class  RealSubject  implements Subject  {                     
   public   void  request()  { 
       System.out.println( " From real subject. " );     
   }  
}  
代理对象：
public   class  StaticProxySubject  implements Subject  { 
    private  RealSubject  realSubject;  // 以真实角色作为代理角色的属性  
    public  ProxySubject()  { }  
    public  void  request()  {  // 该方法封装了真实对象的request方法        
    //懒加载，用的时候才加载
    if ( realSubject  ==   null  )  { 
        realSubject  =   new  RealSubject();        
    }   
    realSubject.request();  // 此处执行真实对象的request方法   
   } 
}
编写客户端类：
public class Client{
    StaticProxySubject sps = new StaticProxySubject();
    sps.request();
}
第三章 动态代理
动态代理：动态代理是指在运行时动态生成代理类。即，代理类的字节码将在运行时生成并载入当前代理的 ClassLoader。
生成动态代理的方法有很多： JDK中自带动态代理，CGlib, javassist等。
3.1 JDK动态代理
Proxy类。该类即为动态代理类，该类最常用的方法为：public static Object newProxyInstance(ClassLoader loader, Class<?>[] interfaces, InvocationHandler h) throws IllegalArgumentException。

newProxyInstance()方法用于根据传入的接口类型interfaces返回一个动态创建的代理类的实例，方法中第一个参数loader表示代理类的类加载器，第二个参数interfaces表示被代理类实现的接口列表，第三个参数h表示所指派的调用处理程序类。

import java.lang.reflect.InvocationHandler;
import java.lang.reflect.Method;
import java.lang.reflect.Proxy;

public class MyInvocationHandler implements InvocationHandler {
    private Class<?> target;//委托类
    public MyInvocationHandler(Class<?> target){
        this.target=target;
    }
    //实际执行类bind
    public  Object bind(Class<?> target){
        //利用JDK提供的Proxy实现动态代理
        return  Proxy.newProxyInstance(target.getClassLoader(),
                new Class[]{target},this);
    }
    
    @Override
    public Object invoke(Object o, Method method, Object[] args) throws Throwable {
        /**代理环绕**/
        //执行实际的方法
        Object invoke = method.invoke(target, args);
        return invoke;
    }
}

3.2 CGLIB动态代理
CGLIB动态代理实现相关类需要在项目中导入 cglib-nodep-2.1_3.jar ，主要涉及两个类：
MethodInterceptor接口。它是代理实例的调用处理程序实现的接口，该接口中定义了如下方法：public Object intercept(Object proxy, Method method, Object[] arg2,    MethodProxy mp);
intercept()方法中第一个参数proxy表示代理类，第二个参数method表示需要代理的方法，第三个参数args表示代理方法的参数数组，第四个参数mp用 来去调用被代理对象方法
package com.demo;

import java.lang.reflect.Method;

import net.sf.cglib.proxy.Enhancer;
import net.sf.cglib.proxy.MethodInterceptor;
import net.sf.cglib.proxy.MethodProxy;

public class MyInterceptor implements MethodInterceptor{    
    private Object target; ;//代理的目标对象
    public MyInterceptor(Object target) {
        this.target = target;
    } 
//proxy 在其上调用方法的代理实例    method拦截的方法    args  拦截的参数
 //invocation 用来去调用被代理对象方法
    @Override
    public Object intercept(Object proxy, Method method, Object[] args, 
                                         MethodProxy invocation) throws Throwable {
        //1.记录日志 2.时间统计开始   3.安全检查
        Object retVal = invocation.invoke(target, args);  
        //4.时间统计结束
        return retVal;   
    }
//创建代理对象的方法
    public Object proxy(Object target) {
        this.target = target;
        Enhancer enhancer = new Enhancer();//该类用于生成代理类      
        enhancer.setSuperclass(this.target.getClass());//设置父类
        enhancer.setCallback(this);//设置回调用对象为本身
        return enhancer.create();

   }
}


https://www.cnblogs.com/mzq123/p/10292694.html
**************************************************
Habse中Rowkey的设计原则——通俗易懂篇
Hbase的Rowkey设计原则
一、 Hbase介绍
HBase -> Hadoop Database，HBase是Apache的Hadoop项目的子项目。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。另一个不同的是HBase基于列的而不是基于行的模式,主要用来存储非结构化和半结构化的松散数据（列存NoSQL数据库）
二、 设计原则
    2.1 Rowkey长度原则
Rowkey是一个二进制码流，Rowkey的长度被很多开发者建议设计在10-100个字节，不过建议是越短越好，不要超过16个字节。
原因如下：
（1）数据的持久化文件HFile中是按照KeyValue存储的，如果Rowkey过长比如100个字节，1000万列数据光Rowkey就要占用100*1000万=10亿个字节，将近1G数据，这会极大影响Hfile的存储效率；
（2）MemStore将缓存部分数据到内存，如果Rowkey字段过长内存的有效利用率降低，系统将无法缓存更多的数据，这会降低检索效率，因此Rowkey的字节长度越短越好。
（3）目前操作系统一般都是64位系统，内存8字节对齐，空值在16个字节，8字节的整数倍利用操作系统的最佳特性。
2.2 Rowkey散列原则
如果Rowkey是按时间戳的方式递增，不要将时间放在二进制码的前面，建议将Rowkey的高位作为散列字段，由程序循环生成，低位放时间字段，这样将提高数据均衡分布在每个Regionserver实现负载均衡的几率。如果没有散列字段，首字段直接是时间信息将产生所有新数据都在一个 RegionServer上堆积的热点现象，这样在做数据检索的时候负载将会集中在个别RegionServer，降低查询效率。
2.3 Rowkey唯一原则
必须在设计Rowkey上保证其唯一性。
2.4 访问hbase table中的行，只有三种方式：
    1 通过单个row key访问
    2 通过row key的range
    3 全表扫描
Hbase
API文档：http://hbase.apache.org/apidocs/index.html?overview-summary.html
 HBase的查询实现只提供两种方式：

    1、按指定RowKey获取唯一一条记录，get方法（org.apache.hadoop.hbase.client.Get）
    2、按指定的条件获取一批记录，scan方法（org.apache.hadoop.hbase.client.Scan）
实现条件查询功能使用的就是scan方式，scan在使用时有以下几点值得注意：
1、 scan可以通过setCaching与setBatch方法提高速度（以空间换时间）；2、 scan可以通过setStartRow与setEndRow来限定范围。范围越小，性能越高。
    通过巧妙的RowKey设计使我们批量获取记录集合中的元素挨在一起（应该在同一个    Region下），可以在遍历结果时获得很好的性能。3、 scan可以通过setFilter方法添加过滤器，这也是分页、多条件查询的基础。
三、 应用场景
   3.1  针对事务数据Rowkey设计
事务数据是带时间属性的，建议将时间信息存入到Rowkey中，这有助于提示查询检索速度。对于事务数据建议缺省就按天为数据建表，这样设计的好处是多方面的。按天分表后，时间信息就可以去掉日期部分只保留小时分钟毫秒，这样4个字节即可搞定。加上散列字段2个字节一共6个字节即可组成唯一 Rowkey。如下图所示：




事务数据Rowkey设计




第0字节


第1字节


第2字节


第3字节


第4字节


第5字节


…




散列字段


时间字段(毫秒)


扩展字段




0~65535(0x0000~0xFFFF)


0~86399999(0x00000000~0x05265BFF)


 




 
这样的设计从操作系统内存管理层面无法节省开销，因为64位操作系统是必须8字节对齐。但是对于持久化存储中Rowkey部分可以节省25%的开销。也许有人要问为什么不将时间字段以主机字节序保存，这样它也可以作为散列字段了。这是因为时间范围内的数据还是尽量保证连续，相同时间范围内的数据查找的概率很大，对查询检索有好的效果，因此使用独立的散列字段效果更好，对于某些应用，我们可以考虑利用散列字段全部或者部分来存储某些数据的字段信息，只要保证相同散列值在同一时间（毫秒）唯一。
针对统计数据的Rowkey设计
统计数据也是带时间属性的，统计数据最小单位只会到分钟（到秒预统计就没意义了）。同时对于统计数据我们也缺省采用按天数据分表，这样设计的好处无需多说。按天分表后，时间信息只需要保留小时分钟，那么0~1400只需占用两个字节即可保存时间信息。由于统计数据某些维度数量非常庞大，因此需要4个字节作为序列字段，因此将散列字段同时作为序列字段使用也是6个字节组成唯一Rowkey。如下图所示：




统计数据Rowkey设计




第0字节


第1字节


第2字节


第3字节


第4字节


第5字节


…




散列字段(序列字段）


时间字段(分钟)


扩展字段




0x00000000~0xFFFFFFFF)


0~1439(0x0000~0x059F)


 




同样这样的设计从操作系统内存管理层面无法节省开销，因为64位操作系统是必须8字节对齐。但是对于持久化存储中Rowkey部分可以节省25%的开销。预统计数据可能涉及到多次反复的重计算要求，需确保作废的数据能有效删除，同时不能影响散列的均衡效果，因此要特殊处理。
针对通用数据的Rowkey设计
通用数据采用自增序列作为唯一主键，用户可以选择按天建分表也可以选择单表模式。这种模式需要确保同时多个入库加载模块运行时散列字段（序列字段）的唯一性。可以考虑给不同的加载模块赋予唯一因子区别。设计结构如下图所示。
 




通用数据Rowkey设计




第0字节


第1字节


第2字节


第3字节


…




散列字段(序列字段）


扩展字段（控制在12字节内）




0x00000000~0xFFFFFFFF)


可由多个用户字段组成




 
支持多条件查询的RowKey设计
 
下面举个形象的例子：
 
我们在表中存储的是文件信息，每个文件有5个属性：文件id（long，全局唯一）、创建时间（long）、文件名（String）、分类名（String）、所有者（User）。
我们可以输入的查询条件：文件创建时间区间（比如从20120901到20120914期间创建的文件），文件名（“快乐大本营”），分类（“综艺”），所有者（“浙江卫视”）。
假设当前我们一共有如下文件：
内容列表 ID CreateTime Name Category UserID 1 2 3 4 5 6 7 8 9 10




20120902


快乐大本营第1期


综艺


1




20120904


快乐大本营第2期


综艺


1




20120906


快乐大本营番外


综艺


1




20120908


快乐大本营第3期


综艺


1




20120910


快乐大本营第4期


综艺


1




20120912


快乐大本营嘉宾采访


综艺花絮


2




20120914


快乐大本营第5期


综艺


1




20120916


快乐大本营录制花絮


综艺花絮


2




20120918


王祖蓝独家专访


花絮


3




20120920


安慕希酸奶广告


综艺广告


4




这里UserID应该对应另一张User表，暂不列出。我们只需知道UserID的含义：

1代表 浙江卫视； 2代表 大本营剧组； 3代表 XX微博； 4代表 赞助商。

调用查询接口的时候将上述5个条件同时输入find(20120901,20121001,"快乐大本营","综艺","浙江卫视")。

此时我们应该得到记录应该有第1、2、3、4、5、7条。第6条由于不属于“浙江卫视”应该不被选中。
我们在设计RowKey时可以这样做：采用UserID + CreateTime + FileID组成rowKey，这样既能满足多条件查询，又能有很快的查询速度。
 
需要注意以下几点：

    1、每条记录的RowKey，每个字段都需要填充到相同长度。假如预期我们最多有10万量级的用户，则userID应该统一填充至6位，如000001，000002...

    2、结尾添加全局唯一的FileID的用意也是使每个文件对应的记录全局唯一。避免当UserID与CreateTime相同时的两个不同文件记录相互覆盖。
按照这种RowKey存储上述文件记录，在HBase表中是下面的结构：
rowKey（userID 6 + time 8 + fileID 6）     name    category ....

00000120120902000001
00000120120904000002
00000120120906000003
00000120120908000004
00000120120910000005
00000120120914000007
00000220120912000006
00000220120916000008
00000320120918000009
00000420120920000010
.....
怎样用这张表？

在建立一个scan对象后，我们setStartRow(00000120120901)，setEndRow(00000120120914)。

这样，scan时只扫描userID=1的数据，且时间范围限定在这个指定的时间段内，满足了按用户以及按时间范围对结果的筛选。并且由于记录集中存储，性能很好。

然后使用SingleColumnValueFilter（org.apache.hadoop.hbase.filter.SingleColumnValueFilter），共4个，分别约束name的上下限，与category的上下限。满足按同时按文件名以及分类名的前缀匹配。

（注意：使用SingleColumnValueFilter会影响查询性能，在真正处理海量数据时会消耗很大的资源，且需要较长的时间。）

如果需要分页还可以再加一个PageFilter限制返回记录的个数。
以上，我们完成了高性能的支持多条件查询的HBase表结构设计。
四、 什么是热点
HBase中的行是按照rowkey的字典顺序排序的，这种设计优化了scan操作，可以将相关的行以及会被一起读取的行存取在临近位置，便于scan。然而糟糕的rowkey设计是热点的源头。热点发生在大量的client直接访问集群的一个或极少数个节点（访问可能是读，写或者其他操作）。大量访问会使热点region所在的单个机器超出自身承受能力，引起性能下降甚至region不可用，这也会影响同一个RegionServer上的其他region，由于主机无法服务其他region的请求。设计良好的数据访问模式以使集群被充分，均衡的利用。
为了避免写热点，设计rowkey使得不同行在同一个region，但是在更多数据情况下，数据应该被写入集群的多个region，而不是一个。
 
下面是一些常见的避免热点的方法以及它们的优缺点：
1. 加盐
这里所说的加盐不是密码学中的加盐，而是在rowkey的前面增加随机数，具体就是给rowkey分配一个随机前缀以使得它和之前的rowkey的开头不同。分配的前缀种类数量应该和你想使用数据分散到不同的region的数量一致。加盐之后的rowkey就会根据随机生成的前缀分散到各个region上，以避免热点。
2. 哈希
哈希会使同一行永远用一个前缀加盐。哈希也可以使负载分散到整个集群，但是读却是可以预测的。使用确定的哈希可以让客户端重构完整的rowkey，可以使用get操作准确获取某一个行数据
3. 反转
第三种防止热点的方法时反转固定长度或者数字格式的rowkey。这样可以使得rowkey中经常改变的部分（最没有意义的部分）放在前面。这样可以有效的随机rowkey，但是牺牲了rowkey的有序性。
3.1 反转rowkey的例子 
以手机号为rowkey，可以将手机号反转后的字符串作为rowkey，这样的就避免了以手机号那样比较固定开头导致热点问题
3.2 时间戳反转
一个常见的数据处理问题是快速获取数据的最近版本，使用反转的时间戳作为rowkey的一部分对这个问题十分有用，可以用Long.Max_Value - timestamp追加到key的末尾，例如[key][reverse_timestamp],[key]的最新值可以通过scan [key]获得[key]的第一条记录，因为HBase中rowkey是有序的，第一条记录是最后录入的数据。
比如需要保存一个用户的操作记录，按照操作时间倒序排序，在设计rowkey的时候，可以这样设计 
[userId反转][Long.Max_Value - timestamp]，在查询用户的所有操作记录数据的时候，直接指定反转后的userId，startRow是[userId反转][000000000000],stopRow是[userId反转][Long.Max_Value - timestamp] 
如果需要查询某段时间的操作记录，startRow是[user反转][Long.Max_Value - 起始时间]，stopRow是[userId反转][Long.Max_Value - 结束时间]
 
3.3 尽量减少行和列的大小



    在HBase中，value永远和它的key一起传输的。当具体的值在系统间传输时，它的rowkey，列名，时间戳也会一起传输。如果你的rowkey和列名很大，HBase storefiles中的索引（有助于随机访问）会占据HBase分配的大量内存，因为具体的值和它的key很大。可以增加block大小使得storefiles索引再更大的时间间隔增加，或者修改表的模式以减小rowkey和列名的大小。压缩也有助于更大的索引。
https://www.cnblogs.com/zmoumou/p/10292676.html
**************************************************
[UWP]实现一个轻量级的应用内消息通知控件
在UWP应用开发中，我们常常有向用户发送一些提示性消息的需求。这种时候我们一般会选择MessageDialog、ContentDialog或者ToastNotification来完成功能。
但是，我们大多数时候仅仅是需要在应用内向用户显示一条提示消息（例如“登录成功！”），不需要用户对这条消息做出处理，在这种情况下这几种方法都不算是很好的解决方式，它们不够轻量，也不够优雅，甚至会阻断用户的当前操作，这是我们所不期望的。
如果有安卓平台开发经验的开发者，可能会想到Toast组件。对，为什么UWP平台没有类似Toast的轻量级应用内消息提示组件呢？
现在，让我们来实现一个UWP可用的Toast组件。
先放一张效果图：

如何实现
在之前《[UWP]使用Popup构建UWP Picker》中我们讲了Picker的实现过程，其中的利用到的主要呈现手段就是Popup。而我们在这里想要构建一个代码中调用的消息通知组件，也可以采用同样的方式来实现。
Toast的主要功能是呈现通知，所以我定义了下面几个依赖属性来控制：

Content：类型为string，设置要向用户呈现的消息内容；
Duration：类型为TimeSpan，设置Toast控件在屏幕上的呈现时长。

在呈现逻辑上使用一个Popup作为加载Toast的容器。这里的逻辑非常简单，我直接贴出代码来，大家一看就能懂。
核心代码如下：
public class Toast : Control
{
    // Using a DependencyProperty as the backing store for Content.  This enables animation, styling, binding, etc...
    public static readonly DependencyProperty ContentProperty =
        DependencyProperty.Register("Content", typeof(string), typeof(Toast), new PropertyMetadata(0));

    // Using a DependencyProperty as the backing store for Duration.  This enables animation, styling, binding, etc...
    public static readonly DependencyProperty DurationProperty =
        DependencyProperty.Register("Duration", typeof(TimeSpan), typeof(Toast),
            new PropertyMetadata(TimeSpan.FromSeconds(2.0)));

    public Toast(string content)
    {
        DefaultStyleKey = typeof(Toast);
        Content = content;
        Width = Window.Current.Bounds.Width;
        Height = Window.Current.Bounds.Height;
        Transitions = new TransitionCollection
        {
            new EntranceThemeTransition()
        };
        Window.Current.SizeChanged += Current_SizeChanged;
    }

    public TimeSpan Duration
    {
        get => (TimeSpan) GetValue(DurationProperty);
        set => SetValue(DurationProperty, value);
    }

    public string Content
    {
        get => (string) GetValue(ContentProperty);
        set => SetValue(ContentProperty, value);
    }

    private void Current_SizeChanged(object sender, WindowSizeChangedEventArgs e)
    {
        Width = Window.Current.Bounds.Width;
        Height = Window.Current.Bounds.Height;
    }

    public async void Show()
    {
        var popup = new Popup
        {
            IsOpen = true,
            Child = this
        };
        await Task.Delay(Duration);
        popup.Child = null;
        popup.IsOpen = false;
        Window.Current.SizeChanged -= Current_SizeChanged;
    }
}
上面代码中，我在构造函数里为Toast控件添加了一个默认的隐式动画EntranceThemeTransition，使它呈现出来的时候不会显得太生硬。
Toast控件的默认样式：
<ResourceDictionary
    xmlns="http://schemas.microsoft.com/winfx/2006/xaml/presentation"
    xmlns:x="http://schemas.microsoft.com/winfx/2006/xaml"
    xmlns:local="using:HHChaosToolkit.UWP.Controls">
    <Style TargetType="local:Toast">
        <Setter Property="Template">
            <Setter.Value>
                <ControlTemplate TargetType="local:Toast">
                    <Border
                        Margin="0,0,0,60"
                        HorizontalAlignment="Center"
                        VerticalAlignment="Bottom"
                        Background="#af000000"
                        CornerRadius="4">
                        <TextBlock
                            Margin="30,15"
                            FontSize="14"
                            Foreground="White"
                            Text="{TemplateBinding Content}" />
                    </Border>
                </ControlTemplate>
            </Setter.Value>
        </Setter>
    </Style>
</ResourceDictionary>
如何调用
我们参考下安卓中Toast的使用方法：
Toast.makeText(getApplicationContext(), "This is a sample toast.",Toast.LENGTH_SHORT).show();
看起来挺长的一句代码，其实就是通过Toast.makeText()静态方法创建了一个新的Toast，然后调用其Show()方法让它出现在手机屏幕上。
在这里，我们也可以直接创建一个Toast，调用其Show()方法呈现。
或者也可以创建一个ToastHelper静态类来更方便的使用Toast组件：
public static class ToastHelper
{
    public static void SendToast(string content, TimeSpan? duration = null)
    {
        var toast = new Toast(content);
        if (duration.HasValue)
        {
            toast.Duration = duration.Value;
        }
        toast.Show();
    }
}
自定义样式
我们可以在自己的应用里为Toast组件新建一个资源字典，然后将自定义的样式添加在其中，例如：
<ResourceDictionary
    xmlns="http://schemas.microsoft.com/winfx/2006/xaml/presentation"
    xmlns:x="http://schemas.microsoft.com/winfx/2006/xaml"
    xmlns:controls="using:HHChaosToolkit.UWP.Controls">
    <Style x:Key="CustomToastStyle" TargetType="controls:Toast">
        <Setter Property="Template">
            <Setter.Value>
                <ControlTemplate TargetType="controls:Toast">
                    <Border
                        Width="160"
                        Height="160"
                        HorizontalAlignment="Center"
                        VerticalAlignment="Center"
                        Background="#af000000"
                        CornerRadius="4">
                        <Grid>
                            <Grid.RowDefinitions>
                                <RowDefinition />
                                <RowDefinition Height="Auto" />
                            </Grid.RowDefinitions>
                            <FontIcon
                                FontFamily="Segoe MDL2 Assets"
                                FontSize="50"
                                Foreground="White"
                                Glyph="&#xF1AD;" />
                            <TextBlock
                                Grid.Row="1"
                                Margin="30,0,30,15"
                                FontSize="14"
                                Foreground="White"
                                TextWrapping="Wrap"
                                Text="{TemplateBinding Content}" />
                        </Grid>
                    </Border>
                </ControlTemplate>
            </Setter.Value>
        </Setter>
    </Style>
</ResourceDictionary>
然后在App.xaml中引入我们编写好的资源字典。
<Application
    x:Class="HHChaosToolkit.Sample.App"
    xmlns="http://schemas.microsoft.com/winfx/2006/xaml/presentation"
    xmlns:x="http://schemas.microsoft.com/winfx/2006/xaml"
    xmlns:local="using:HHChaosToolkit.Sample"
    xmlns:viewModels="using:HHChaosToolkit.Sample.ViewModels">
    <Application.Resources>
        <ResourceDictionary>
            <viewModels:ViewModelLocator x:Name="Locator" />
            <ResourceDictionary.MergedDictionaries>
                <ResourceDictionary Source="Themes/Toast.xaml" />
            </ResourceDictionary.MergedDictionaries>
        </ResourceDictionary>
    </Application.Resources>
</Application>
使用时，我们只需要为Toast控件设置预定义的样式即可，或者在我们上面写的ToastHelper类中增加调用自定义样式Toast的静态方法：
        public static void SendCustomToast(string content, TimeSpan? duration = null)
        {
            var toast = new Toast(content);
            toast.Style = App.Current.Resources["CustomToastStyle"] as Style;
            if (duration.HasValue)
            {
                toast.Duration = duration.Value;
            }
            toast.Show();
        }
结尾
Toast组件是我的开源项目HHChaosToolkit项目中的一部分，其中还有一个与Toast原理差不多的组件WaitingDialog，原理是一样的，之后不会再单独写博文赘述了。
完整的示例代码在这里（GitHub），欢迎大家随意吐槽&提意见！
这篇博文到此结束，谢谢大家阅读！

https://www.cnblogs.com/hhchaos/p/10292665.html
**************************************************
看看redis中那些好玩的module (sql on redis, bf/cf on redis)
　　
　　自从redis加入了module功能之后，redis的生态就很有意思了，每个领域的大佬都会以插件的形式给redis扩展一些新的功能，比如本篇说到的rediSQL，rebloom。
 
一：rediSQL
  1. 背景
        redis虽然是牛逼，但还是有很多人吐槽redis操作性太弱，比如你想要在redis上实现一个比较复杂的业务逻辑，可能对你来说是一个灾难，有些同学会说用redis的
存储过程lua撒，但是lua不是每个程序员都会的，更何况那些数据分析师，但要是问sql会不会，基本上合格的程序员和分析师在这个上面都是没毛病的，真的要是让sql
落在redis上，那真是如虎添翼，可能最早让sql落到redis上的，应该是spark sql 吧，让redis作为spark的rdd，但这里说到的是另外一个通过module实现的sql on redis。
 
  2. 下载
     源代码可以到 github：https://github.com/RedBeardLab/rediSQL  ，下载地址是：https://github.com/RedBeardLab/rediSQL/releases    

直接下载这个编译好的文件，拿来就用就好了。
 
3. 加载
    这个简单，先把rediSQL_0.7.1.so 导入到centos中，然后只需使用module load  rediSQL_0.7.1.so 返回ok即可。

1 [root@localhost redis]# ls
2 00-RELEASENOTES  COPYING   Makefile   README.md        redis.conf        runtest           src
3 appendonly.aof   deps      MANIFESTO  redis-check-aof  rediSQL_0.7.1.so  runtest-cluster   tests
4 BUGS             dump.rdb  module     redis-check-rdb  redis-server      runtest-sentinel  utils
5 CONTRIBUTING     INSTALL   mydata     redis-cli        redis-trib.rb     sentinel.conf

 

[root@localhost redis]# ./redis-cli
127.0.0.1:6379> module load /data/redis/rediSQL_0.7.1.so
OK

 
4. 简单使用
    既然要让sql落到redis中，那就先得建库建表啦，这里database：Datamip， table：customer，然后做了一个简单的查询，如下：

127.0.0.1:6379> REDISQL.CREATE_DB Datamip
OK
127.0.0.1:6379> REDISQL.EXEC Datamip "CREATE TABLE customer(id int, username varchar(10));"
1) DONE
2) (integer) 0
127.0.0.1:6379> REDISQL.EXEC Datamip "INSERT INTO customer VALUES(1, 'jack');"
1) DONE
2) (integer) 1
127.0.0.1:6379> REDISQL.EXEC Datamip "INSERT INTO customer VALUES(2, 'mary');"
1) DONE
2) (integer) 1
127.0.0.1:6379> REDISQL.EXEC Datamip "SELECT * FROM customer WHERE id=2"
1) 1) (integer) 2
   2) "mary"
127.0.0.1:6379> 

 
   是不是很爽的感觉，不过作者也是要吃饭的，所以企业版还是要收点压箱底的钱。
 
二： rebloom
1. 背景
　　这个module也很有意思，它给redis新增了两种过滤器，一个叫做bloom filter，一个叫做 cuckoo filter， bloomfilter 估计大家都知道，用极小的错误率换取
原有的HashSet的1/8 -1/4的空间利用率，具体场景大家看着用吧，cuckoofilter 翻译过来就是布谷鸟过滤性，可能作者家就是养鸟的，不然怎么那么多鸟呢，
大家只要理解cuckoofilter比bloomfilter更省空间，更低的错误率，而且还是支持删除。
具体的大家可以看论文：http://www.cs.cmu.edu/~binfan/papers/conext14_cuckoofilter.pdf 。
 
2. 下载
    github地址：https://github.com/RedisLabsModules/rebloom   然后找到release模式，下载完之后需要自己make一下。


[root@localhost module]# ls
v1.1.0.tar.gz
[root@localhost module]# tar -xzvf v1.1.0.tar.gz
rebloom-1.1.0/
rebloom-1.1.0/.circleci/
rebloom-1.1.0/.circleci/config.yml
rebloom-1.1.0/.clang-format
rebloom-1.1.0/.gitignore
rebloom-1.1.0/Dockerfile
rebloom-1.1.0/LICENSE
rebloom-1.1.0/Makefile
rebloom-1.1.0/README.md
rebloom-1.1.0/contrib/
rebloom-1.1.0/contrib/MurmurHash2.c
rebloom-1.1.0/contrib/bloom.c
rebloom-1.1.0/contrib/bloom.h
rebloom-1.1.0/contrib/murmurhash2.h
rebloom-1.1.0/docs/
rebloom-1.1.0/docs/Bloom_Commands.md
rebloom-1.1.0/docs/CNAME
rebloom-1.1.0/docs/Cuckoo_Commands.md
rebloom-1.1.0/docs/Java_Client.md
rebloom-1.1.0/docs/Quick_Start.md
rebloom-1.1.0/docs/_config.yml
rebloom-1.1.0/docs/index.md
rebloom-1.1.0/mkdocs.yml
rebloom-1.1.0/ramp.yml
rebloom-1.1.0/src/
rebloom-1.1.0/src/cf.c
rebloom-1.1.0/src/cf.h
rebloom-1.1.0/src/cuckoo.c
rebloom-1.1.0/src/cuckoo.h
rebloom-1.1.0/src/print_version.c
rebloom-1.1.0/src/rebloom.c
rebloom-1.1.0/src/redismodule.h
rebloom-1.1.0/src/sb.c
rebloom-1.1.0/src/sb.h
rebloom-1.1.0/src/version.h
rebloom-1.1.0/tests/
rebloom-1.1.0/tests/Makefile
rebloom-1.1.0/tests/cuckoo.py
rebloom-1.1.0/tests/pytests.py
rebloom-1.1.0/tests/test-basic.c
rebloom-1.1.0/tests/test-cuckoo.c
rebloom-1.1.0/tests/test-perf.c
rebloom-1.1.0/tests/test.h
[root@localhost module]# ls
rebloom-1.1.0  v1.1.0.tar.gz
[root@localhost module]# cd rebloom-1.1.0
[root@localhost rebloom-1.1.0]# ls
contrib  Dockerfile  docs  LICENSE  Makefile  mkdocs.yml  ramp.yml  README.md  src  tests
[root@localhost rebloom-1.1.0]# make
cc  -Wall -Wno-unused-function -g -ggdb -O2 -fPIC -std=gnu99 -D_GNU_SOURCE -I/data/redis/module/rebloom-1.1.0 -I/data/redis/module/rebloom-1.1.0/contrib  -c -o /data/redis/module/rebloom-1.1.0/src/rebloom.o /data/redis/module/rebloom-1.1.0/src/rebloom.c
cc  -Wall -Wno-unused-function -g -ggdb -O2 -fPIC -std=gnu99 -D_GNU_SOURCE -I/data/redis/module/rebloom-1.1.0 -I/data/redis/module/rebloom-1.1.0/contrib  -c -o /data/redis/module/rebloom-1.1.0/contrib/MurmurHash2.o /data/redis/module/rebloom-1.1.0/contrib/MurmurHash2.c
cc  -Wall -Wno-unused-function -g -ggdb -O2 -fPIC -std=gnu99 -D_GNU_SOURCE -I/data/redis/module/rebloom-1.1.0 -I/data/redis/module/rebloom-1.1.0/contrib  -c -o /data/redis/module/rebloom-1.1.0/src/sb.o /data/redis/module/rebloom-1.1.0/src/sb.c
cc  -Wall -Wno-unused-function -g -ggdb -O2 -fPIC -std=gnu99 -D_GNU_SOURCE -I/data/redis/module/rebloom-1.1.0 -I/data/redis/module/rebloom-1.1.0/contrib  -c -o /data/redis/module/rebloom-1.1.0/src/cf.o /data/redis/module/rebloom-1.1.0/src/cf.c
In file included from /data/redis/module/rebloom-1.1.0/src/cf.c:6:0:
/data/redis/module/rebloom-1.1.0/src/cuckoo.c: In function ‘CuckooFilter_Count’:
/data/redis/module/rebloom-1.1.0/src/cuckoo.c:157:9: warning: passing argument 1 of ‘filterCount’ from incompatible pointer type [enabled by default]
         ret += filterCount(filter->filters[ii], &params);
         ^
/data/redis/module/rebloom-1.1.0/src/cuckoo.c:139:15: note: expected ‘const uint8_t (*)[2]’ but argument is of type ‘uint8_t (*)[2]’
 static size_t filterCount(const CuckooBucket *filter, const LookupParams *params) {
               ^
ld /data/redis/module/rebloom-1.1.0/src/rebloom.o /data/redis/module/rebloom-1.1.0/contrib/MurmurHash2.o /data/redis/module/rebloom-1.1.0/src/sb.o /data/redis/module/rebloom-1.1.0/src/cf.o -o /data/redis/module/rebloom-1.1.0/rebloom.so -shared -Bsymbolic -Bsymbolic-functions -lm -lc
[root@localhost rebloom-1.1.0]# ls
contrib  Dockerfile  docs  LICENSE  Makefile  mkdocs.yml  ramp.yml  README.md  rebloom.so  src  tests

 
最后标红的 rebloom.so 就是你最终要找的加载文件。

[root@localhost redis]# ./redis-cli
127.0.0.1:6379> module load /data/redis/module/rebloom-1.1.0/rebloom.so
OK

 
3.  简单使用
 
《1》 bloomfilter 的简单使用，比如塞入1，2，3，4 。 判断3，5是否在其中，如下：

127.0.0.1:6379> bf.add myfilter 1
(integer) 1
127.0.0.1:6379> bf.add myfilter 2
(integer) 1
127.0.0.1:6379> bf.add myfilter 3
(integer) 1
127.0.0.1:6379> bf.add myfilter 4
(integer) 1
127.0.0.1:6379> bf.exists myfilter 3
(integer) 1
127.0.0.1:6379> bf.exists myfilter 5
(integer) 0
127.0.0.1:6379>

 
《2》 在github的quickstart中并没有找到cuckoofilter的使用方式，没关系撒，找找源文件就好啦。
   
比如下面的源码就是告诉你怎么去使用。

 
接下来就可以简单的add，delete，exists 啦。

127.0.0.1:6379> cf.add myfilter2 1
(integer) 1
127.0.0.1:6379> cf.add myfilter2 2
(integer) 1
127.0.0.1:6379> cf.add myfilter2 3
(integer) 1
127.0.0.1:6379> cf.add myfilter2 4
(integer) 1
127.0.0.1:6379> cf.del myfilter 2
(error) Not found
127.0.0.1:6379> cf.del myfilter2 2
(integer) 1
127.0.0.1:6379> cf.exists myfilter2 1
(integer) 1

 
   好了，这就是本篇给大家介绍的module，还是蛮有意思的。
 
https://www.cnblogs.com/huangxincheng/p/10292303.html
**************************************************
第一次离职
    毕业后的第一份工作，不知不觉已经待了将近三年，不舍、感恩。
    我接触IT这行算是比较晚的，虽然是科班出身但在上大学之前我连打字都不利索，大学四年虽然一些基础可能相对比较扎实但连入门都算不上，可以说是在公司的这段时间我才逐渐成长，学习如何去学习、尝试去深度思考，渐渐能够独挡一面。
    感恩我的同事们，他们的成长也时刻督促着我成长，从他们身上学到很多，他们做事情的风格、对技术孜孜不倦的钻研、处世的态度等也让我产生一些蜕变。感恩我的领导、大老板、二老板，我的领导总是愿意将一些重要的机会充满信心的交给我，我很感激也因此我的水平也进步了非常多，我非常敬佩我的大老板和二老板，无论是技术层面还是思考问题的深度以及为人处事在我眼里都是专家级别，也非常开心自己能够得到他们的认可，认可我看待事物的态度、热衷于钻研并且有那么一点点的天分，真的十分感激。
    在公司也算和大部分人都相处的不错，离职因为怕产生不好的影响就非常低调，大家都非常好奇我居然回离职而且还这么果断，大家的关心越发让我感动不舍，人事的小伙伴们更为惊讶，居然在她们没有接到任何风声的情况下我就已经走完所有交接流程了，也很感动她们在办理我的事情时马上就给我办好离职证明，跟大老板告别后正式离开公司。
    离职的是因为个人原因，最近我总确认不下方向，想休息一段时间确认好，我怕方向错了导致一步错步步错，最近是对“万物互联”很感兴趣，在我看来这是一件很有意义也很有趣的事情。
    最后希望借我大老板对我说的最后一句话结尾，“可能你遇到一个好的团队，你的热衷钻研你的那一丝丝天分会可能给你带来蜕变”。2019，我开始了新的征程。
https://www.cnblogs.com/gt-xy/p/10292225.html
**************************************************
变量、标识符及变量提升
一、变量的概念
变量就是给“值”起名字，然后引用这个名字，就等同于引用这个值。变量的名字就是变量名；
1、注意事项：
（1）、如果只是声明变量而没有赋值，则该变量的值是undefined。
undefined是一个特殊的值，表示“无定义”

var a;
a // undefined

（2）、进行变量赋值的时候，没有写var命令，这条语句也是有效的。

var a = 1;
// 基本等同
a = 1;

但是，不写var的做法，不利于表达意图，而且容易不知不觉地创建全局变量，所以建议总是使用var命令声明变量。
（3）、可以在同一条var命令中声明多个变量。

var a, b，c;


var a=b=c=21;
//a=undefined;
//b=undefined;
//c=21

(4)、
JavaScript 是一种动态类型语言，也就是说，变量的类型没有限制，变量可以随时更改类型。

var a = 1;
a = 'hello';

上面代码中，变量a起先被赋值为一个数值，后来又被重新赋值为一个字符串。第二次赋值的时候，因为变量a已经存在，所以不需要使用var命令。
（5）、
使用var重新声明一个已经存在的变量，是无效的；

var a=1;
var a;//此操作毫无意义

但是，如果第二次声明的时候还进行了赋值，则会覆盖掉前面的值。

var a = 2;
var a = 4;
//代码执行到此行时，a=4;

上述代码等同于下面这段代码

var a = 2;
var a;
a = 4

二、标识符：
标识符（identifier）指的是用来识别各种值的合法名称。最常见的标识符就是变量名，以及后面要提到的函数名。JavaScript 语言的标识符对大小写敏感，所以a和A是两个不同的标识符。
标识符有一套命名规则，不符合规则的就是非法标识符。JavaScript 引擎遇到非法标识符，就会报错。
简单说，标识符命名规则如下。
1、第一个字符，可以是任意 Unicode 字母（包括英文字母和其他语言的字母），以及美元符号（$）和下划线（_）。
2、第二个字符及后面的字符，除了 Unicode 字母、美元符号和下划线，还可以用数字0-9。
下面这些都是合法标识符。

arg0
_tmp
$elem
π

下面这些则是不合法的标识符。

1a  // 第一个字符不能是数字
23  // 同上
***  // 标识符不能包含星号
a+b  // 标识符不能包含加号
-d  // 标识符不能包含减号或连词线

三、变量提升
JavaScript 引擎的工作方式是，先解析代码，获取所有被声明的变量，然后再一行一行地运行。这造成的结果，就是所有的变量的声明语句，都会被提升到代码的头部，这就叫做变量提升（hoisting）。

console.log(a);
var a = 1;

上面代码首先使用console.log方法，在控制台（console）显示变量a的值。这时变量a还没有声明和赋值，所以这是一种错误的做法，但是实际上不会报错。因为存在变量提升，真正运行的是下面的代码。

var a;
console.log(a);
a = 1;

最后的结果是显示undefined，表示变量a已声明，但还未赋值。
变量提升的五种情况：

不管判断条件是否成立变量都进行提升
变量提升只发生在等号左边
return后面的代码不进行变量提升；下面的不受影响；
变量名重复，不再重新声明，执行后重新定义；
匿名函数不需要进行变量提升

 
https://www.cnblogs.com/qingtiao/p/10292211.html
**************************************************
Hystrix断路器配置属性解析
HystrixCommand

配置方式
我们的配置都是基于 HystrixCommand 的，我们通过在方法上添加 @HystrixCommand 注解并配置注解的参数来实现配置，但有的时候一个类里面会有多个 Hystrix 方法，每个方法都是类似配置的话会冗余很多代码，这时候我们可以在类上使用 @DefaultProperties 注解来给整个类的 Hystrix 方法设置一个默认值。
配置项
下面是 HystrixCommand 支持的参数，除了 commandKey/observableExecutionMode/fallbackMethod 外，都可以使用 @DefaultProperties 配置默认值。


commandKey：用来标识一个 Hystrix 命令，默认会取被注解的方法名。需要注意：Hystrix 里同一个键的唯一标识并不包括 groupKey，建议取一个独一二无的名字，防止多个方法之间因为键重复而互相影响。


groupKey：一组 Hystrix 命令的集合， 用来统计、报告，默认取类名，可不配置。


threadPoolKey：用来标识一个线程池，如果没设置的话会取 groupKey，很多情况下都是同一个类内的方法在共用同一个线程池，如果两个共用同一线程池的方法上配置了同样的属性，在第一个方法被执行后线程池的属性就固定了，所以属性会以第一个被执行的方法上的配置为准。


commandProperties：与此命令相关的属性。


threadPoolProperties：与线程池相关的属性，


observableExecutionMode：当 Hystrix 命令被包装成 RxJava 的 Observer 异步执行时，此配置指定了 Observable 被执行的模式，默认是 ObservableExecutionMode.EAGER，Observable 会在被创建后立刻执行，而 ObservableExecutionMode.EAGER模式下，则会产生一个 Observable 被 subscribe 后执行。我们常见的命令都是同步执行的，此配置项可以不配置。


ignoreExceptions：默认 Hystrix 在执行方法时捕获到异常时执行回退，并统计失败率以修改熔断器的状态，而被忽略的异常则会直接抛到外层，不会执行回退方法，也不会影响熔断器的状态。


raiseHystrixExceptions：当配置项包括 HystrixRuntimeException 时，所有的未被忽略的异常都会被包装成 HystrixRuntimeException，配置其他种类的异常好像并没有什么影响。


fallbackMethod：方法执行时熔断、错误、超时时会执行的回退方法，需要保持此方法与 Hystrix 方法的签名和返回值一致。


defaultFallback：默认回退方法，当配置 fallbackMethod 项时此项没有意义，另外，默认回退方法不能有参数，返回值要与 Hystrix方法的返回值相同。


commandProperties

配置方式
Hystrix 的命令属性是由 @HystrixProperty 注解数组构成的，HystrixProperty 由 name 和 value 两个属性，数据类型都是字符串。
以下将所有的命令属性分组来介绍。
线程隔离(Isolation)


execution.isolation.strategy： 配置请求隔离的方式，有 threadPool（线程池，默认）和 semaphore（信号量）两种，信号量方式高效但配置不灵活，我们一般采用 Java 里常用的线程池方式。


execution.timeout.enabled：是否给方法执行设置超时，默认为 true。


execution.isolation.thread.timeoutInMilliseconds：方法执行超时时间，默认值是 1000，即 1秒，此值根据业务场景配置。


execution.isolation.thread.interruptOnTimeout： execution.isolation.thread.interruptOnCancel：是否在方法执行超时/被取消时中断方法。需要注意在 JVM 中我们无法强制中断一个线程，如果 Hystrix 方法里没有处理中断信号的逻辑，那么中断会被忽略。


execution.isolation.semaphore.maxConcurrentRequests：默认值是 10，此配置项要在 execution.isolation.strategy 配置为 semaphore 时才会生效，它指定了一个 Hystrix 方法使用信号量隔离时的最大并发数，超过此并发数的请求会被拒绝。信号量隔离的配置就这么一个，也是前文说信号量隔离配置不灵活的原因。


统计器(Metrics)
滑动窗口： Hystrix 的统计器是由滑动窗口来实现的，我们可以这么来理解滑动窗口：一位乘客坐在正在行驶的列车的靠窗座位上，列车行驶的公路两侧种着一排挺拔的白杨树，随着列车的前进，路边的白杨树迅速从窗口滑过，我们用每棵树来代表一个请求，用列车的行驶代表时间的流逝，那么，列车上的这个窗口就是一个典型的滑动窗口，这个乘客能通过窗口看到的白杨树就是 Hystrix 要统计的数据。
桶： bucket 是 Hystrix 统计滑动窗口数据时的最小单位。同样类比列车窗口，在列车速度非常快时，如果每掠过一棵树就统计一次窗口内树的数据，显然开销非常大，如果乘客将窗口分成十分，列车前进行时每掠过窗口的十分之一就统计一次数据，开销就完全可以接受了。 Hystrix 的 bucket （桶）也就是窗口 N分之一 的概念。


metrics.rollingStats.timeInMilliseconds：此配置项指定了窗口的大小，单位是 ms，默认值是 1000，即一个滑动窗口默认统计的是 1s 内的请求数据。


metrics.healthSnapshot.intervalInMilliseconds：它指定了健康数据统计器（影响 Hystrix 熔断）中每个桶的大小，默认是 500ms，在进行统计时，Hystrix 通过 metrics.rollingStats.timeInMilliseconds / metrics.healthSnapshot.intervalInMilliseconds 计算出桶数，在窗口滑动时，每滑过一个桶的时间间隔时就统计一次当前窗口内请求的失败率。


metrics.rollingStats.numBuckets：Hystrix 会将命令执行的结果类型都统计汇总到一块，给上层应用使用或生成统计图表，此配置项即指定了，生成统计数据流时滑动窗口应该拆分的桶数。此配置项最易跟上面的 metrics.healthSnapshot.intervalInMilliseconds 搞混，认为此项影响健康数据流的桶数。 此项默认是 10，并且需要保持此值能被 metrics.rollingStats.timeInMilliseconds 整除。


metrics.rollingPercentile.enabled：是否统计方法响应时间百分比，默认为 true 时，Hystrix 会统计方法执行的 1%,10%,50%,90%,99% 等比例请求的平均耗时用以生成统计图表。


metrics.rollingPercentile.timeInMilliseconds：统计响应时间百分比时的窗口大小，默认为 60000，即一分钟。


metrics.rollingPercentile.numBuckets：统计响应时间百分比时滑动窗口要划分的桶用，默认为6，需要保持能被metrics.rollingPercentile.timeInMilliseconds 整除。


metrics.rollingPercentile.bucketSize：统计响应时间百分比时，每个滑动窗口的桶内要保留的请求数，桶内的请求超出这个值后，会覆盖最前面保存的数据。默认值为 100，在统计响应百分比配置全为默认的情况下，每个桶的时间长度为 10s = 60000ms / 6，但这 10s 内只保留最近的 100 条请求的数据。


熔断器(Circuit Breaker)


circuitBreaker.enabled：是否启用熔断器，默认为 true;


circuitBreaker.forceOpen： circuitBreaker.forceClosed：是否强制启用/关闭熔断器，强制启用关闭都想不到什么应用的场景，保持默认值，不配置即可。


circuitBreaker.requestVolumeThreshold：启用熔断器功能窗口时间内的最小请求数。试想如果没有这么一个限制，我们配置了 50% 的请求失败会打开熔断器，窗口时间内只有 3 条请求，恰巧两条都失败了，那么熔断器就被打开了，5s 内的请求都被快速失败。此配置项的值需要根据接口的 QPS 进行计算，值太小会有误打开熔断器的可能，值太大超出了时间窗口内的总请求数，则熔断永远也不会被触发。建议设置为 QPS * 窗口秒数 * 60%。


circuitBreaker.errorThresholdPercentage：在通过滑动窗口获取到当前时间段内 Hystrix 方法执行的失败率后，就需要根据此配置来判断是否要将熔断器打开了。 此配置项默认值是 50，即窗口时间内超过 50% 的请求失败后会打开熔断器将后续请求快速失败。


circuitBreaker.sleepWindowInMilliseconds：熔断器打开后，所有的请求都会快速失败，但何时服务恢复正常就是下一个要面对的问题。熔断器打开时，Hystrix 会在经过一段时间后就放行一条请求，如果这条请求执行成功了，说明此时服务很可能已经恢复了正常，那么会将熔断器关闭，如果此请求执行失败，则认为服务依然不可用，熔断器继续保持打开状态。此配置项指定了熔断器打开后经过多长时间允许一次请求尝试执行，默认值是 5000。


其他(Context/Fallback)


requestCache.enabled：是否启用请求结果缓存。默认是 true，但它并不意味着我们的每个请求都会被缓存。缓存请求结果和从缓存中获取结果都需要我们配置 cacheKey，并且在方法上使用 @CacheResult 注解声明一个缓存上下文。


requestLog.enabled：是否启用请求日志，默认为 true。


fallback.enabled：是否启用方法回退，默认为 true 即可。


fallback.isolation.semaphore.maxConcurrentRequests：回退方法执行时的最大并发数，默认是10，如果大量请求的回退方法被执行时，超出此并发数的请求会抛出 REJECTED_SEMAPHORE_FALLBACK 异常。


threadPoolProperties

配置方式
线程池的配置也是由 HystrixProperty 数组构成，配置方式与命令属性一致。
配置项


coreSize：核心线程池的大小，默认值是 10，一般根据 QPS * 99% cost + redundancy count 计算得出。


allowMaximumSizeToDivergeFromCoreSize：是否允许线程池扩展到最大线程池数量，默认为 false;


maximumSize：线程池中线程的最大数量，默认值是 10，此配置项单独配置时并不会生效，需要启用 allowMaximumSizeToDivergeFromCoreSize 项。


maxQueueSize：作业队列的最大值，默认值为 -1，设置为此值时，队列会使用 SynchronousQueue，此时其 size 为0，Hystrix 不会向队列内存放作业。如果此值设置为一个正的 int 型，队列会使用一个固定 size 的 LinkedBlockingQueue，此时在核心线程池内的线程都在忙碌时，会将作业暂时存放在此队列内，但超出此队列的请求依然会被拒绝。


queueSizeRejectionThreshold：由于 maxQueueSize 值在线程池被创建后就固定了大小，如果需要动态修改队列长度的话可以设置此值，即使队列未满，队列内作业达到此值时同样会拒绝请求。此值默认是 5，所以有时候只设置了 maxQueueSize 也不会起作用。


keepAliveTimeMinutes：由上面的 maximumSize，我们知道，线程池内核心线程数目都在忙碌，再有新的请求到达时，线程池容量可以被扩充为到最大数量，等到线程池空闲后，多于核心数量的线程还会被回收，此值指定了线程被回收前的存活时间，默认为 2，即两分钟。


工作方式
Hystrix 内线程池的使用是基于 Java 内置线程池的简单包装，通常有以下三种状态：


如果请求量少，达不到 coreSize，通常会使用核心线程来执行任务。


如果设置了 maxQueueSize，当请求数超过了 coreSize, 通常会把请求放到 queue 里，待核心线程有空闲时消费。


如果 queue 长度无法存储请求，则会创建新线程执行直到达到 maximumSize 最大线程数，多出核心线程数的线程会在空闲时回收。


 
Hystrix常量配置类

package com.ley.springcloud.hystrix.utils;

import com.netflix.hystrix.HystrixCommand;

/**
 * HystrixCommand配置常量Key
 *
 * @see com.netflix.hystrix.HystrixCommandProperties
 * @see com.netflix.hystrix.HystrixThreadPoolProperties
 * @see com.netflix.hystrix.HystrixCollapserProperties
 **/
public final class HystrixCommandConfigConstants {


    //Command properties is use to control HystrixCommand

    /**
     * <b>execution 配置控制HystrixCommand.run的执行</b>
     *
     * @see HystrixCommand#run()
     * @see com.netflix.hystrix.contrib.javanica.annotation.HystrixCommand#commandProperties()
     * @see com.netflix.hystrix.contrib.javanica.annotation.HystrixProperty
     **/
    public static class ExecutionIsolationConstants {

        /**
         * 配置HystrixCommand.run的执行策略: THREAD|SEMAPHORE
         **/
        public static final String EXECUTION_ISOLATION_STRATEGY = "execution.isolation.strategy";


        /**
         * 配置HystrixCommand执行超时时间: 默认1000ms
         **/
        public static final String EXECUTION_ISOLATION_THREAD_TIMEOUT_IN_MILLISECONDS = "execution.isolation.thread.timeoutInMilliseconds";


        /**
         * 配置HystrixCommand.run执行是否启用超时时间: 默认为true
         **/
        public static final String EXECUTION_TIMEOUT_ENABLED = "execution.timeout.enabled";


        /**
         * 配置HystrixCommand.run()执行被取消的时候是否将它中断: 默认为true
         **/
        public static final String EXECUTION_ISOLATION_THREAD_INTERRUPT_ON_TIMEOUT = "execution.isolation.thread.interruptOnTimeout";


        /**
         * 当HystrixCommand的隔离策略使用信号量的时候,该属性用来配置信号量的大小(并发请求数),<br/>
         * 当最大并发请求数达到该设置值,后续的请求将被拒绝: 默认10
         **/
        public static final String EXECUTION_ISOLATION_SEMAPHORE_MAX_CONCURRENT_REQUESTS = "execution.isolation.semaphore.maxConcurrentRequests";
    }


    /**
     * fallback 控制HystrixCommand.getFallback()执行对于线程池或者信号量执行策略都生效
     *
     * @see HystrixCommand#getFallback()
     * @see com.netflix.hystrix.contrib.javanica.annotation.HystrixCommand#commandProperties()
     * @see com.netflix.hystrix.contrib.javanica.annotation.HystrixProperty
     **/
    public static class FallbackConstants {

        /**
         * 配置从调用线程中允许HystrixCommand.getFallback()方法的最大执行请求数<br/>
         * 当达到最大并发请求数时,后续请求将被拒绝并抛出异常: 默认10
         **/
        public static final String FALLBACK_ISOLATION_SEMAPHORE_MAX_CONCURRENT_REQUESTS = "fallback.isolation.semaphore.maxConcurrentRequests";


        /**
         * 配置服务降级策略是否启用: 默认true
         **/
        public static final String FALLBACK_ENABLED = "fallback.enabled";
    }


    /**
     * circuitBreaker配置 断路器用来控制HystrixCircuitBreaker
     *
     * @see com.netflix.hystrix.HystrixCircuitBreaker
     * @see com.netflix.hystrix.contrib.javanica.annotation.HystrixCommand#commandProperties()
     * @see com.netflix.hystrix.contrib.javanica.annotation.HystrixProperty
     **/
    public static class CircuitBreakerConstants {

        /**
         * 配置当服务请求命令失败时,是否使用断路器来跟踪其健康指标和熔断请求: 默认true
         **/
        public static final String CIRCUIT_BREAKER_ENABLED = "circuitBreaker.enabled";


        /**
         * 配置在滚动时间窗中,断路器的最小请求数: 默认20
         **/
        public static final String CIRCUIT_BREAKER_REQUEST_VOLUME_THRESHOLD = "circuitBreaker.requestVolumeThreshold";


        /**
         * 配置当断路器打开之后的休眠时间窗,休眠时间窗结束之后,会将断路器设置为"半开",<br/>
         * 尝试熔断的请求命令,如果依然失败就将断路器继续设置为"打开"状态,如果成功,就设置"关闭"<br/>
         * 默认5000ms
         **/
        public static final String CIRCUIT_BREAKER_SLEEP_WINDOW_IN_MILLISECONDS = "circuitBreaker.sleepWindowInMilliseconds";


        /**
         * 配置断路器打开的错误百分比条件: 默认50%<br/>
         * 当在滚动时间窗中,如果请求数量超过circuitBreaker.errorThresholdPercentage前提下<br/>
         * 如果错误请求数的百分比超过50%,就把断路器设置为"打开"状态
         **/
        public static final String CIRCUIT_BREAKER_ERROR_THRESHOLD_PERCENTAGE = "circuitBreaker.errorThresholdPercentage";


        /**
         * 如果该属性为true,断路器强制进入"打开"状态,拒绝所有请求,优于circuitBreaker.forceClosed.<br/>
         * 默认为false
         **/
        public static final String CIRCUIT_BREAKER_FORCE_OPEN = "circuitBreaker.forceOpen";


        /**
         * 如果该值设置为true,断路器强制进入"关闭"状态,它会接受所有请求,默认为true<br/>
         * 当配置了circuitBreaker.forceOpen,该属性配置无效
         **/
        public static final String CIRCUIT_BREAKER_FORCE_CLOSED = "circuitBreaker.forceClosed";
    }


    /**
     * metrics配置 主要度量HystrixCommand 和 HystrixObservableCommand 的执行指标信息
     *
     * @see HystrixCommand#run()
     * @see com.netflix.hystrix.contrib.javanica.annotation.HystrixCommand#commandProperties()
     * @see com.netflix.hystrix.contrib.javanica.annotation.HystrixProperty
     **/
    public static class MetricsConstants {

        /**
         * 配置滚动时间窗的长度,单位毫秒: 默认 10000ms(10s).<br/>
         * 该时间用于断路器判断健康度时需要手机信息的持续时间.<br/>
         * 断路器在收集指标时会根据设置的时间窗拆分为多个"桶"来累计各度量值,<br/>
         * 每个"桶"记录了一段时间内的采集指标.<br/>
         * 该属性从Hystrix 1.4.12版本开始,只有在应用初始化的时候生效,<br/>
         * 通过动态刷新配置不会产生效果.避免出现运行期监测数据丢失的情况
         **/
        public static final String METRICS_ROLLING_STATS_TIME_IN_MILLISECONDS = "metrics.rollingStats.timeInMilliseconds";

        /**
         * 配置滚动时间窗统计信息时划分"桶"的数量: 默认10<br/>
         * metrics.rollingStats.timeInMilliseconds参数设置必须能被该参数整除,否则抛出异常.<br/>
         * 该属性从Hystrix 1.4.12版本开始,只有在应用初始化的时候生效,<br/>
         * 通过动态刷新配置不会产生效果.避免出现运行期监测数据丢失的情况.
         **/
        public static final String METRICS_ROLLING_STATS_NUM_BUCKETS = "metrics.rollingStats.numBuckets";

        /**
         * 配置对命令执行是否使用百分位数来跟踪和计算: 默认true<br/>
         * 如果配置<b>false</b>,所有概要统计均返回-1
         **/
        public static final String METRICS_ROLLING_PERCENTILE_ENABLED = "metrics.rollingPercentile.enabled";

        /**
         * 配置百分位统计的滚动窗口的持续时间: 默认:60000ms(60s)<br/>
         * 该属性从Hystrix 1.4.12版本开始,只有在应用初始化的时候生效,<br/>
         * 通过动态刷新配置不会产生效果.避免出现运行期监测数据丢失的情况.
         **/
        public static final String METRICS_ROLLING_PERCENTILE_TIME_IN_MILLISECONDS = "metrics.rollingPercentile.timeInMilliseconds";

        /**
         * 配置百分位统计滚动窗口中使用"桶"的数量: 默认6<br/>
         * metrics.rollingPercentile.timeInMilliseconds必须能被该参数整除,否则抛出异常<br/>
         * 该属性从Hystrix 1.4.12版本开始,只有在应用初始化的时候生效,<br/>
         * 通过动态刷新配置不会产生效果.避免出现运行期监测数据丢失的情况.
         **/
        public static final String METRICS_ROLLING_PERCENTILE_NUM_BUCKETS = "metrics.rollingPercentile.numBuckets";


        /**
         * 配置在执行过程中每个"桶"中保留的最大执行次数: 默认100<br/>
         * 如果在滚动时间窗内超过该设定值的执行次数,就从最初的位置开始重写<br/>
         * 增加该值的大小会增加内存量的消耗,并增加排序百分位数所需的计算时间<br/>
         * 该属性从Hystrix 1.4.12版本开始,只有在应用初始化的时候生效,<br/>
         * 通过动态刷新配置不会产生效果.避免出现运行期监测数据丢失的情况.
         **/
        public static final String METRICS_ROLLING_PERCENTILE_BUCKET_SIZE = "metrics.rollingPercentile.bucketSize";

        /**
         * 配置采集影响断路器状态的健康快照(请求的成功,错误百分比)的间隔等待时间: 默认500ms
         **/
        public static final String METRICS_HEALTH_SNAPSHOT_INTERVAL_IN_MILLISECONDS = "metrics.healthSnapshot.intervalInMilliseconds";
    }


    /**
     * hystrix request context 配置
     *
     * @see com.netflix.hystrix.strategy.concurrency.HystrixRequestContext
     * @see com.netflix.hystrix.contrib.javanica.annotation.HystrixCommand#commandProperties()
     * @see com.netflix.hystrix.contrib.javanica.annotation.HystrixProperty
     **/
    public static class RequestContextConstants {

        /**
         * 配置是否开启请求缓存: 默认为true
         *
         * @see com.netflix.hystrix.HystrixRequestCache
         **/
        public static final String REQUEST_CACHE_ENABLED = "requestCache.enabled";

        /**
         * 配置HystrixCommand的执行和事件是否打印日志到HystrixRequestLog: 默认为true
         *
         * @see com.netflix.hystrix.HystrixRequestLog
         **/
        public static final String REQUEST_LOG_ENABLED = "requestLog.enabled";
    }


    /**
     * hystrix collapser constants
     *
     * @see com.netflix.hystrix.HystrixCollapser
     * @see com.netflix.hystrix.contrib.javanica.annotation.HystrixCollapser
     * @see com.netflix.hystrix.contrib.javanica.annotation.HystrixProperty
     **/
    public static class CollapserConstants {

        /**
         * 配置一次请求合并并批处理中允许最大请求数: 默认Integer.MAX_VALUE
         **/
        public static final String MAX_REQUESTS_IN_BATCH = "maxRequestsInBatch";

        /**
         * 配置批处理过程中每个命令延迟时间: 默认10ms
         **/
        public static final String TIMER_DELAY_IN_MILLISECONDS = "timerDelayInMilliseconds";

        /**
         * 配置批处理过程中是否开启请求缓存
         **/
        public static final String REQUEST_CACHE_ENABLED = "requestCache.enabled";
    }


    /**
     * hystrix command thread pool constants
     *
     * @see com.netflix.hystrix.contrib.javanica.annotation.HystrixCommand#threadPoolProperties()
     * @see com.netflix.hystrix.contrib.javanica.annotation.HystrixProperty
     **/
    public static class ThreadPoolConstants {

        /**
         * 配置命令线程池的核心线程数,命令执行最大并发量: 默认10
         **/
        public static final String CORE_SIZE = "coreSize";

        /**
         * maximum size of thread pool: 默认10
         **/
        public static final String MAXIMUM_SIZE = "maximumSize";

        /**
         * 配置线程池的最大队列大小: 默认-1<br/>
         * 配置-1,线程池使用{@link java.util.concurrent.SynchronousQueue}实现的队列,<br/>
         * 否则使用{@link java.util.concurrent.LinkedBlockingQueue}
         **/
        public static final String MAXQUEUE_SIZE = "maxQueueSize";

        /**
         * 配置队列拒绝阈值.配置该参数,即使队列没有达到最大值也拒绝请求: 默认5<br/>
         * 当maxQueueSize=-1,该配置不生效
         **/
        public static final String QUEUE_SIZE_REJECTION_THRESHOLD = "queueSizeRejectionThreshold";

        /**
         * minutes to keep a thread alive: 默认1分钟(60s)
         **/
        public static final String KEEP_ALIVE_TIMEMINUTES = "keepAliveTimeMinutes";

        /**
         * should the maximumSize config value get read and used in configuring the threadPool
         * turning this on should be a conscious decision by the user, so we default it to false: 默认false
         **/
        public static final String ALLOW_MAXIMUM_SIZE_TO_DIVERGE_FROM_CORE_SIZE = "allowMaximumSizeToDivergeFromCoreSize";

        /**
         * 配置滚动时间窗的床都,单位ms: 默认10000(10s)<br/>
         * 该滚动时间窗的长度用于线程池的指标度量,分成多个"桶"统计指标
         **/
        public static final String METRICS_ROLLING_STATS_TIME_IN_MILLISECONDS = "metrics.rollingStats.timeInMilliseconds";

        /**
         * 配置滚动时间窗被划分成"桶"的数量: 默认10<br/>
         * metrics.rollingStats.timeInMilliseconds必须能被该参数整除,不然抛出异常
         **/
        public static final String METRICS_ROLLINGSTATS_NUMBUCKETS = "metrics.rollingStats.numBuckets";
    }
}

 
https://www.cnblogs.com/liuenyuan1996/p/10292095.html
**************************************************
我的第一个上线小程序，案例实战篇二——LayaAir游戏开始界面开发
不知不觉我的第一个小程序已经上线一周了，uv也稳定的上升着。
很多人说我的小程序没啥用，我默默一笑，心里说：“它一直敦促我学习，敦促我进步”。我的以一个小程序初衷是经验分享，目前先把经验分享到博客园，边学习边完善小程序。同时我会持续学习，持续更新，功能定会一天天的完善起来。
欢迎大家扫码体验。

闲话少叙，今天我们一起来练习LayaAirIDE搭建游戏开始界面。
既然要开发游戏，那自然少不了图片素材，大家可以根据实际情况自行备图。大致一下几种素材图片：
1、一张背景图片（750*1333）
2、开始按钮背景图片，尺寸自己选择即可
3、游戏规则按钮背景图片，尺寸自己选择即可
4、其它的游戏名称图片和Logo之类的图片可以自行选择
素材准备好了，下面就是一个字，干！
第一步：新建一个空的LayaAir 空项目（PS:不会的话，清先跳转我的第一个上线小程序，案例实战篇一）
第二步：我们创建一个StartScene的场景，并且设置其宽度750，高度1333，点击确定
第三步：我们将场景切换到编辑模式，在Assets文件处右键-》新建目录-》并命名为images文件夹。如图：

然后我们把准备好的图片素材拖拽至images文件夹上即可将图片放入该文件夹中。
第四步：我们选中StartScene.scene场景文件，对场景进行编辑。

我们想场景中拖入一个image，并向其常用属性skin中拖入我们准备好的背景素材。
我们继续向场景中拖一个button，并设置其var 属性为btnRule，设置其stateNum为1,向其常用属性skin中拖入我们准备好的规则背景素材。
我们继续向场景中拖一个button，并设置其var 属性为btnStart,设置其stateNum为1,向其常用属性skin中拖入我们准备好的开始背景素材。
第五步：我们把窗口切换到IDE的代码模式，在src目录下面新建一个scripts的文件夹，在scripts文件夹右键-》新建模板文件-》新建脚本文件，命名为StartScene点击确定生成脚本文件，脚本和场景关联大家可以参考第一步。代码如下：

export default class StartScene extends Laya.Scene {

    constructor() { super(); }
    onBtnRoleClick(){

    }
    onBtnStartClick(){

    }
    onEnable() {
        this.btnRule.on(Laya.Event.CLICK,this,this.onBtnRoleClick);
        this.btnStart.on(Laya.Event.CLICK,this,this.onBtnStartClick);
    }

    onDisable() {
    }
}


　　
最终效果图：

 
若是喜欢微信小游戏开发，不妨和我一起学习成长！！！
https://www.cnblogs.com/wyang/p/10290218.html
**************************************************
Percona Toolkit mysql辅助利器
1 PT介绍
Percona Toolkit简称pt工具—PT-Tools，是Percona公司开发用于管理MySQL的工具，功能包括检查主从复制的数据一致性、检查重复索引、定位IO占用高的表文件、在线DDL等，DBA熟悉掌握后将极大提高工作效率。
2 PT 安装
 下载地址 ：https://www.percona.com/downloads/percona-toolkit/

下载完上传到linux 服务器
安装PT

[root@master01 ~]# yum -y install percona-toolkit-3.0.12-1.el7.x86_64.rpm


　　
创建一个表生成10000000 条测试数据 创建一个用户表：

create table uc_user (
user_id int NOT NULL auto_increment primary key , user_name varchar(32) ,
create_time datetime
)


　　
随机生成10000000 条测试数据

#创建存储过程 
delimiter //
create procedure user_data()
begin
    declare i int default 1;
    set i = 1;
    while i<=10000000 do
        -- rymd 表示随机年月日
        set @rymd = CONCAT(FLOOR(1990 + (RAND() * 28)),'-',LPAD(FLOOR(1 + (RAND() * 12)),2,0),'-',LPAD(FLOOR(3 + (RAND() * 8)),2,0));
        -- rhms 表示随机分钟小时秒
        set @rhms = CONCAT(LPAD(FLOOR(0 + (RAND() * 23)),2,0),':',LPAD(FLOOR(0 + (RAND() * 59)),2,0),':',LPAD(FLOOR(0 + (RAND() * 59)),2,0));
        -- rstring 生成4位随机字符串
        set @rstring = substring(MD5(RAND()),1,4);
        insert into uc_user values (i,@rstring,concat(@rymd,' ',@rhms));
        set i=i+1;
    end while; 
end //
delimiter ;


　　
生成10000000 条测试数据

#开启一个事务
mysql> start transaction;
#调用存储过程
mysql> call user_data();

#结束事务，提交到硬盘
mysql> commit;


　　

我的mysql 版本是5.7.22 
3 PT 常用的几个工具
3.1 pt-archiver
pt-archive 是MySQL的在线归档，无影响生产数据
为什么要归档: 若干年前的数据则很少再被使用.归档的意思就是将某些不常使用的数据 放置到其他地方.
归档前提条件: pt-archive： —归档 用此操作的表必须有主键。一般的表设计的都会有主键的。


归档历史数据；
























































在线删除大批量数据；
























































数据导出和备份；
























































数据远程归档
























































数据清理














































































































pt-archiver –help pt-archiver 有很多参数，用这可以help一下 常用的一些选项说明:

--limit=1000 每次去1000行数据用pt-archiver处理
--txn-size 1000 设置1000行，为一个事务提交一次
--where 'id<3000' 设置操作条件， id表示表的column
--progress 5000  每处理5000行输出一次处理信息
--statistics  输出执行过程及最后的操作统计。(只要不加上--quiet,默认情况下pt-archiver都会输出执行过程的)
--charset=UTF8 指定字符集为UTF8 -- 这个最后加上不然可能出现乱码
--bulk-delete  批量删除source 上的旧数据（例如每次1000行的批量删除操作）

选项很多很多 ，不会的多看help


　　
注： 我这用的都是root 用户， 在实际环境 最好建立相应的用户赋予相应的权限进行归档。
3.1.1 用法示例
(1). 将uc_user表中create_time字段大于2017-01-01 00:00:00时间的数据进行归档,不删除原表记录
先看看大于create_time 大于2017-01-01 00:00:00 时间有多少数据

mysql> select count(*) from uc_user where create_time > "2017-01-01 00:00:00";
+----------+
| count(*) |
+----------+
|   356624 |
+----------+
1 row in set (2.23 sec)


　　

语法如下：

pt-archiver \
--source h=源ip,P=端口号,u=用户,p=密码,D=库名,t=表名 \ 
--dest h=目标ip,P=端口号,u=用户,p=密码,D=库名,t=表名  \
--no-check-charset --where 'create_time>"2017-01-01 00:00:00"' \
--progress 5000 --no-delete --limit=10000 –statistics


　　
源服务器的表结构和目标服务器的表结构如下： 注：它们之间没有主从关系，只是单纯的两台数据库实例:
 



源服务器 ip
归档服务器ip























































10.0.0.201
10.0.0.202




































































































































































 
原服务器的表结构:

 
 归档服务器的表结构
 
 
 归档语句如下：

[root@master01 ~]# pt-archiver \
--source h=10.0.0.201,P=3306,u=root,p=123456,D=course,t=uc_user \
--dest h=10.0.0.202,P=3306,u=root,D=course,t=uc_user_history \
--no-check-charset --where 'create_time>"2017-01-01 00:00:00"' \
--progress 5000 --no-delete --limit=10000 --statistics


　　

 
 
归档完成，看看归档的服务器数据是否有356624条

 
 (2).将表中CREATE_DATE字段小于1991-01-01 00:00:00时间的数据进行归档, 删除原表记录（不用加no-delete） 看看小于1991-01-01 00:00:00 有多少条数据，在进行归档

 
 归档删除原表记录

[root@master01 ~]# pt-archiver \
--source h=10.0.0.201,P=3306,u=root,p=123456,D=course,t=uc_user \
--dest h=10.0.0.202,P=3306,u=root,p=123456,D=course,t=uc_user_history \
--no-check-charset --where 'create_time < "1991-01-01 00:00:00"' \
--progress 5000   --limit=10000 --statistics


　
 
看看原表的数据少了，少了357115

 
 (3).归档时加上字符集 --charset

 
 

[root@master01 ~]# pt-archiver  \
--charset 'utf8'  \
--source h=10.0.0.201,P=3306,u=root,p=123456,D=course,t=uc_user  \
--dest h=10.0.0.202,P=3306,u=root,p=123456,D=course,t=uc_user_history \
--no-check-charset --where 'create_time < "1992-02-01 00:00:00"' \
--progress 5000   --limit=10000 --statistics


　　一般归档的最好还是指定好原表字符集和归档数据的字符集一致。
归档途中就会显示如下： 每个Limit 执行的时间，pt-arvhive的开始时间及结束时间 ，源服务器，目标服务器。最后操作的insert ，delete, commit 的数量

 
3.2 pt-kill
pt-kill 是一个优秀的kill MySQL连接的一个工具，是percona toolkit的一部分，这个工具可以kill掉你想Kill的任何语句，特别出现大量的阻塞，死锁，某个有问题的sql导致mysql负载很高黑客攻击。当有很多语句时你不可能用show processlist去查看，当QPS很高时，你根本找不到你找的语句或ID,这时就可以用pt-kill来帮你帮完成。他可以根据运行时间，开源IP,用户名，数据库名。SQL语句,sleep,running 等状态进行匹配然后kill. 能匹配的太多了一一举例肯定不现实，拿几个案例看看。。可以用pt-kill –help 进行查看帮助
pt-kill 一些常用参数 

--daemonize  放在后台以守护进程的形式运行；
--interval       多久运行一次，单位可以是s,m,h，d等默认是s  ，不加这个默认是5秒
--victims         默认是oldest,只杀最古老的查询。这是防止被查杀是不是真的长时间运行的查询，他们只是长期等待 这种种匹配按时间查询，杀死一个时间最高值。
 --all               杀掉所有满足的线程
 --kill-query      只杀掉连接执行的语句，但是线程不会被终止
 --print              打印满足条件的语句
 --busy-time      批次查询已运行的时间超过这个时间的线程；
 --idle-time      杀掉sleep 空闲了多少时间的连接线程，必须在
 --match-command sleep时才有效—也就是匹配使用
--match-command    匹配相关的语句。
--ignore-command   忽略相关的匹配。
这两个搭配使用一定是ignore-commandd在前 match-command在后，
--match-db cdelzone  匹配哪个库
 command有：Query、Sleep、Binlog Dump、Connect、Delayed insert、Execute、Fetch、Init DB、Kill、Prepare、Processlist、Quit、Reset stmt、Table Dump


　　
3.2.1 pt-kill 举例
例如如下例子


杀掉空闲链接sleep 5秒的 SQL 并把日志放到/home/pt-kill.log文件中



[root@master01 ~]# /usr/bin/pt-kill \
--match-command Sleep \
--idle-time 5 \
--victim all \
--interval 5 \
--kill --daemonize -S /tmp/mysql.sock \
--user=root --password=123456 --port=3306 \
--pid=/tmp/ptkill.pid --print --log=/home/pt-kill.log  &


　　
我这没什么Sleep 所以没什么日志输出

　　 2. KILL 掉 查询SELECT 超过20秒的语句

[root@master01 ~]# /usr/bin/pt-kill  --user=root --password=123456 --port=3306 \
--busy-time 20 --match-info "SELECT|select" \
--victim all --interval 5  \
--kill --daemonize -S /tmp/mysql.sock --pid=/tmp/ptkill.pid --print --log=/home/pt-kill.log &


注： 一定要看服务这个后台服务启动没有 ，上面pt-kill 进程的pid 也是放在/tmp/ptkill.pid，所以一定的上面的那个进程给kill 掉， 然后开启新的进程
　模拟测试超过查询20s
　
超过了20s pt-kill 设置的规则 ，会终止这个select ，看下日志如下：

 
　　 3. Kill掉 select ifnull*语句开头的SQL，
 

[root@master01 ~]# ps aux | grep pt-kill | grep -v grep  | awk -F' ' '{print $2}' | xargs kill -9
[root@master01 ~]# pt-kill --user=root --password=123456 --port=3306 \
--victims all --busy-time=0 --match-info="select ifnull*"  \
--interval 1 -S /tmp/mysql.sock  \
--kill --daemonize --pid=/tmp/ptkill.pid --print --log=/home/pt-kill123.log &


　　

 
　　4. kill掉state Locked

[root@master01 ~]# ps aux | grep pt-kill | grep -v grep  | awk -F' ' '{print $2}' | xargs kill -9
[root@master01 ~]# /usr/bin/pt-kill --user=root --password=123456 --port=3306 \
--victims all --match-state='Locked'  --victim all --interval 5 \
--kill --daemonize -S /tmp/mysql.sock --pid=/tmp/ptkill.pid --print --log=/home/pt-kill-Locked.log &


　　
　　5. kill掉 qz_business_server 库，web为110.59.2.37的链接

pt-kill  --user=root --password=123456 --port=3306 \
--victims all --match-db='qz_business_service' \
--match-host='10.59.2.37' --kill --daemonize \
--interval 10 -S /tmp/mysql.sock \
--pid=/tmp/ptkill.pid --print --log=/home/pt-kill.log &


　　
　　6. 指定哪个用户kill

pt-kill --user=root --password=123456 --port=3306 \
--victims all --match-user='root' --kill --daemonize --interval 10 -S /tmp/mysql.sock \
--pid=/tmp/ptkill.pid --print --log=/home/pt-kill.log &


　　
　　7. kill掉 command query | Execute

/usr/bin/pt-kill --user=root --password=123456 --port=3306 \
--victims all --match-command= "query|Execute" --interval 5 \
--kill --daemonize -S /tmp/mysql.sock \
--pid=/tmp/ptkill.pid --print --log=/home/pt-kill.log &


　　
3.2.2 pt-kill 使用注意事项
每台主服务器部署pt-kill进程后台跑着。根据业务情况设置间隔时间，多久检测一次（建议只设置SELECT ）update 、DELETE不建议。

pt-kill --log-dsn D=dba,t=killed_sql_table \
--create-log-table --host=127.0.0.1 --user=root --password='密码' --port=6006 \
--busy-time=300 --print --kill-query --ignore-info "into|INTO|update|UPDATE|delete|DELETE" \
--match-info "SELECT|select" --victims all &


　　
上面的语句会把大于5分钟的SELECT 语句insert 到本机的dba库里的killed_sql_table表里。注意：区分大小写的。
记录到本机是因为kill 到哪些语句，记录下来，方便好查询。
 
3.4 pt-online-schema-change
3.4.1 pt-osc 介绍
业界简称 pt-osc 在线更改表结构 MySQL 大字段的DDL操作：加减字段、索引、修改字段属性等，在5.1之前都是非常耗时耗力的，特别是会对MySQL服务产生影响。在5.1之后随着Plugin Innodb的出现在线加索引的提高了很多，但是还会影响（时间缩短了），主要是出现了MDL锁(MySQL为了保护数据字典元数据，使用了metadata lock)。不过5.6可以避免上面的情况，但目前大部分在用的版本都是5.6之前的，所以DDL操作一直是数据库管理人员“头疼”的事。那如何在不锁表的情况下安全快速地更新表结构？
pt-osc模仿MySQL内部的改表方式进行改表，但整个改表过程是通过对原始表的拷贝来完成的，即在改表过程中原始表不会被锁定，并不影响对该表的读写操作。
首先，osc创建与原始表相同的不包含数据的新表（下划线开头）并按照需求进行表结构的修改，然后将原始表中的数据按逐步拷贝到新表中，当拷贝完成后，会自动同时修改原始表和新表的名字并默认将原始表删除
有两个注意点：被操作的表如果有 触发器，或外键用不了。要特别注意（标准规范MySQL是不建议用外键与触发器的）如果有，要把外键与触发器去掉再操作
注：1. 虽然有工具可以修改，但是修改的时候最好是在业务低峰期进行操作 

2 还需了解官方的online-ddl ,做好安全措施，哪些可以改都要了解清楚 https://dev.mysql.com/doc/refman/5.7/en/innodb-online-ddl.html
 
3.4.2 pt-osc 简单使用说明
pt-online-schema-change —- 在线DDL操作，对上亿的大表加索引加字段且对生产无影响 主要工作原理： 1.创建一个和要执行 alter 操作的表一样的新的空表结构(是alter之前的结构) 2.在新表执行alter table 语句（速度应该很快） 3.在原表中创建触发器3个触发器分别对应insert,update,delete操作 4.以一定块大小从原表拷贝数据到临时表，拷贝过程中通过原表上的触发器在原表进行的写操作都会更新到新建的临时表 5.Rename 原表到old表中，在把临时表Rename为原表 6.如果有参考该表的外键，根据alter-foreign-keys-method参数的值，检测外键相关的表，做相应设置的处理
7. 默认最后将旧原表删除 
如果执行失败了，或手动停止了，需要手动删除下划线开头的表(_表名)及三个触发器
主要几个参数： --max-load默认为Threads_running=25。每个chunk拷贝完后，会检查SHOW GLOBAL STATUS的内容，检查指标是否超过了指定的阈值。如果超过，则先暂停。这里可以用逗号分隔，指定多个条件，每个条件格式： status指标=MAX_VALUE或者status指标:MAX_VALUE。如果不指定MAX_VALUE，那么工具会这只其为当前值的120%。

 
 --critical-load 默认为Threads_running=50。用法基本与–max-load类似，如果不指定MAX_VALUE，那么工具会这只其为当前值的200%。如果超过指定值，则工具直接退出，而不是暂停。

--user: -u，连接的用户名
--password： -p，连接的密码 
--database： -D，连接的数据库
 --port -P，连接数据库的端口 
--host: -h，连接的主机地址 
--socket: -S，连接mysql套接字文件
--statistics 打印出内部事件的数目，可以看到复制数据插入的数目。 
--dry-run 创建和修改新表，但不会创建触发器、复制数据、和替换原表。并不真正执行，可以看到生成的执行语句，了解其执行步骤与细节。--dry-run与--execute必须指定一个，二者相互排斥。和--print配合最佳。
--execute 确定修改表，则指定该参数。真正执行。--dry-run与--execute必须指定一个，二者相互排斥。
--print  打印SQL语句到标准输出。指定此选项可以让你看到该工具所执行的语句，和--dry-run配合最佳。 --progress 复制数据的时候打印进度报告，二部分组成：第一部分是百分比，第二部分是时间。 
--quiet -q，不把信息标准输出。


　　
3.4.3 pt-osc 案例
1 . 添加索引的案例 对t=uc_user ,对uc_user表的user_name 列添加索引，索引名为index_uname。

pt-online-schema-change \
--user=root --password='123456'  --port=3306 --host=127.0.0.1 --critical-load Threads_running=100  \
--alter "ADD INDEX index_uname (user_name)" D=course,t=uc_user --print --execute


　　

 
修改过程中

 
 查看uc_user表的结构

ok


对uc_user 添加periodID列



pt-online-schema-change \
--user=root --password='123456' --port=3306 --host=127.0.0.1 \
--critical-load Threads_running=200 --alter "ADD COLUMN periodID int(11)" \
D=course,t=uc_user --print --execute


　　

 ok

 
　　3. 删除列 periodID 删除过程中，中断这个语句会发生什么，看看创建的_uc_user_new 触发器等，试一下。

看看数据库是否还有_uc_user_new 还有这个表没有

 
还有这个表， 那数据是不是还有，如果_uc_user_new 有数据，原表是否还有这个数据
 
数据也都有，对原来这个表的数据没影响。
上面说过： 如果执行失败了，或手动停止了，需要手动删除下划线开头的表(_表名)及三个触发器



DROP TABLE IF EXISTS `course`.`_uc_user_new`;
DROP TRIGGER IF EXISTS `course`.`pt_osc_course_uc_user_del`;
DROP TRIGGER IF EXISTS `course`.`pt_osc_course_uc_user_upd`;
DROP TRIGGER IF EXISTS `course`.`pt_osc_course_uc_user_ins`;


　　

 
 现在删除列 periodID

pt-online-schema-change \
--user=root --password='123456' --port=3306 --host=127.0.0.1 \
--critical-load Threads_running=200 --alter "drop column periodID" D=course,t=uc_user --print --execute


　　
验证：

3.5 pt-query-digest
慢查询Log的分析—此对DBA抓取慢查询很有帮助：
使用这个的前提必须开启了MySQL慢查询. 虽然可以cat 慢查询的日志文件，但慢查询文件特别大了， 哪一个sql 执行是最慢的了，哪一个执行次数最多了。 用pt-query-digest 查询就很方便
 
3.5.1 pt-query-digest 常用参数

--create-review-table  当使用--review参数把分析结果输出到表中时，如果没有表就自动创建。
--create-history-table  当使用--history参数把分析结果输出到表中时，如果没有表就自动创建。
--filter  对输入的慢查询按指定的字符串进行匹配过滤后再进行分析
--limit限制输出结果百分比或数量，默认值是20,即将最慢的20条语句输出，如果是50%则按总响应时间占比从大到小排序，输出到总和达到50%位置截止。
--host  mysql服务器地址
--user  mysql用户名
--password  mysql用户密码
--history 将分析结果保存到表中，分析结果比较详细，下次再使用--history时，如果存在相同的语句，且查询所在的时间区间和历史表中的不同，则会记录到数据表中，可以通过查询同一CHECKSUM来比较某类型查询的历史变化。
--review 将分析结果保存到表中，这个分析只是对查询条件进行参数化，一个类型的查询一条记录，比较简单。当下次使用--review时，如果存在相同的语句分析，就不会记录到数据表中。
--output 分析结果输出类型，值可以是report(标准分析报告)、slowlog(Mysql slow log)、json、json-anon，一般使用report，以便于阅读。
--since 从什么时间开始分析，值为字符串，可以是指定的某个”yyyy-mm-dd [hh:mm:ss]”格式的时间点，也可以是简单的一个时间值：s(秒)、h(小时)、m(分钟)、d(天)，如12h就表示从12小时前开始统计。
--until 截止时间，配合—since可以分析一段时间内的慢查询。


　　
3.5.2 pt-query-digst 案例


分析指定时间段的慢查询


分析2018-09-07 00:00:00 到 2018-12-24 15:50:00

pt-query-digest /usr/local/mysql/data/master01-slow.log  --since '2018-09-07 00:00:00' --until '2018-12-24 15:50:00'


　　
分析指定时间的慢查询日志 ，这样出的数据直接打印到屏幕上，可以输入到一个文件里 后面加上 > 就可以了 
展示图如下：靠前的都是比较慢的SQL ，需要优先处理

 
Overall: 总共有多少条查询，上例为总共14个查询(这是我的测试机器)。 Time range: 查询执行的时间范围。 unique: 唯一查询数量，即对查询条件进行参数化以后，总共有多少个不同的查询，该例为64。 
total: 总计 min : 最小 max : 最大 avg : 平均 95%: 把所有值从小到大排列，位置位于95%的那个数，这个数一般最具有参考价值。 median : 中位数，把所有值从小到大排列，位置位于中间那个数。
　　2. 分析指含有select语句的慢查询
 

pt-query-digest  --filter '$event->{fingerprint} =~ m/^select/i'  /usr/local/mysql/data/master01-slow.log  > slow_report4.log


　　
　　3. 针对某个用户的慢查询

pt-query-digest  --filter '($event->{user} || "") =~ m/^root/i'  /usr/local/mysql/data/master01-slow.log  > slow_report5.log


　　
3.6 pt-slave-delay
--pt-slave-delay ---就是指定从库比主库延迟多长时间，从库上执行
MySQL 5.6 之后就有自带的延迟配置 延迟复制配置，通过设置Slave上的MASTER TO MASTER_DELAY 参数实现：

 CHANGE MASTER TO MASTER_DELAY = N；


　　
作用： MySQL在做主从同步时，可以指定从库从主库延迟多长时间，这样有一个好处，当主库上勿删数据时，可以到延迟从库上stop slave 上，然后可以从从库上恢复一些数据
原理： 通过启动和停止从服务器的sql线程来设置从落后于主。它是通过slave的relay log（中继日志）的position(偏移量），不断启动，关闭replication SQL thread来保持主从一直延时固定长的时间来实现。因此不需要连接到主服务器。如果IO进程不落后主服务器太多的话，这个检查方式还是有效的,如果IO线程延时过大，pt-slave-delay也可以连接到主库来获取binlog的位置信息。
 
例如：

pt-slave-delay --delay=1m --interval=15s --run-time=10m u=root,p=123456,h=127.0.0.1,P=3306


　　

--delay ：从库延迟主库的时间，上面为1分钟。
--interval ：检查的间隔时间，上面为15s检查一次。（可选），不选则1分钟检查一次(默认)。 
--run-time ：该命令运行时间，上面为该命令运行10分钟关闭。（可选），不选则永远运行。--一搬不加此参数 


　　
注意：延迟的时间实际为 delay+interval，即该命令的让从延迟主75s。
3.7 pt-table-checksum & pt-table-sync
3.7.1 检查 pt-table-checksum
pt-table-checksum & pt-table-sync—–检查主从是否一致性—–检查主从不一致之后用这个工具进行处理 
这两个一搬是搭配使用（一搬主从不一样肯定要查一下，不能直接修复就完事了。） –参数讲解： 
 
 

replicate=test.checksum：主从不一致的结果放到哪一张表中，一般我放在一个既有的数据库中，这个checksum表由pt-table-checksum工具自行建立。 
databases=testdb ：我们要检测的数据库有哪些，这里是testdb数据库，如果想检测所有数据库那么就不要写这个参数了，如果有多个数据库，我们用逗号连接就可以了。 
host=’127.0.0.1’ ：主库的IP地址或者主机名。 
user=dba ：主机用户名。 —确定此用户可以访问主从数据库 
port=6006：主库端口号。 
recursion-method=hosts ：主库探测从库的方式。 
empty-replicate-table：清理上一次的检测结果后开始新的检测。 
no-check-bin-log-format：不检查二进制日志格式，鉴于目前大多数生产数据库都将二进制日志设置为“ROW”格式，而我们的pt-table-checksum会话会自行设定用“STATEMENT”格式，所以这个选项请务必加上。(具体什么格式，在服务器最好查一下show variables like ‘binlog_format’;)


　　
pt-table-checksum的使用

pt-table-checksum --nocheck-replication-filters  \
--no-check-binlog-format --replicate=test.checksums --recursion-method=hosts --databases=log_manage  \
h=localhost,u=sys_dba,p='密码',P=6006


　　
注：主与从库一定要有一个公用帐号。权限大一些，也就是从主库能登录到从库上的。不然会有如下报错。提示找不到，因为会自动找从库比对

下面加上正确的权限之后：

 
TS ：完成检查的时间。 ERRORS ：检查时候发生错误和警告的数量。 DIFFS ：0表示一致，1表示不一致。当指定–no-replicate-check时，会一直为0，当指定–replicate-check-only会显示不同的信息。 ROWS ：表的行数。 CHUNKS ：被划分到表中的块的数目。 SKIPPED ：由于错误或警告或过大，则跳过块的数目。 TIME ：执行的时间。 TABLE ：被检查的表名 
 
 


 
检测DIFF有异常时，立刻到从库去看：记住了是从库：this_crc.这是本机。 上面这个是正常没有差异的

SELECT db, tbl, SUM(this_cnt) AS total_rows, COUNT(*) AS chunks
FROM test.checksums
WHERE (
master_cnt <> this_cnt
OR master_crc <> this_crc
OR ISNULL(master_crc) <> ISNULL(this_crc))
GROUP BY db, tbl;



 
　　
3.7.2 修复pt-table-sync
检测有差异之后到从库上执行一下修复： 用这个前提是此表必须要有主键或唯一索引。
 
 

pt-table-sync --sync-to-master --replicate=test.checksums h=127.0.0.1,u=dba,P=6006,p=‘密码’ --print


　　
只打印不执行—看详细

执行

pt-table-sync --sync-to-master --replicate=test.checksums h=127.0.0.1,u=dba,P=6006,p='密码' --execute


　　
开始执行就修复了，再看一下就OK了 再检测就没有了
3.8 pt-find
　　1. 找出大于10G的表

/usr/bin/pt-find --socket=/mysql-socket的文件 --user=root --password='密码' --port=6006 --tablesize +10G


　　
　　2. 25分钟之修改过的表

/usr/bin/pt-find --socket=/mysql-socket的文件 --user=root --password='密码' --port=6006 --mmin -25


　　
 　　3. 空表没有数据的表

/usr/bin/pt-find --socket=/mysql-socket的文件 --user=root --password='密码' --port=6006 --empty


　　
3.9 pt-slave-restart
pt-slave-restart —–主从报错，跳过报错 ,在从库执行
常用参数：

--always        :永不停止slave线程，手工停止也不行
--ask-pass      :替换-p命令，防止密码输入被身后的开发窥屏
--error-numbers ：指定跳过哪些错误，可用,进行分隔
--error-text    ：根据错误信息进行匹配跳过
--log           ：输出到文件
--recurse       ：在主端执行，监控从端
--runtime       ：工具执行多长时间后退出：默认秒， m=minute,h=hours,d=days
--slave-user --slave-password ：从库的账号密码，从主端运行时使用
--skip-count    ：一次跳过错误的个数，胆大的可以设置大些，不指定默认1个
--master-uuid   :级联复制的时候，指定跳过上级或者上上级事务的错误
--until-master  :到达指定的master_log_pos,file位置后停止，
                              格式：”file:pos“
--until-relay   :和上面一样，但是时根据relay_log的位置来停止


　　
自动跳过主从同步1032的报错 建议在从库上如下这个就可以了，多个以逗号隔开就可以了
例：

/usr/bin/pt-slave-restart --user=root --password='密码' --port=6006 --host=127.0.0.1 --error-numbers=1032


　　
3.10 pt-mysql-summary
pt-mysql-summary —MySQL的描述信息，包括配置文件的描述
show processlist 查看MySQL的连接， pt-find打印出来的信息包括：版本信息、数据目录、命令的统计、用户，数据库以及复制等信息还包括各个变量（status、variables）信息和各个变量的比例信息，还有配置文件等信息。

pt-mysql-summary  --user=root --password='password' --host=127.0.0.1 --port=6007


　　

 
工具很多 可以看看官方文档：https://www.percona.com/doc/percona-toolkit/LATEST/index.html
https://www.cnblogs.com/keme/p/10237590.html
**************************************************
Java实现Ip代理池
设置Ip代理很多时候都会有用到，尤其是在写爬虫相关项目的时候。虽然自己目前没有接触这种需求，但由于最近比较闲，就写着当作练习吧
爬取代理IP
爬取
关于爬取代理IP，国内首先想到的网站当然是 西刺代理 。首先写个爬虫获取该网站内的Ip吧。
先对 国内Http代理 标签页面进行爬取，解析页面使用的Jsoup ，这里大概代码如下
 private List<IPBean> crawl(String api, int index){
        String html = HttpUtils.getResponseContent(api + index);
        System.out.println(html);

        Document document = Jsoup.parse(html);
        Elements eles = document.selectFirst("table").select("tr");

        for (int i = 0; i < eles.size(); i++){
            if (i == 0) continue;
            Element ele = eles.get(i);
            String ip = ele.children().get(1).text();
            int port = Integer.parseInt(ele.children().get(2).text().trim());
            String typeStr = ele.children().get(5).text().trim();

            int type;
            if ("HTTP".equalsIgnoreCase(typeStr))
                type = IPBean.TYPE_HTTP;
            else
                type = IPBean.TYPE_HTTPS;

            IPBean ipBean = new IPBean(ip, port, type);
            ipList.add(ipBean);
        }
        return ipList;
    }
对某些不明白的变量，可以参考我Github
其中关键的就是css选择器语法，这里需要注意的是不要乱加空格，不然会导致找不到出现空指针。
css选择器语法具体参考这里 ， 这里就不讲解了。
爬取的信息包括 ip地址、端口号、和代理类型(http或https), 这三个信息我放在IPBean这个类里面。
过滤
上面爬取完成后，还要进一步过滤，筛选掉不能使用的。
筛选大概原理就是先设置上代理，然后请求某个网页，若成功则代表此代理ip有效。
其中请求成功的标志我们可以直接获取请求的返回码，若为200即成功。
    /**
     * 检测代理ip是否有效
     *
     * @param ipBean
     * @return
     */
    public static boolean isValid(IPBean ipBean) {
        Proxy proxy = new Proxy(Proxy.Type.HTTP, new InetSocketAddress(ipBean.getIp(), ipBean.getPort()));
        try {
            URLConnection httpCon = new URL("https://www.baidu.com/").openConnection(proxy);
            httpCon.setConnectTimeout(5000);
            httpCon.setReadTimeout(5000);
            int code = ((HttpURLConnection) httpCon).getResponseCode();
            System.out.println(code);
            return code == 200;
        } catch (IOException e) {
            e.printStackTrace();
        }
        return false;
    }
注意这里要设置两个超时，连接超时和读取超时。连接超时还好，它默认只是有点长；然而读取超时如果不设置，它好像就会一直阻塞着。
时间设置为5s就够了，毕竟如果ip有效的话，会很快就请求成功的。这样过滤后，就得到有效的代理ip了
设置代理
单次代理
单次代理表示只在这一次连接中有效，即每次都需要代理。
http方式的代理非常简单，在URL对象的openConnection方法中加上个Proxy对象即可

 Proxy proxy = new Proxy(Proxy.Type.HTTP, new InetSocketAddress(ipBean.getIp(), ipBean.getPort()));

 connection = (HttpsURLConnection) new URL(url).openConnection(proxy);

https 稍微复杂点了，中间加上了ssl协议

    /**
     * @param url
     * @param headerMap 请求头部
     * @param ipBean
     * @return
     * @throws Exception
     */
    public static String getResponseContent(String url, Map<String, List<String>> headerMap, IPBean ipBean) throws Exception {
        HttpsURLConnection connection = null;

        // 设置代理
        if (ipBean != null) {
            Proxy proxy = new Proxy(Proxy.Type.HTTP, new InetSocketAddress(ipBean.getIp(), ipBean.getPort()));

            connection = (HttpsURLConnection) new URL(url).openConnection(proxy);

            if (ipBean.getType() == IPBean.TYPE_HTTPS) {
                SSLContext sslContext = SSLContext.getInstance("SSL");
                sslContext.init(null, new TrustManager[]{new TrustAnyTrustManager()}, new java.security.SecureRandom());
                connection.setSSLSocketFactory(sslContext.getSocketFactory());
                connection.setHostnameVerifier(new TrustAnyHostnameVerifier());
            }
        }

        if (connection == null)
            connection = (HttpsURLConnection) new URL(url).openConnection();

        // 添加请求头部
        connection.setRequestProperty("User-Agent", "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.81 Safari/537.36");
        if (headerMap != null) {
            Iterator<Map.Entry<String, List<String>>> iterator = headerMap.entrySet().iterator();
            while (iterator.hasNext()) {
                Map.Entry<String, List<String>> entry = iterator.next();
                List<String> values = entry.getValue();
                for (String value : values)
                    connection.setRequestProperty(entry.getKey(), value);
            }
        }

        InputStream inputStream = connection.getInputStream();
        BufferedReader reader = new BufferedReader(new InputStreamReader(inputStream));

        StringBuilder stringBuilder = new StringBuilder();
        String line;
        while ((line = reader.readLine()) != null) {
            stringBuilder.append(line);
        }
        reader.close();
        inputStream.close();
        return stringBuilder.toString();
    }


    private static class TrustAnyTrustManager implements X509TrustManager {

        public void checkClientTrusted(X509Certificate[] chain, String authType) throws CertificateException {
        }

        public void checkServerTrusted(X509Certificate[] chain, String authType) throws CertificateException {
        }

        public X509Certificate[] getAcceptedIssuers() {
            return new X509Certificate[]{};
        }
    }

    private static class TrustAnyHostnameVerifier implements HostnameVerifier {
        public boolean verify(String hostname, SSLSession session) {
            return true;
        }
    }

这里https方法参考了 这篇博客
全局代理
直接上代码，就几行代码
package util;

import other.IPBean;

/**
 * @author Asche
 * @github: https://github.com/asche910
 * @date 2019年1月19日
 */
public class ProxyUtils {

    /**
     * 设置全局代理
     * @param ipBean
     */
    public static void setGlobalProxy(IPBean ipBean){
        System.setProperty("proxyPort", String.valueOf(ipBean.getPort()));
        System.setProperty("proxyHost", ipBean.getIp());
        System.setProperty("proxySet", "true");
    }

}

需要注意一点就是全局只是在该java项目中生效，它不会更改系统中的代理。
检测
设置完代理后，也可以用另外一种方法来判断是否代理成功，即直接获取当前ip地址。
这里我使用的是 https://www.ipip.net/ip.html 这个网站，请求获取html后再解析得到自己的当前ip

 private static final String MY_IP_API = "https://www.ipip.net/ip.html";

    // 获取当前ip地址，判断是否代理成功
    public static String getMyIp() {
        try {
            String html = HttpUtils.getResponseContent(MY_IP_API);

            Document doc = Jsoup.parse(html);
            Element element = doc.selectFirst("div.tableNormal");

            Element ele = element.selectFirst("table").select("td").get(1);

            String ip = element.selectFirst("a").text();

            // System.out.println(ip);
            return ip;
        } catch (Exception e) {
            e.printStackTrace();
        }
        return null;
    }
优化
emmm 优化些啥呢？？？
速度
爬取ip时就几个网页，优化估计效果不大。而真正耗时的是检测ip是否有效，因此这里采用多线程，对每个ip的检测请求使用一个线程，最后副线程全部结束后再统计出有多少有效ip。然而问题又来了，怎么判断所有副线程全部结束了呢？？？ 脑中立刻想到的是join方法，然而仔细想想，才发现这样并不可取。最佳方法应该是设置一个计数器，每个线程结束后计数器加一，然后在主线程循环判断计数器的值是否与线程总数相等即可。由于涉及到并发，需要给某些方法加上锁。这里我代码中实现了，可以参考github

持久化
emmm 由于目前只是练练手，并没有这样的需求，比较懒， ~(￣▽￣)~*
所以这个需求暂时放放吧，以后有时间再写
最后github入口：Asche910

https://www.cnblogs.com/asche/p/10291701.html
**************************************************
Raft 基础
目录

三个状态
什么是任期
节点之间的通信

1. 三个状态
Raft 设计了 3 个状态，用于表示节点的状态，分别是跟随者，候选者，领导者。

领导者：通常只有一个领导人，并且其他节点都是跟随者。
跟随者：跟随者不会发送任何请求，只是简单的响应领导者或者候选人的请求，由领导人处理所有的客户端请求（如果客户端请求了跟随者，那么跟随者会将请求转发给领导者）
候选者：选举新的领导人时使用。

3 个角色的转换关系和流程：

可以看到，这张图里的几个概念：

初始化：所有服务器启动时，都是跟随者
超时：当初始时， 没有收到领导人的心跳，便开始选举。如果候选者选举的时候，也超时了，便重新选举。
大多数的选票：成为领导人的关键是 —— 获取大多数服务器的选票，可以看做是鸽笼原理。

2. 什么是任期
目录：

2 个阶段
任期
服务器之间的通信

背景：由于机器的物理时间是不可靠的，所以需要一个逻辑时间。称之为任期（term）。
从 2 个阶段开始说起：
集群节点通常有 2 个节点：

选举阶段
领导者正常运行

例外：当选举失败，会进行加时赛，即连续有 2 个选举过程。
见下图：

图 2 可见任期 3 选举失败，然后进行了重新选举。
重新选举会继续失败吗？
答：如果不做限制，是会理解失败的。
Raft 使用了一个限制来规避：每台服务器在选举的时候，使用一个随机数（150 -200），即每个服务器在不同的时间发出选举。防止冲突。称之为“加时赛”。
任期号
在上文中说道：由于机器的物理时间是不可靠的，所以需要一个逻辑时间。称之为任期（term）。
有关任期的几个特点：

节点有可能观察不到任何一次选举或者任期。
任期充当逻辑时钟，服务器节点可以通过任期号查明过期的领导者或过期的信息。
每个节点存储当前任期号，单调递增。
服务器之间的每次通信，都会交换当前任期号，交换任期号的目的： 4.1 如果一个服务器的当前任期号比其他人小，则更新自己。 4.2 如果一个候选人或者领导人发现自己的任期号过期了，立刻将自己变成跟随者状态。 4.3 如果一个节点收到一个过期的任期号的请求，直接拒绝。

3. 节点之间的通信
服务器之间通过 RPC 通信，即远程方法调用。
在 Raft 中，RPC 分成 3 种：

请求投票： RequestVote 由候选人在选举任期发起
附加条目 AppendEntries 由领导人发起，用来复制日志和提供心跳。
为了传输快照单独增加的 RPC。

备注：当服务没有及时收到 RPC 的响应时，会发起重试。
Summary

3 个状态的意义和他们之间切换
任期的设计背景，任期在节点通信时的作用，任期在增长策略。
节点直接的通信 RPC 类型以及策略。

https://www.cnblogs.com/stateis0/p/9993170.html
**************************************************
Python机器学习笔记 Logistic回归
Logistic回归公式推导和代码实现
1，引言
　　logistic回归是机器学习中最常用最经典的分类方法之一，有人称之为逻辑回归或者逻辑斯蒂回归。虽然他称为回归模型，但是却处理的是分类问题，这主要是因为它的本质是一个线性模型加上一个映射函数Sigmoid，将线性模型得到的连续结果映射到离散型上。它常用于二分类问题，在多分类问题的推广叫softmax。
　　本文首先阐述Logistic回归的定义，然后介绍一些最优化算法，其中包括基本的梯度上升法和一个改进的随机梯度上升法，这些最优化算法将用于分类器的训练，最好本文将给出一个Logistic回归的实例，预测一匹病马是否能被治愈。
　　在我们的日常生活中遇到过很多最优化问题，比如如何在最短时间内从A点到达B点？如何投入最少工作量却获得最大的效益？如何设计发动机使得油耗最少而功率最大？可见，最优化的作用十分强大，所以此处我们介绍几个最优化算法，并利用它们训练出一个非线性函数用于分类。
　　现在假设有一些数据点，我们用一条直线对这些点进行拟合（该线称为最佳拟合直线），这个拟合过程就称作回归。利用logistic回归进行分类的主要思想是：根据现有数据对分类边界线建立回归公式，以此进行分类，这里的“回归”一词源于最佳拟合，表示要找到最佳拟合参数集。训练分类器时的做法就是寻找最佳拟合参数，使用的是最优化算法，下面我们首先介绍一下这个二值型输出分类器的数学原理。
2，Logistic回归的一般过程
（1）收集数据：采用任意方法收集数据
（2）准备数据：由于需要进行距离计算，因此要求数据类型为数值型。另外，结构化数据格式则最佳
（3）分析数据：采用任意方法对数据进行分析
（4）训练算法：大部分时间将用于训练，训练的目的是为了找到最佳的分类回归系数
（5）使用算法：首先，我们需要输入一些数据，并将其转换成对应的结构化数值；接着，基于训练好的回归系数就可以对这些数值进行简单的回归计算，判定他们属于哪个类别；在这之后，我们就可以在输出的类别上做一些其他分析工作。
3，Logistic回归的优缺点
优点：计算代码不高，易于理解和实现，计算代价不高，速度快，存储资源低
缺点：容易欠拟合，分类精度可能不高
适用数据类型：数值型和标称型数据
4，基于Logistic回归和Sigmoid函数的分类
 　　我们想要的函数应该是，能接受所有的输入，然后预测出类型。例如，在两个类的情况下，上述函数输出0或1。该函数称为海维赛德阶跃函数（Heaviside step function），或者直接称为单位阶跃函数。然而，海维赛德阶跃函数的问题在于：该函数在跳跃点上从0瞬间跳跃到1，这个瞬间跳跃过程有时很难处理。幸好，另一个函数也有类似的性质（可以输出0或者1），且数学上更易处理，这就是Sigmoid函数。Sigmoid函数具体的计算公式如下：

　　图5-1给出了Sigmoid函数在不同坐标尺度下的两条曲线图。当x为0时，Sigmoid函数值为0.5。随着x的增大，对应的Sigmoid值将逼近于1；而随着x的减少，Sigmoid值将逼近于0.如果横坐标刻度足够大，Sigmoid函数看起来很像一个阶跃函数。
　　因此，为了实现Logistic回归分类器，我们可以在每个特征上都乘以一个回归系数，然后把所有的结果值相加，将这个总和带入Sigmoid函数中，进而得到一个范围在0~1之间的数值。任何大于0.5的数据被分入1类，小于0.5即被归入0类，所以，Logistic回归也可以被看成是一种概率估计。
　　确定了分类器的函数形式之后，现在的问题变成了：最佳回归系数是多少？如何确定其大小。

5，基于最优化方法的最佳回归系数确定
　　Sigmoid函数的输入记为z，由下面公式得到：

　　如果采用向量的写法，上述公式可以写成  z = wTx  ，它表示将这两个数值向量对应元素相乘，然后全部加起来即得到z值。

 
　　其中的向量x是分类器的输入数据，向量w也就是我们要找到的最佳参数（系数），从而使得分类器尽可能的准确，为了寻找该最佳参数，需要用到最优化理论的一些知识。
　　然后再看看我们的Logistic回归模型的公式：

 
　　这里假设 W>0，Y与X各维度叠加的图形关系，如下图所示（x为了方便取1维）：

 
 
　　下面首先学习梯度上升的最优化方法，我们将学习到如何使用该方法求得数据集的最佳参数，接下来，展示如何绘制梯度上升法产生的决策边界图，该图将梯度上升法的分类效果可视化的呈现出来，最后我们将学习随机梯度上升算法，以及如何对其进行修改以获得很好地结果。

    可能我们最常听到的是梯度下降算法，它与这里的梯度上升算法是一样的，只是公式中的
加法需要变成减法，梯度上升算法用来求函数的最大值，而梯度下降算法是用来求函数的最小值


　　
6，梯度上升法 
　　梯度上升法基于的思想是：要找到某函数的最大值，最好的方法是沿着该函数的梯度方向探寻，如果梯度记为，则函数 f(x,y) 的梯度由下面式子表示：

　　这个梯度意味着要沿着x的方向移动，沿着y方向移动，其中函数f(x,y)必须要在待计算的点上有定义并且可微，一个具体的函数例子见图5-2：

 　　上图中的梯度上升算法沿梯度方向移动了一步，可以看出，梯度算子总是指向函数值增长最快的方向。这里所说的移动方向，而未提到移动量的大小。该量值称为步长，记为。用向量来表示的话，梯度算法的迭代公式如下：

　　该公式将一直被迭代执行，直至达到某个停止条件为止，比如迭代次数达到某个指定值或算法达到某个可以允许的误差范围。
　　基于上面的内容，我们来看一个Logistic回归分类器的应用例子，从图5-3可以看到我们采用的数据集。

梯度上升法的公式推导
　　由于Sigmoid函数的特性，我们可以做如下的假设：

　　上式即为在已知样本X和参数θ的情况下。样本X属性正类（y=1）和负类（y=0）的条件概率，将两个公式合并成一个，如下：

　　假定样本与样本之间相互独立，那么整个样本集生成的概率即为所有样本生成概率的乘积：

　　为了简化问题，我们对整个表达式求对数：

　　满足似然函数（θ）的最大的θ值即时我们需要求解的模型。
　　那么梯度上升法就像爬坡一样，一点一点逼近极值，而上升这个动作用数学公式表达即为：

　　其中，α 为步长。
　　回到Logistic回归问题，我们同样对函数求偏导。

对这个公式进行分解，先看：

　　我们可以看到，对函数求偏导，分解为三部分，然后我们对这三部分分布求导。
其中：

再由：

可得：

接下来：

最后：

综合三部分即得到：

因此梯度迭代公式为：
 
  
7，训练算法：使用梯度上升找到最佳参数
 　　上图有100个样本点，每个点包含两个数值型特征：X1和X2，在此数据集上，我们将通过使用梯度上升法找到最佳回归系数，也就是拟合出Logistic回归模型的最佳参数。
　　梯度上升法的伪代码如下：

每个回归系数初始化为1

重复R次：
    
    计算整个数据集的梯度

    使用alpha * gradient 更新回归系数的向量

    返回回归系数


　　testSet.txt的文件内容如下：

-0.017612	14.053064	0
-1.395634	4.662541	1
-0.752157	6.538620	0
-1.322371	7.152853	0
0.423363	11.054677	0
0.406704	7.067335	1
0.667394	12.741452	0
-2.460150	6.866805	1
0.569411	9.548755	0
-0.026632	10.427743	0
0.850433	6.920334	1
1.347183	13.175500	0
1.176813	3.167020	1
-1.781871	9.097953	0
-0.566606	5.749003	1
0.931635	1.589505	1
-0.024205	6.151823	1
-0.036453	2.690988	1
-0.196949	0.444165	1
1.014459	5.754399	1
1.985298	3.230619	1
-1.693453	-0.557540	1
-0.576525	11.778922	0
-0.346811	-1.678730	1
-2.124484	2.672471	1
1.217916	9.597015	0
-0.733928	9.098687	0
-3.642001	-1.618087	1
0.315985	3.523953	1
1.416614	9.619232	0
-0.386323	3.989286	1
0.556921	8.294984	1
1.224863	11.587360	0
-1.347803	-2.406051	1
1.196604	4.951851	1
0.275221	9.543647	0
0.470575	9.332488	0
-1.889567	9.542662	0
-1.527893	12.150579	0
-1.185247	11.309318	0
-0.445678	3.297303	1
1.042222	6.105155	1
-0.618787	10.320986	0
1.152083	0.548467	1
0.828534	2.676045	1
-1.237728	10.549033	0
-0.683565	-2.166125	1
0.229456	5.921938	1
-0.959885	11.555336	0
0.492911	10.993324	0
0.184992	8.721488	0
-0.355715	10.325976	0
-0.397822	8.058397	0
0.824839	13.730343	0
1.507278	5.027866	1
0.099671	6.835839	1
-0.344008	10.717485	0
1.785928	7.718645	1
-0.918801	11.560217	0
-0.364009	4.747300	1
-0.841722	4.119083	1
0.490426	1.960539	1
-0.007194	9.075792	0
0.356107	12.447863	0
0.342578	12.281162	0
-0.810823	-1.466018	1
2.530777	6.476801	1
1.296683	11.607559	0
0.475487	12.040035	0
-0.783277	11.009725	0
0.074798	11.023650	0
-1.337472	0.468339	1
-0.102781	13.763651	0
-0.147324	2.874846	1
0.518389	9.887035	0
1.015399	7.571882	0
-1.658086	-0.027255	1
1.319944	2.171228	1
2.056216	5.019981	1
-0.851633	4.375691	1
-1.510047	6.061992	0
-1.076637	-3.181888	1
1.821096	10.283990	0
3.010150	8.401766	1
-1.099458	1.688274	1
-0.834872	-1.733869	1
-0.846637	3.849075	1
1.400102	12.628781	0
1.752842	5.468166	1
0.078557	0.059736	1
0.089392	-0.715300	1
1.825662	12.693808	0
0.197445	9.744638	0
0.126117	0.922311	1
-0.679797	1.220530	1
0.677983	2.556666	1
0.761349	10.693862	0
-2.168791	0.143632	1
1.388610	9.341997	0
0.317029	14.739025	0


　　
　　下面具体实现梯度上升算法的代码：

#_*_coding:utf-8_*_
from numpy import *

# 读取数据
def loadDataSet(filename):
    '''
        对于testSet.txt，每行前两个值分别是X1和X2，第三个值数据对应的类别标签
        而且为了设置方便，该函数还将X0的值设置为1.0
        :return:
        '''
    dataMat = []
    labelMat = []
    fr = open(filename)
    for line in fr.readlines():
        lineArr = line.strip().split()
        dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])])
        labelMat.append(int(lineArr[2]))
    return dataMat,labelMat

def sigmoid(inX):
    return 1.0/(1+exp(-inX))

def gradAscent(dataMatIn,classLabels):
    '''
        :param dataMatIn: 是一个2维Numpy数组，每列分别代表每个不同的特征
        每行则代表每个训练样本。
        :param classLabels: 是类别标签，是一个1*100的行向量，为了便于矩阵运算，需要将行向量
        转换为列向量，就是矩阵的转置，再将其赋值与labelMat。
        :return:
        '''
    dataMatrix = mat(dataMatIn)
    labelMat = mat(classLabels).transpose()
    # labelMat = mat(classLabels).T
    m,n = shape(dataMatrix)
    # alpha是向目标移动的步长
    alpha = 0.001
    # 迭代次数
    maxCycles = 500
    weights = ones((n,1))
    for k in range(maxCycles):
        h = sigmoid(dataMatrix*weights)
        error = (labelMat-h)
        weights = weights + alpha*dataMatrix.transpose()*error
    return weights


　　测试结果如下：

if __name__  == '__main__':
    filename = 'testSet.txt'
    dataArr,labelMat = loadDataSet(filename)
    weights_res = gradAscent(dataArr,labelMat)
    print(weights_res)
    
'''
[[ 4.12414349]
 [ 0.48007329]
 [-0.6168482 ]]
 '''


　　上面已经解出了一组回归系数，它确定了不同类别数据之间的分割线，那么怎样画出该分割线，从而使得优化的过程便于理解呢？下面代码来解决这个问题。
　　画出数据集和Logistic回归最佳拟合直线的函数代码：

def plotBestFit(wei):
    import matplotlib.pyplot as plt
    weights = wei.getA()
    dataMat,labelMat = loadDataSet(filename)
    dataArr = array(dataMat)
    n = shape(dataArr)[0]
    xcord1 = []
    ycord1 = []
    xcord2 = []
    ycord2 = []
    for i in range(n):
        if int(labelMat[i]) ==1:
            xcord1.append(dataArr[i,1])
            ycord1.append(dataArr[i,2])
        else:
            xcord2.append(dataArr[i, 1])
            ycord2.append(dataArr[i, 2])
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.scatter(xcord1,ycord1,s=30,c='red',marker='s')
    ax.scatter(xcord2,ycord2,s=30,c='green')
    x = arange(-3.0,3.0,0.1)
    y = (-weights[0]-weights[1] * x) / weights[2]
    ax.plot(x,y)
    plt.xlabel('X1')
    plt.ylabel('X2')
    plt.show()


　　
　　输出的结果和代码如下图所示：

if __name__  == '__main__':
    filename = 'testSet.txt'
    dataArr,labelMat = loadDataSet(filename)
    weights_res = gradAscent(dataArr,labelMat)
    print(weights_res)
    plotBestFit(weights_res)

 

梯度上升算法在500次迭代后得到的Logistic回归最佳拟合直线
　　这个分类结果相当不错，从图上看，只错分了四个点。但是，尽管例子简单且数据集很小，这个方法却需要大量的计算（300次乘法），因此下一节将对算法稍作改进，从而使它可以用在真实数据集上。
8，训练算法：随机梯度上升
 　　梯度上升算法在每次更新回归系数时都需遍历整个数据集，该方法在处理100个左右的数据集尚可，但是若有数十亿样本和成千上万的特征，那么该方法的计算复杂度就太高了。一种改进方法是一次仅用一个样本点来更新回归系数，该方法称为随机梯度上升算法。由于可以在新样本到来时对分类器进行增量式更新，因而随机梯度上升算法是一个在线学习算法，与“在线学习”相对应，一次处理所有数据被称作是“批处理”。
　　随机梯度上升算法可以写成如下的伪代码：

所有回归系数初始化为1

对数据集中每个样本
    
    计算该样本的梯度

    使用alpha*gradient 更新回归系数值

返回回归系数值


　　以下是随机梯度上升算法的实现代码：

# 随机梯度上升算法
def stocGradAscent0(dataMatrix,classLabels):
    m,n = shape(dataMatrix)
    alpha = 0.01
    weights = ones(n)
    for i in range(m):
        h = sigmoid(sum(dataMatrix[i]*weights))
        error = classLabels[i] - h
        weights = weights + alpha*error*dataMatrix[i]
    return weights


　　实现的代码如下：

if __name__  == '__main__':
    filename = 'testSet.txt'
    dataArr,labelMat = loadDataSet(filename)
    weights_res = stocGradAscent0(array(dataArr),labelMat)
    print(weights_res)
    plotBestFit(weights_res)


　　
图5-5  随机梯度上升算法在上述数据集上的执行结果，最佳拟合直线并非最佳分类线
　　可以看出，拟合出来的直线效果还不错，但并不像，但是不像上面那个完美，这里的分类器错分了三分之一的样本。
　　直接比较结果两个结果是不公平的，后者的结果是在整个数据集上迭代了500次才得到的。一个判断优化算法优劣的可靠方法是看它是否收敛，也就是说参数是否达到了稳定值，是否还会不断地变化？对此，我们在上面的的随机梯度算法上做了些修改，使其在整个数据集上运行200次，最终绘制的三个回归系数的变化情况如下图所示：

　　上图展示了随机梯度上升算法在200次迭代过程中回归系数的变化情况，其中的系数2，也就是图5-5中的X2只经过了50次迭代就达到了稳定值，但系数1和0则需要更多次的迭代。另外值得注意的是，在大的波动停止后，还有一些小的周期性波动。不难理解，产生这种现象的原因是存在一些不能正确分类的样本点（数据集并非现象可分），在每次迭代时会引发系数的剧烈改变。我们期望算法能避免来回波动，从而收敛到某个值。另外，收敛速度也需要加快。
　　改进的随机梯度上升算法代码如下：

# 改进的随机梯度上升算法
def stocGradAscent1(dataMatrix,classLabels,numIter=150):
    m,n = shape(dataMatrix)
    weights = ones(n)
    for j in range(numIter):
        dataIndex = list(range(m))
        for i in range(m):
            alpha = 4/(1.0+j+i)+0.01
            randIndex = int(random.uniform(0,len(dataIndex)))
            h = sigmoid(sum(dataMatrix[randIndex]*weights))
            error = classLabels[randIndex] - h
            weights = weights + alpha *error*dataMatrix[randIndex]
            del(dataIndex[randIndex])
    return weights


　　上述代码大体上与之前的随机梯度上升算法一致，修改了两处，一处是alpha在每次迭代的时候都会调整，这会环节之前的数据波动或者高频波动。另外，虽然alpha会随着迭代次数不断减少，但永远不会减少到0,。必须这样做的原因是为了保证在多次迭代之后新数据仍然具有一定的影响。如果要处理的问题是动态变化的，那么可以适当增加常数项，来确保新的值获得更大的回归系数。另外一点值得注意的是，在降低alpha的函数中，alpha每次减少1/(j+1)，其中j是迭代次数，i是样本点的下标，这样当j<max(i)的时候，alpha就不是严格下降的，避免参数的严格下降也常见于模拟退火算法等其他优化算法中。程序的第二个改进地方就是通过随机选取样本来更新回归系数，这种方法将减少周期性的波动，并且改进的算法还增加一个迭代次数作为第三个参数，如果该参数没有给定的话，算法将默认迭代150次，如果给定，那么算法将按照新的参数值进行迭代。
　　与stocGradAscent1()类似，下图显示了每次迭代时各个回归系数的变化情况。

　　比较5-7和5-6可以看到两点不同，第一点是，图5-7中的系数没有像5-6里那样出现周期性的波动，这归功于stocGradAscent1()里的样本随机选择机制，第二点是5-7的水平轴比5-6的短了很多，这是由于stocGradAscent1()可以收敛的更快，这次我们仅仅对数据集做了20次遍历，之前是500次。
　　下面看看在同一个数据集上的分类效果，将程序运行可以看到：

if __name__  == '__main__':
    filename = 'testSet.txt'
    dataArr,labelMat = loadDataSet(filename)
    weights_res = stocGradAscent1(array(dataArr),labelMat)
    print(weights_res)
    plotBestFit(weights_res)


　　

　　该分割线达到了与GradientAscent()差不多的效果，但是所使用的计算量更少。
　　默认的迭代次数是150次，但是我们通过stocGradAscent()的第三个参数来对此进行修改，例如：

weights_res = stocGradAscent1(array(dataArr),labelMat,500)


 
　　迄今为止我们分析了回归系数的变化情况，但是还没有达到目的，，即完成具体的分类任务，下面我们将使用随机梯度上升算法来解决病马的生死预测问题。,
9，示例：从疝气病症预测病马的死亡率
　　本次将使用logistic回归来预测患有疝气的马的存活问题，这里的数据包含了368个样本和28个特征。（疝气指的是马胃肠痛的术语，然而这种病不一定源于马 的肠胃问题，其他问题也可以引发疝气）该数据集包含了医院检测马疝气病的一些指标，有的指标比较主观，有的指标难以预测，例如马的疼痛级别
使用Logistic回归估计马疝气病的死亡率的流程
（1）收集数据：使用给定数据文件
（2）准备数据：用Python解析文本文件并填充缺失值
（3）分析数据：可视化并观察数据
（4）训练算法：使用优化算法，找到最佳的系数
（5）测试算法：为了量化回归的结果，需要观察错误率，根据错误率决定是否回退到训练阶段，通过改变迭代的次数和步长等参数来得到更好的回归系数
（6）使用算法：实现一个简单的命令行程序来收集马的症状并输出预测结果
 
 
准备数据：处理数据中的缺失值
　　数据中的缺失值是个非常棘手的问题，有很少文献都致力于解决这个问题。那么，数据缺少究竟带来了什么问题？假设有100个样本和20个特征，这些数据都是机器收集回来的，若机器上的某个传感器损坏导致一个特征无效时该怎么办？此时是否要扔掉整个数据？这种情况下，另外19个特征怎么办？它们是否还可用？答案是肯定的。因为有时候数据相当昂贵，扔掉和重新获取都是不可取的，所以必须采取一些方法来解决这个问题。
　　下面给出了一些可选的做法：

使用可用特征的均值来填补缺失值；
使用特征值来填充缺失值，如-1
忽略有缺失值的样本
使用相似样本的均值填补缺失值
使用另外的机器学习算法预测缺失值

 　　现在，我们对下一节要用的数据集进行预处理，使其可以顺利地使用分类算法。在预处理阶段需要做两件事：第一，所有的缺失值必须用一个实数值来替换，因为我们使用的Numpy数据类型不允许包含缺失值。这里选择实数0来替换，因为我们使用的Numpy数据类型不允许包括缺失值，这里选择实数0来替换所有缺失值，恰好能适应于Logistic回归。这样做的直觉在于，我们需要的是一个在更新时不会影响系数的值，回归系数的更新公式如下：

weights = weights + alpha *error*dataMatrix[randIndex]


　　如果dataMatirx的某特征对应值为0，那么该特征的系数将不做更新，即：

weights = weights


　　另外，由于sigmoid(0) = 0.5 ，即对它结果的预测不具有任何倾向性，因此上述做法也不会对误差项造成任何影响，基于上述原因，将缺失值用0代替既可以保留现有数据，也不需要对优化算法进行修改，此外，数据集中的特征值一般不取0，因此在某种意义上说它也满足“特殊值”这个要求。
　　预处理中做的第二件事，是如果在测试数据集中发现了一条数据的类别标签以及缺失，那么我们的简单做法是将该条数据丢弃。这是因为类别标签与特征不同，很难确定采用某个合适的值来替换，采用Logistic回归进行预处理之后保存两个文件，horseColicTest.txt和horseColicTraining.txt。如果想对于原始数据和预处理之后的数据做个比较。
　　我们有一个“干净”可用的数据集和一个不错的优化算法，下面将这些部分融合在一起训练出一个分类器，然后利用该分类器来预测病马的生死问题。
horseColicTest.txt的数据：

2	1	38.50	54	20	0	1	2	2	3	4	1	2	2	5.90	0	2	42.00	6.30	0	0	1
2	1	37.60	48	36	0	0	1	1	0	3	0	0	0	0	0	0	44.00	6.30	1	5.00	1
1	1	37.7	44	28	0	4	3	2	5	4	4	1	1	0	3	5	45	70	3	2	1
1	1	37	56	24	3	1	4	2	4	4	3	1	1	0	0	0	35	61	3	2	0
2	1	38.00	42	12	3	0	3	1	1	0	1	0	0	0	0	2	37.00	5.80	0	0	1
1	1	0	60	40	3	0	1	1	0	4	0	3	2	0	0	5	42	72	0	0	1
2	1	38.40	80	60	3	2	2	1	3	2	1	2	2	0	1	1	54.00	6.90	0	0	1
2	1	37.80	48	12	2	1	2	1	3	0	1	2	0	0	2	0	48.00	7.30	1	0	1
2	1	37.90	45	36	3	3	3	2	2	3	1	2	1	0	3	0	33.00	5.70	3	0	1
2	1	39.00	84	12	3	1	5	1	2	4	2	1	2	7.00	0	4	62.00	5.90	2	2.20	0
2	1	38.20	60	24	3	1	3	2	3	3	2	3	3	0	4	4	53.00	7.50	2	1.40	1
1	1	0	140	0	0	0	4	2	5	4	4	1	1	0	0	5	30	69	0	0	0
1	1	37.90	120	60	3	3	3	1	5	4	4	2	2	7.50	4	5	52.00	6.60	3	1.80	0
2	1	38.00	72	36	1	1	3	1	3	0	2	2	1	0	3	5	38.00	6.80	2	2.00	1
2	9	38.00	92	28	1	1	2	1	1	3	2	3	0	7.20	0	0	37.00	6.10	1	1.10	1
1	1	38.30	66	30	2	3	1	1	2	4	3	3	2	8.50	4	5	37.00	6.00	0	0	1
2	1	37.50	48	24	3	1	1	1	2	1	0	1	1	0	3	2	43.00	6.00	1	2.80	1
1	1	37.50	88	20	2	3	3	1	4	3	3	0	0	0	0	0	35.00	6.40	1	0	0
2	9	0	150	60	4	4	4	2	5	4	4	0	0	0	0	0	0	0	0	0	0
1	1	39.7	100	30	0	0	6	2	4	4	3	1	0	0	4	5	65	75	0	0	0
1	1	38.30	80	0	3	3	4	2	5	4	3	2	1	0	4	4	45.00	7.50	2	4.60	1
2	1	37.50	40	32	3	1	3	1	3	2	3	2	1	0	0	5	32.00	6.40	1	1.10	1
1	1	38.40	84	30	3	1	5	2	4	3	3	2	3	6.50	4	4	47.00	7.50	3	0	0
1	1	38.10	84	44	4	0	4	2	5	3	1	1	3	5.00	0	4	60.00	6.80	0	5.70	0
2	1	38.70	52	0	1	1	1	1	1	3	1	0	0	0	1	3	4.00	74.00	0	0	1
2	1	38.10	44	40	2	1	3	1	3	3	1	0	0	0	1	3	35.00	6.80	0	0	1
2	1	38.4	52	20	2	1	3	1	1	3	2	2	1	0	3	5	41	63	1	1	1
1	1	38.20	60	0	1	0	3	1	2	1	1	1	1	0	4	4	43.00	6.20	2	3.90	1
2	1	37.70	40	18	1	1	1	0	3	2	1	1	1	0	3	3	36.00	3.50	0	0	1
1	1	39.1	60	10	0	1	1	0	2	3	0	0	0	0	4	4	0	0	0	0	1
2	1	37.80	48	16	1	1	1	1	0	1	1	2	1	0	4	3	43.00	7.50	0	0	1
1	1	39.00	120	0	4	3	5	2	2	4	3	2	3	8.00	0	0	65.00	8.20	3	4.60	1
1	1	38.20	76	0	2	3	2	1	5	3	3	1	2	6.00	1	5	35.00	6.50	2	0.90	1
2	1	38.30	88	0	0	0	6	0	0	0	0	0	0	0	0	0	0	0	0	0	0
1	1	38.00	80	30	3	3	3	1	0	0	0	0	0	6.00	0	0	48.00	8.30	0	4.30	1
1	1	0	0	0	3	1	1	1	2	3	3	1	3	6.00	4	4	0	0	2	0	0
1	1	37.60	40	0	1	1	1	1	1	1	1	0	0	0	1	1	0	0	2	2.10	1
2	1	37.50	44	0	1	1	1	1	3	3	2	0	0	0	0	0	45.00	5.80	2	1.40	1
2	1	38.2	42	16	1	1	3	1	1	3	1	0	0	0	1	0	35	60	1	1	1
2	1	38	56	44	3	3	3	0	0	1	1	2	1	0	4	0	47	70	2	1	1
2	1	38.30	45	20	3	3	2	2	2	4	1	2	0	0	4	0	0	0	0	0	1
1	1	0	48	96	1	1	3	1	0	4	1	2	1	0	1	4	42.00	8.00	1	0	1
1	1	37.70	55	28	2	1	2	1	2	3	3	0	3	5.00	4	5	0	0	0	0	1
2	1	36.00	100	20	4	3	6	2	2	4	3	1	1	0	4	5	74.00	5.70	2	2.50	0
1	1	37.10	60	20	2	0	4	1	3	0	3	0	2	5.00	3	4	64.00	8.50	2	0	1
2	1	37.10	114	40	3	0	3	2	2	2	1	0	0	0	0	3	32.00	0	3	6.50	1
1	1	38.1	72	30	3	3	3	1	4	4	3	2	1	0	3	5	37	56	3	1	1
1	1	37.00	44	12	3	1	1	2	1	1	1	0	0	0	4	2	40.00	6.70	3	8.00	1
1	1	38.6	48	20	3	1	1	1	4	3	1	0	0	0	3	0	37	75	0	0	1
1	1	0	82	72	3	1	4	1	2	3	3	0	3	0	4	4	53	65	3	2	0
1	9	38.20	78	60	4	4	6	0	3	3	3	0	0	0	1	0	59.00	5.80	3	3.10	0
2	1	37.8	60	16	1	1	3	1	2	3	2	1	2	0	3	0	41	73	0	0	0
1	1	38.7	34	30	2	0	3	1	2	3	0	0	0	0	0	0	33	69	0	2	0
1	1	0	36	12	1	1	1	1	1	2	1	1	1	0	1	5	44.00	0	0	0	1
2	1	38.30	44	60	0	0	1	1	0	0	0	0	0	0	0	0	6.40	36.00	0	0	1
2	1	37.40	54	18	3	0	1	1	3	4	3	2	2	0	4	5	30.00	7.10	2	0	1
1	1	0	0	0	4	3	0	2	2	4	1	0	0	0	0	0	54	76	3	2	1
1	1	36.6	48	16	3	1	3	1	4	1	1	1	1	0	0	0	27	56	0	0	0
1	1	38.5	90	0	1	1	3	1	3	3	3	2	3	2	4	5	47	79	0	0	1
1	1	0	75	12	1	1	4	1	5	3	3	0	3	5.80	0	0	58.00	8.50	1	0	1
2	1	38.20	42	0	3	1	1	1	1	1	2	2	1	0	3	2	35.00	5.90	2	0	1
1	9	38.20	78	60	4	4	6	0	3	3	3	0	0	0	1	0	59.00	5.80	3	3.10	0
2	1	38.60	60	30	1	1	3	1	4	2	2	1	1	0	0	0	40.00	6.00	1	0	1
2	1	37.80	42	40	1	1	1	1	1	3	1	0	0	0	3	3	36.00	6.20	0	0	1
1	1	38	60	12	1	1	2	1	2	1	1	1	1	0	1	4	44	65	3	2	0
2	1	38.00	42	12	3	0	3	1	1	1	1	0	0	0	0	1	37.00	5.80	0	0	1
2	1	37.60	88	36	3	1	1	1	3	3	2	1	3	1.50	0	0	44.00	6.00	0	0	0


　　
horseColicTraining.txt的数据：

2.000000	1.000000	38.500000	66.000000	28.000000	3.000000	3.000000	0.000000	2.000000	5.000000	4.000000	4.000000	0.000000	0.000000	0.000000	3.000000	5.000000	45.000000	8.400000	0.000000	0.000000	0.000000
1.000000	1.000000	39.200000	88.000000	20.000000	0.000000	0.000000	4.000000	1.000000	3.000000	4.000000	2.000000	0.000000	0.000000	0.000000	4.000000	2.000000	50.000000	85.000000	2.000000	2.000000	0.000000
2.000000	1.000000	38.300000	40.000000	24.000000	1.000000	1.000000	3.000000	1.000000	3.000000	3.000000	1.000000	0.000000	0.000000	0.000000	1.000000	1.000000	33.000000	6.700000	0.000000	0.000000	1.000000
1.000000	9.000000	39.100000	164.000000	84.000000	4.000000	1.000000	6.000000	2.000000	2.000000	4.000000	4.000000	1.000000	2.000000	5.000000	3.000000	0.000000	48.000000	7.200000	3.000000	5.300000	0.000000
2.000000	1.000000	37.300000	104.000000	35.000000	0.000000	0.000000	6.000000	2.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	74.000000	7.400000	0.000000	0.000000	0.000000
2.000000	1.000000	0.000000	0.000000	0.000000	2.000000	1.000000	3.000000	1.000000	2.000000	3.000000	2.000000	2.000000	1.000000	0.000000	3.000000	3.000000	0.000000	0.000000	0.000000	0.000000	1.000000
1.000000	1.000000	37.900000	48.000000	16.000000	1.000000	1.000000	1.000000	1.000000	3.000000	3.000000	3.000000	1.000000	1.000000	0.000000	3.000000	5.000000	37.000000	7.000000	0.000000	0.000000	1.000000
1.000000	1.000000	0.000000	60.000000	0.000000	3.000000	0.000000	0.000000	1.000000	0.000000	4.000000	2.000000	2.000000	1.000000	0.000000	3.000000	4.000000	44.000000	8.300000	0.000000	0.000000	0.000000
2.000000	1.000000	0.000000	80.000000	36.000000	3.000000	4.000000	3.000000	1.000000	4.000000	4.000000	4.000000	2.000000	1.000000	0.000000	3.000000	5.000000	38.000000	6.200000	0.000000	0.000000	0.000000
2.000000	9.000000	38.300000	90.000000	0.000000	1.000000	0.000000	1.000000	1.000000	5.000000	3.000000	1.000000	2.000000	1.000000	0.000000	3.000000	0.000000	40.000000	6.200000	1.000000	2.200000	1.000000
1.000000	1.000000	38.100000	66.000000	12.000000	3.000000	3.000000	5.000000	1.000000	3.000000	3.000000	1.000000	2.000000	1.000000	3.000000	2.000000	5.000000	44.000000	6.000000	2.000000	3.600000	1.000000
2.000000	1.000000	39.100000	72.000000	52.000000	2.000000	0.000000	2.000000	1.000000	2.000000	1.000000	2.000000	1.000000	1.000000	0.000000	4.000000	4.000000	50.000000	7.800000	0.000000	0.000000	1.000000
1.000000	1.000000	37.200000	42.000000	12.000000	2.000000	1.000000	1.000000	1.000000	3.000000	3.000000	3.000000	3.000000	1.000000	0.000000	4.000000	5.000000	0.000000	7.000000	0.000000	0.000000	1.000000
2.000000	9.000000	38.000000	92.000000	28.000000	1.000000	1.000000	2.000000	1.000000	1.000000	3.000000	2.000000	3.000000	0.000000	7.200000	1.000000	1.000000	37.000000	6.100000	1.000000	0.000000	0.000000
1.000000	1.000000	38.200000	76.000000	28.000000	3.000000	1.000000	1.000000	1.000000	3.000000	4.000000	1.000000	2.000000	2.000000	0.000000	4.000000	4.000000	46.000000	81.000000	1.000000	2.000000	1.000000
1.000000	1.000000	37.600000	96.000000	48.000000	3.000000	1.000000	4.000000	1.000000	5.000000	3.000000	3.000000	2.000000	3.000000	4.500000	4.000000	0.000000	45.000000	6.800000	0.000000	0.000000	0.000000
1.000000	9.000000	0.000000	128.000000	36.000000	3.000000	3.000000	4.000000	2.000000	4.000000	4.000000	3.000000	3.000000	0.000000	0.000000	4.000000	5.000000	53.000000	7.800000	3.000000	4.700000	0.000000
2.000000	1.000000	37.500000	48.000000	24.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	1.000000
1.000000	1.000000	37.600000	64.000000	21.000000	1.000000	1.000000	2.000000	1.000000	2.000000	3.000000	1.000000	1.000000	1.000000	0.000000	2.000000	5.000000	40.000000	7.000000	1.000000	0.000000	1.000000
2.000000	1.000000	39.400000	110.000000	35.000000	4.000000	3.000000	6.000000	0.000000	0.000000	3.000000	3.000000	0.000000	0.000000	0.000000	0.000000	0.000000	55.000000	8.700000	0.000000	0.000000	1.000000
1.000000	1.000000	39.900000	72.000000	60.000000	1.000000	1.000000	5.000000	2.000000	5.000000	4.000000	4.000000	3.000000	1.000000	0.000000	4.000000	4.000000	46.000000	6.100000	2.000000	0.000000	1.000000
2.000000	1.000000	38.400000	48.000000	16.000000	1.000000	0.000000	1.000000	1.000000	1.000000	3.000000	1.000000	2.000000	3.000000	5.500000	4.000000	3.000000	49.000000	6.800000	0.000000	0.000000	1.000000
1.000000	1.000000	38.600000	42.000000	34.000000	2.000000	1.000000	4.000000	0.000000	2.000000	3.000000	1.000000	0.000000	0.000000	0.000000	1.000000	0.000000	48.000000	7.200000	0.000000	0.000000	1.000000
1.000000	9.000000	38.300000	130.000000	60.000000	0.000000	3.000000	0.000000	1.000000	2.000000	4.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	50.000000	70.000000	0.000000	0.000000	1.000000
1.000000	1.000000	38.100000	60.000000	12.000000	3.000000	3.000000	3.000000	1.000000	0.000000	4.000000	3.000000	3.000000	2.000000	2.000000	0.000000	0.000000	51.000000	65.000000	0.000000	0.000000	1.000000
2.000000	1.000000	37.800000	60.000000	42.000000	0.000000	0.000000	0.000000	1.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	1.000000
1.000000	1.000000	38.300000	72.000000	30.000000	4.000000	3.000000	3.000000	2.000000	3.000000	3.000000	3.000000	2.000000	1.000000	0.000000	3.000000	5.000000	43.000000	7.000000	2.000000	3.900000	1.000000
1.000000	1.000000	37.800000	48.000000	12.000000	3.000000	1.000000	1.000000	1.000000	0.000000	3.000000	2.000000	1.000000	1.000000	0.000000	1.000000	3.000000	37.000000	5.500000	2.000000	1.300000	1.000000
1.000000	1.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000
2.000000	1.000000	37.700000	48.000000	0.000000	2.000000	1.000000	1.000000	1.000000	1.000000	1.000000	1.000000	1.000000	1.000000	0.000000	0.000000	0.000000	45.000000	76.000000	0.000000	0.000000	1.000000
2.000000	1.000000	37.700000	96.000000	30.000000	3.000000	3.000000	4.000000	2.000000	5.000000	4.000000	4.000000	3.000000	2.000000	4.000000	4.000000	5.000000	66.000000	7.500000	0.000000	0.000000	0.000000
2.000000	1.000000	37.200000	108.000000	12.000000	3.000000	3.000000	4.000000	2.000000	2.000000	4.000000	2.000000	0.000000	3.000000	6.000000	3.000000	3.000000	52.000000	8.200000	3.000000	7.400000	0.000000
1.000000	1.000000	37.200000	60.000000	0.000000	2.000000	1.000000	1.000000	1.000000	3.000000	3.000000	3.000000	2.000000	1.000000	0.000000	4.000000	5.000000	43.000000	6.600000	0.000000	0.000000	1.000000
1.000000	1.000000	38.200000	64.000000	28.000000	1.000000	1.000000	1.000000	1.000000	3.000000	1.000000	0.000000	0.000000	0.000000	0.000000	4.000000	4.000000	49.000000	8.600000	2.000000	6.600000	1.000000
1.000000	1.000000	0.000000	100.000000	30.000000	3.000000	3.000000	4.000000	2.000000	5.000000	4.000000	4.000000	3.000000	3.000000	0.000000	4.000000	4.000000	52.000000	6.600000	0.000000	0.000000	1.000000
2.000000	1.000000	0.000000	104.000000	24.000000	4.000000	3.000000	3.000000	2.000000	4.000000	4.000000	3.000000	0.000000	3.000000	0.000000	0.000000	2.000000	73.000000	8.400000	0.000000	0.000000	0.000000
2.000000	1.000000	38.300000	112.000000	16.000000	0.000000	3.000000	5.000000	2.000000	0.000000	0.000000	1.000000	1.000000	2.000000	0.000000	0.000000	5.000000	51.000000	6.000000	2.000000	1.000000	0.000000
1.000000	1.000000	37.800000	72.000000	0.000000	0.000000	3.000000	0.000000	1.000000	5.000000	3.000000	1.000000	0.000000	1.000000	0.000000	1.000000	1.000000	56.000000	80.000000	1.000000	2.000000	1.000000
2.000000	1.000000	38.600000	52.000000	0.000000	1.000000	1.000000	1.000000	1.000000	3.000000	3.000000	2.000000	1.000000	1.000000	0.000000	1.000000	3.000000	32.000000	6.600000	1.000000	5.000000	1.000000
1.000000	9.000000	39.200000	146.000000	96.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000
1.000000	1.000000	0.000000	88.000000	0.000000	3.000000	3.000000	6.000000	2.000000	5.000000	3.000000	3.000000	1.000000	3.000000	0.000000	4.000000	5.000000	63.000000	6.500000	3.000000	0.000000	0.000000
2.000000	9.000000	39.000000	150.000000	72.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	47.000000	8.500000	0.000000	0.100000	1.000000
2.000000	1.000000	38.000000	60.000000	12.000000	3.000000	1.000000	3.000000	1.000000	3.000000	3.000000	1.000000	1.000000	1.000000	0.000000	2.000000	2.000000	47.000000	7.000000	0.000000	0.000000	1.000000
1.000000	1.000000	0.000000	120.000000	0.000000	3.000000	4.000000	4.000000	1.000000	4.000000	4.000000	4.000000	1.000000	1.000000	0.000000	0.000000	5.000000	52.000000	67.000000	2.000000	2.000000	0.000000
1.000000	1.000000	35.400000	140.000000	24.000000	3.000000	3.000000	4.000000	2.000000	4.000000	4.000000	0.000000	2.000000	1.000000	0.000000	0.000000	5.000000	57.000000	69.000000	3.000000	2.000000	0.000000
2.000000	1.000000	0.000000	120.000000	0.000000	4.000000	3.000000	4.000000	2.000000	5.000000	4.000000	4.000000	1.000000	1.000000	0.000000	4.000000	5.000000	60.000000	6.500000	3.000000	0.000000	0.000000
1.000000	1.000000	37.900000	60.000000	15.000000	3.000000	0.000000	4.000000	2.000000	5.000000	4.000000	4.000000	2.000000	2.000000	0.000000	4.000000	5.000000	65.000000	7.500000	0.000000	0.000000	1.000000
2.000000	1.000000	37.500000	48.000000	16.000000	1.000000	1.000000	1.000000	1.000000	1.000000	1.000000	1.000000	1.000000	1.000000	0.000000	1.000000	0.000000	37.000000	6.500000	0.000000	0.000000	1.000000
1.000000	1.000000	38.900000	80.000000	44.000000	3.000000	3.000000	3.000000	2.000000	2.000000	3.000000	3.000000	2.000000	2.000000	7.000000	3.000000	1.000000	54.000000	6.500000	3.000000	0.000000	0.000000
2.000000	1.000000	37.200000	84.000000	48.000000	3.000000	3.000000	5.000000	2.000000	4.000000	1.000000	2.000000	1.000000	2.000000	0.000000	2.000000	1.000000	73.000000	5.500000	2.000000	4.100000	0.000000
2.000000	1.000000	38.600000	46.000000	0.000000	1.000000	1.000000	2.000000	1.000000	1.000000	3.000000	2.000000	1.000000	1.000000	0.000000	0.000000	2.000000	49.000000	9.100000	1.000000	1.600000	1.000000
1.000000	1.000000	37.400000	84.000000	36.000000	1.000000	0.000000	3.000000	2.000000	3.000000	3.000000	2.000000	0.000000	0.000000	0.000000	4.000000	5.000000	0.000000	0.000000	3.000000	0.000000	0.000000
2.000000	1.000000	0.000000	0.000000	0.000000	1.000000	1.000000	3.000000	1.000000	1.000000	3.000000	1.000000	0.000000	0.000000	0.000000	2.000000	2.000000	43.000000	7.700000	0.000000	0.000000	1.000000
2.000000	1.000000	38.600000	40.000000	20.000000	0.000000	0.000000	0.000000	1.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	41.000000	6.400000	0.000000	0.000000	1.000000
2.000000	1.000000	40.300000	114.000000	36.000000	3.000000	3.000000	1.000000	2.000000	2.000000	3.000000	3.000000	2.000000	1.000000	7.000000	1.000000	5.000000	57.000000	8.100000	3.000000	4.500000	0.000000
1.000000	9.000000	38.600000	160.000000	20.000000	3.000000	0.000000	5.000000	1.000000	3.000000	3.000000	4.000000	3.000000	0.000000	0.000000	4.000000	0.000000	38.000000	0.000000	2.000000	0.000000	0.000000
1.000000	1.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	24.000000	6.700000	0.000000	0.000000	1.000000
1.000000	1.000000	0.000000	64.000000	36.000000	2.000000	0.000000	2.000000	1.000000	5.000000	3.000000	3.000000	2.000000	2.000000	0.000000	0.000000	0.000000	42.000000	7.700000	0.000000	0.000000	0.000000
1.000000	1.000000	0.000000	0.000000	20.000000	4.000000	3.000000	3.000000	0.000000	5.000000	4.000000	3.000000	2.000000	0.000000	0.000000	4.000000	4.000000	53.000000	5.900000	3.000000	0.000000	0.000000
2.000000	1.000000	0.000000	96.000000	0.000000	3.000000	3.000000	3.000000	2.000000	5.000000	4.000000	4.000000	1.000000	2.000000	0.000000	4.000000	5.000000	60.000000	0.000000	0.000000	0.000000	0.000000
2.000000	1.000000	37.800000	48.000000	32.000000	1.000000	1.000000	3.000000	1.000000	2.000000	1.000000	0.000000	1.000000	1.000000	0.000000	4.000000	5.000000	37.000000	6.700000	0.000000	0.000000	1.000000
2.000000	1.000000	38.500000	60.000000	0.000000	2.000000	2.000000	1.000000	1.000000	1.000000	2.000000	2.000000	2.000000	1.000000	0.000000	1.000000	1.000000	44.000000	7.700000	0.000000	0.000000	1.000000
1.000000	1.000000	37.800000	88.000000	22.000000	2.000000	1.000000	2.000000	1.000000	3.000000	0.000000	0.000000	2.000000	0.000000	0.000000	4.000000	0.000000	64.000000	8.000000	1.000000	6.000000	0.000000
2.000000	1.000000	38.200000	130.000000	16.000000	4.000000	3.000000	4.000000	2.000000	2.000000	4.000000	4.000000	1.000000	1.000000	0.000000	0.000000	0.000000	65.000000	82.000000	2.000000	2.000000	0.000000
1.000000	1.000000	39.000000	64.000000	36.000000	3.000000	1.000000	4.000000	2.000000	3.000000	3.000000	2.000000	1.000000	2.000000	7.000000	4.000000	5.000000	44.000000	7.500000	3.000000	5.000000	1.000000
1.000000	1.000000	0.000000	60.000000	36.000000	3.000000	1.000000	3.000000	1.000000	3.000000	3.000000	2.000000	1.000000	1.000000	0.000000	3.000000	4.000000	26.000000	72.000000	2.000000	1.000000	1.000000
2.000000	1.000000	37.900000	72.000000	0.000000	1.000000	1.000000	5.000000	2.000000	3.000000	3.000000	1.000000	1.000000	3.000000	2.000000	3.000000	4.000000	58.000000	74.000000	1.000000	2.000000	1.000000
2.000000	1.000000	38.400000	54.000000	24.000000	1.000000	1.000000	1.000000	1.000000	1.000000	3.000000	1.000000	2.000000	1.000000	0.000000	3.000000	2.000000	49.000000	7.200000	1.000000	0.000000	1.000000
2.000000	1.000000	0.000000	52.000000	16.000000	1.000000	0.000000	3.000000	1.000000	0.000000	0.000000	0.000000	2.000000	3.000000	5.500000	0.000000	0.000000	55.000000	7.200000	0.000000	0.000000	1.000000
2.000000	1.000000	38.000000	48.000000	12.000000	1.000000	1.000000	1.000000	1.000000	1.000000	3.000000	0.000000	1.000000	1.000000	0.000000	3.000000	2.000000	42.000000	6.300000	2.000000	4.100000	1.000000
2.000000	1.000000	37.000000	60.000000	20.000000	3.000000	0.000000	0.000000	1.000000	3.000000	0.000000	3.000000	2.000000	2.000000	4.500000	4.000000	4.000000	43.000000	7.600000	0.000000	0.000000	0.000000
1.000000	1.000000	37.800000	48.000000	28.000000	1.000000	1.000000	1.000000	1.000000	1.000000	2.000000	1.000000	2.000000	0.000000	0.000000	1.000000	1.000000	46.000000	5.900000	2.000000	7.000000	1.000000
1.000000	1.000000	37.700000	56.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000
1.000000	1.000000	38.100000	52.000000	24.000000	1.000000	1.000000	5.000000	1.000000	4.000000	3.000000	1.000000	2.000000	3.000000	7.000000	1.000000	0.000000	54.000000	7.500000	2.000000	2.600000	0.000000
1.000000	9.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	37.000000	4.900000	0.000000	0.000000	0.000000
1.000000	9.000000	39.700000	100.000000	0.000000	3.000000	3.000000	5.000000	2.000000	2.000000	3.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	48.000000	57.000000	2.000000	2.000000	0.000000
1.000000	1.000000	37.600000	38.000000	20.000000	3.000000	3.000000	1.000000	1.000000	3.000000	3.000000	2.000000	0.000000	0.000000	0.000000	3.000000	0.000000	37.000000	68.000000	0.000000	0.000000	1.000000
2.000000	1.000000	38.700000	52.000000	20.000000	2.000000	0.000000	1.000000	1.000000	1.000000	1.000000	1.000000	1.000000	1.000000	0.000000	1.000000	1.000000	33.000000	77.000000	0.000000	0.000000	1.000000
1.000000	1.000000	0.000000	0.000000	0.000000	3.000000	3.000000	3.000000	3.000000	5.000000	3.000000	3.000000	3.000000	2.000000	0.000000	4.000000	5.000000	46.000000	5.900000	0.000000	0.000000	0.000000
1.000000	1.000000	37.500000	96.000000	18.000000	1.000000	3.000000	6.000000	2.000000	3.000000	4.000000	2.000000	2.000000	3.000000	5.000000	0.000000	4.000000	69.000000	8.900000	3.000000	0.000000	1.000000
1.000000	1.000000	36.400000	98.000000	35.000000	3.000000	3.000000	4.000000	1.000000	4.000000	3.000000	2.000000	0.000000	0.000000	0.000000	4.000000	4.000000	47.000000	6.400000	3.000000	3.600000	0.000000
1.000000	1.000000	37.300000	40.000000	0.000000	0.000000	3.000000	1.000000	1.000000	2.000000	3.000000	2.000000	3.000000	1.000000	0.000000	3.000000	5.000000	36.000000	0.000000	3.000000	2.000000	1.000000
1.000000	9.000000	38.100000	100.000000	80.000000	3.000000	1.000000	2.000000	1.000000	3.000000	4.000000	1.000000	0.000000	0.000000	0.000000	1.000000	0.000000	36.000000	5.700000	0.000000	0.000000	1.000000
1.000000	1.000000	38.000000	0.000000	24.000000	3.000000	3.000000	6.000000	2.000000	5.000000	0.000000	4.000000	1.000000	1.000000	0.000000	0.000000	0.000000	68.000000	7.800000	0.000000	0.000000	0.000000
1.000000	1.000000	37.800000	60.000000	80.000000	1.000000	3.000000	2.000000	2.000000	2.000000	3.000000	3.000000	0.000000	2.000000	5.500000	4.000000	0.000000	40.000000	4.500000	2.000000	0.000000	1.000000
2.000000	1.000000	38.000000	54.000000	30.000000	2.000000	3.000000	3.000000	3.000000	3.000000	1.000000	2.000000	2.000000	2.000000	0.000000	0.000000	4.000000	45.000000	6.200000	0.000000	0.000000	1.000000
1.000000	1.000000	0.000000	88.000000	40.000000	3.000000	3.000000	4.000000	2.000000	5.000000	4.000000	3.000000	3.000000	0.000000	0.000000	4.000000	5.000000	50.000000	7.700000	3.000000	1.400000	0.000000
2.000000	1.000000	0.000000	40.000000	16.000000	0.000000	0.000000	0.000000	1.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	50.000000	7.000000	2.000000	3.900000	0.000000
2.000000	1.000000	39.000000	64.000000	40.000000	1.000000	1.000000	5.000000	1.000000	3.000000	3.000000	2.000000	2.000000	1.000000	0.000000	3.000000	3.000000	42.000000	7.500000	2.000000	2.300000	1.000000
2.000000	1.000000	38.300000	42.000000	10.000000	1.000000	1.000000	1.000000	1.000000	1.000000	1.000000	1.000000	0.000000	0.000000	0.000000	0.000000	0.000000	38.000000	61.000000	0.000000	0.000000	1.000000
2.000000	1.000000	38.000000	52.000000	16.000000	0.000000	0.000000	0.000000	0.000000	2.000000	0.000000	0.000000	0.000000	3.000000	1.000000	1.000000	1.000000	53.000000	86.000000	0.000000	0.000000	1.000000
2.000000	1.000000	40.300000	114.000000	36.000000	3.000000	3.000000	1.000000	2.000000	2.000000	3.000000	3.000000	2.000000	1.000000	7.000000	1.000000	5.000000	57.000000	8.100000	3.000000	4.500000	0.000000
2.000000	1.000000	38.800000	50.000000	20.000000	3.000000	1.000000	1.000000	1.000000	1.000000	1.000000	1.000000	2.000000	1.000000	0.000000	3.000000	1.000000	42.000000	6.200000	0.000000	0.000000	1.000000
2.000000	1.000000	0.000000	0.000000	0.000000	3.000000	3.000000	1.000000	1.000000	5.000000	3.000000	3.000000	1.000000	1.000000	0.000000	4.000000	5.000000	38.000000	6.500000	0.000000	0.000000	0.000000
2.000000	1.000000	37.500000	48.000000	30.000000	4.000000	1.000000	3.000000	1.000000	0.000000	2.000000	1.000000	1.000000	1.000000	0.000000	1.000000	1.000000	48.000000	8.600000	0.000000	0.000000	1.000000
1.000000	1.000000	37.300000	48.000000	20.000000	0.000000	1.000000	2.000000	1.000000	3.000000	3.000000	3.000000	2.000000	1.000000	0.000000	3.000000	5.000000	41.000000	69.000000	0.000000	0.000000	1.000000
2.000000	1.000000	0.000000	84.000000	36.000000	0.000000	0.000000	3.000000	1.000000	0.000000	3.000000	1.000000	2.000000	1.000000	0.000000	3.000000	2.000000	44.000000	8.500000	0.000000	0.000000	1.000000
1.000000	1.000000	38.100000	88.000000	32.000000	3.000000	3.000000	4.000000	1.000000	2.000000	3.000000	3.000000	0.000000	3.000000	1.000000	4.000000	5.000000	55.000000	60.000000	0.000000	0.000000	0.000000
2.000000	1.000000	37.700000	44.000000	40.000000	2.000000	1.000000	3.000000	1.000000	1.000000	3.000000	2.000000	1.000000	1.000000	0.000000	1.000000	5.000000	41.000000	60.000000	0.000000	0.000000	1.000000
2.000000	1.000000	39.600000	108.000000	51.000000	3.000000	3.000000	6.000000	2.000000	2.000000	4.000000	3.000000	1.000000	2.000000	0.000000	3.000000	5.000000	59.000000	8.000000	2.000000	2.600000	1.000000
1.000000	1.000000	38.200000	40.000000	16.000000	3.000000	3.000000	1.000000	1.000000	1.000000	3.000000	0.000000	0.000000	0.000000	0.000000	1.000000	1.000000	34.000000	66.000000	0.000000	0.000000	1.000000
1.000000	1.000000	0.000000	60.000000	20.000000	4.000000	3.000000	4.000000	2.000000	5.000000	4.000000	0.000000	0.000000	1.000000	0.000000	4.000000	5.000000	0.000000	0.000000	0.000000	0.000000	0.000000
2.000000	1.000000	38.300000	40.000000	16.000000	3.000000	0.000000	1.000000	1.000000	2.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	37.000000	57.000000	0.000000	0.000000	1.000000
1.000000	9.000000	38.000000	140.000000	68.000000	1.000000	1.000000	1.000000	1.000000	3.000000	3.000000	2.000000	0.000000	0.000000	0.000000	2.000000	1.000000	39.000000	5.300000	0.000000	0.000000	1.000000
1.000000	1.000000	37.800000	52.000000	24.000000	1.000000	3.000000	3.000000	1.000000	4.000000	4.000000	1.000000	2.000000	3.000000	5.700000	2.000000	5.000000	48.000000	6.600000	1.000000	3.700000	0.000000
1.000000	1.000000	0.000000	70.000000	36.000000	1.000000	0.000000	3.000000	2.000000	2.000000	3.000000	2.000000	2.000000	0.000000	0.000000	4.000000	5.000000	36.000000	7.300000	0.000000	0.000000	1.000000
1.000000	1.000000	38.300000	52.000000	96.000000	0.000000	3.000000	3.000000	1.000000	0.000000	0.000000	0.000000	1.000000	1.000000	0.000000	1.000000	0.000000	43.000000	6.100000	0.000000	0.000000	1.000000
2.000000	1.000000	37.300000	50.000000	32.000000	1.000000	1.000000	3.000000	1.000000	1.000000	3.000000	2.000000	0.000000	0.000000	0.000000	1.000000	0.000000	44.000000	7.000000	0.000000	0.000000	1.000000
1.000000	1.000000	38.700000	60.000000	32.000000	4.000000	3.000000	2.000000	2.000000	4.000000	4.000000	4.000000	0.000000	0.000000	0.000000	4.000000	5.000000	53.000000	64.000000	3.000000	2.000000	0.000000
1.000000	9.000000	38.400000	84.000000	40.000000	3.000000	3.000000	2.000000	1.000000	3.000000	3.000000	3.000000	1.000000	1.000000	0.000000	0.000000	0.000000	36.000000	6.600000	2.000000	2.800000	0.000000
1.000000	1.000000	0.000000	70.000000	16.000000	3.000000	4.000000	5.000000	2.000000	2.000000	3.000000	2.000000	2.000000	1.000000	0.000000	4.000000	5.000000	60.000000	7.500000	0.000000	0.000000	0.000000
1.000000	1.000000	38.300000	40.000000	16.000000	3.000000	0.000000	0.000000	1.000000	1.000000	3.000000	2.000000	0.000000	0.000000	0.000000	0.000000	0.000000	38.000000	58.000000	1.000000	2.000000	1.000000
1.000000	1.000000	0.000000	40.000000	0.000000	2.000000	1.000000	1.000000	1.000000	1.000000	3.000000	1.000000	1.000000	1.000000	0.000000	0.000000	5.000000	39.000000	56.000000	0.000000	0.000000	1.000000
1.000000	1.000000	36.800000	60.000000	28.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	10.000000	0.000000
1.000000	1.000000	38.400000	44.000000	24.000000	3.000000	0.000000	4.000000	0.000000	5.000000	4.000000	3.000000	2.000000	1.000000	0.000000	4.000000	5.000000	50.000000	77.000000	0.000000	0.000000	1.000000
2.000000	1.000000	0.000000	0.000000	40.000000	3.000000	1.000000	1.000000	1.000000	3.000000	3.000000	2.000000	0.000000	0.000000	0.000000	0.000000	0.000000	45.000000	70.000000	0.000000	0.000000	1.000000
1.000000	1.000000	38.000000	44.000000	12.000000	1.000000	1.000000	1.000000	1.000000	3.000000	3.000000	3.000000	2.000000	1.000000	0.000000	4.000000	5.000000	42.000000	65.000000	0.000000	0.000000	1.000000
2.000000	1.000000	39.500000	0.000000	0.000000	3.000000	3.000000	4.000000	2.000000	3.000000	4.000000	3.000000	0.000000	3.000000	5.500000	4.000000	5.000000	0.000000	6.700000	1.000000	0.000000	0.000000
1.000000	1.000000	36.500000	78.000000	30.000000	1.000000	0.000000	1.000000	1.000000	5.000000	3.000000	1.000000	0.000000	1.000000	0.000000	0.000000	0.000000	34.000000	75.000000	2.000000	1.000000	1.000000
2.000000	1.000000	38.100000	56.000000	20.000000	2.000000	1.000000	2.000000	1.000000	1.000000	3.000000	1.000000	1.000000	1.000000	0.000000	0.000000	0.000000	46.000000	70.000000	0.000000	0.000000	1.000000
1.000000	1.000000	39.400000	54.000000	66.000000	1.000000	1.000000	2.000000	1.000000	2.000000	3.000000	2.000000	1.000000	1.000000	0.000000	3.000000	4.000000	39.000000	6.000000	2.000000	0.000000	1.000000
1.000000	1.000000	38.300000	80.000000	40.000000	0.000000	0.000000	6.000000	2.000000	4.000000	3.000000	1.000000	0.000000	2.000000	0.000000	1.000000	4.000000	67.000000	10.200000	2.000000	1.000000	0.000000
2.000000	1.000000	38.700000	40.000000	28.000000	2.000000	1.000000	1.000000	1.000000	3.000000	1.000000	1.000000	0.000000	0.000000	0.000000	1.000000	0.000000	39.000000	62.000000	1.000000	1.000000	1.000000
1.000000	1.000000	38.200000	64.000000	24.000000	1.000000	1.000000	3.000000	1.000000	4.000000	4.000000	3.000000	2.000000	1.000000	0.000000	4.000000	4.000000	45.000000	7.500000	1.000000	2.000000	0.000000
2.000000	1.000000	37.600000	48.000000	20.000000	3.000000	1.000000	4.000000	1.000000	1.000000	1.000000	3.000000	2.000000	1.000000	0.000000	1.000000	1.000000	37.000000	5.500000	0.000000	0.000000	0.000000
1.000000	1.000000	38.000000	42.000000	68.000000	4.000000	1.000000	1.000000	1.000000	3.000000	3.000000	2.000000	2.000000	2.000000	0.000000	4.000000	4.000000	41.000000	7.600000	0.000000	0.000000	1.000000
1.000000	1.000000	38.700000	0.000000	0.000000	3.000000	1.000000	3.000000	1.000000	5.000000	4.000000	2.000000	0.000000	0.000000	0.000000	0.000000	0.000000	33.000000	6.500000	2.000000	0.000000	1.000000
1.000000	1.000000	37.400000	50.000000	32.000000	3.000000	3.000000	0.000000	1.000000	4.000000	4.000000	1.000000	2.000000	1.000000	0.000000	1.000000	0.000000	45.000000	7.900000	2.000000	1.000000	1.000000
1.000000	1.000000	37.400000	84.000000	20.000000	0.000000	0.000000	3.000000	1.000000	2.000000	3.000000	3.000000	0.000000	0.000000	0.000000	0.000000	0.000000	31.000000	61.000000	0.000000	1.000000	0.000000
1.000000	1.000000	38.400000	49.000000	0.000000	0.000000	0.000000	1.000000	1.000000	0.000000	0.000000	1.000000	2.000000	1.000000	0.000000	0.000000	0.000000	44.000000	7.600000	0.000000	0.000000	1.000000
1.000000	1.000000	37.800000	30.000000	12.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000
2.000000	1.000000	37.600000	88.000000	36.000000	3.000000	1.000000	1.000000	1.000000	3.000000	3.000000	2.000000	1.000000	3.000000	1.500000	0.000000	0.000000	44.000000	6.000000	0.000000	0.000000	0.000000
2.000000	1.000000	37.900000	40.000000	24.000000	1.000000	1.000000	1.000000	1.000000	2.000000	3.000000	1.000000	0.000000	0.000000	0.000000	0.000000	3.000000	40.000000	5.700000	0.000000	0.000000	1.000000
1.000000	1.000000	0.000000	100.000000	0.000000	3.000000	0.000000	4.000000	2.000000	5.000000	4.000000	0.000000	2.000000	0.000000	0.000000	2.000000	0.000000	59.000000	6.300000	0.000000	0.000000	0.000000
1.000000	9.000000	38.100000	136.000000	48.000000	3.000000	3.000000	3.000000	1.000000	5.000000	1.000000	3.000000	2.000000	2.000000	4.400000	2.000000	0.000000	33.000000	4.900000	2.000000	2.900000	0.000000
1.000000	1.000000	0.000000	0.000000	0.000000	3.000000	3.000000	3.000000	2.000000	5.000000	3.000000	3.000000	3.000000	2.000000	0.000000	4.000000	5.000000	46.000000	5.900000	0.000000	0.000000	0.000000
1.000000	1.000000	38.000000	48.000000	0.000000	1.000000	1.000000	1.000000	1.000000	1.000000	2.000000	4.000000	2.000000	2.000000	0.000000	4.000000	5.000000	0.000000	0.000000	0.000000	0.000000	1.000000
2.000000	1.000000	38.000000	56.000000	0.000000	1.000000	2.000000	3.000000	1.000000	1.000000	1.000000	1.000000	1.000000	1.000000	0.000000	1.000000	1.000000	42.000000	71.000000	0.000000	0.000000	1.000000
2.000000	1.000000	38.000000	60.000000	32.000000	1.000000	1.000000	0.000000	1.000000	3.000000	3.000000	0.000000	1.000000	1.000000	0.000000	0.000000	0.000000	50.000000	7.000000	1.000000	1.000000	1.000000
1.000000	1.000000	38.100000	44.000000	9.000000	3.000000	1.000000	1.000000	1.000000	2.000000	2.000000	1.000000	1.000000	1.000000	0.000000	4.000000	5.000000	31.000000	7.300000	0.000000	0.000000	1.000000
2.000000	1.000000	36.000000	42.000000	30.000000	0.000000	0.000000	5.000000	1.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	64.000000	6.800000	0.000000	0.000000	0.000000
1.000000	1.000000	0.000000	120.000000	0.000000	4.000000	3.000000	6.000000	2.000000	5.000000	4.000000	4.000000	0.000000	0.000000	0.000000	4.000000	5.000000	57.000000	4.500000	3.000000	3.900000	0.000000
1.000000	1.000000	37.800000	48.000000	28.000000	1.000000	1.000000	1.000000	2.000000	1.000000	2.000000	1.000000	2.000000	0.000000	0.000000	1.000000	1.000000	46.000000	5.900000	2.000000	7.000000	1.000000
1.000000	1.000000	37.100000	84.000000	40.000000	3.000000	3.000000	6.000000	1.000000	2.000000	4.000000	4.000000	3.000000	2.000000	2.000000	4.000000	5.000000	75.000000	81.000000	0.000000	0.000000	0.000000
2.000000	1.000000	0.000000	80.000000	32.000000	3.000000	3.000000	2.000000	1.000000	2.000000	3.000000	3.000000	2.000000	1.000000	0.000000	3.000000	0.000000	50.000000	80.000000	0.000000	0.000000	1.000000
1.000000	1.000000	38.200000	48.000000	0.000000	1.000000	3.000000	3.000000	1.000000	3.000000	4.000000	4.000000	1.000000	3.000000	2.000000	4.000000	5.000000	42.000000	71.000000	0.000000	0.000000	1.000000
2.000000	1.000000	38.000000	44.000000	12.000000	2.000000	1.000000	3.000000	1.000000	3.000000	4.000000	3.000000	1.000000	2.000000	6.500000	1.000000	4.000000	33.000000	6.500000	0.000000	0.000000	0.000000
1.000000	1.000000	38.300000	132.000000	0.000000	0.000000	3.000000	6.000000	2.000000	2.000000	4.000000	2.000000	2.000000	3.000000	6.200000	4.000000	4.000000	57.000000	8.000000	0.000000	5.200000	1.000000
2.000000	1.000000	38.700000	48.000000	24.000000	0.000000	0.000000	0.000000	0.000000	1.000000	1.000000	0.000000	1.000000	1.000000	0.000000	1.000000	0.000000	34.000000	63.000000	0.000000	0.000000	1.000000
2.000000	1.000000	38.900000	44.000000	14.000000	3.000000	1.000000	1.000000	1.000000	2.000000	3.000000	2.000000	0.000000	0.000000	0.000000	0.000000	2.000000	33.000000	64.000000	0.000000	0.000000	1.000000
1.000000	1.000000	39.300000	0.000000	0.000000	4.000000	3.000000	6.000000	2.000000	4.000000	4.000000	2.000000	1.000000	3.000000	4.000000	4.000000	4.000000	75.000000	0.000000	3.000000	4.300000	0.000000
1.000000	1.000000	0.000000	100.000000	0.000000	3.000000	3.000000	4.000000	2.000000	0.000000	4.000000	4.000000	2.000000	1.000000	2.000000	0.000000	0.000000	68.000000	64.000000	3.000000	2.000000	1.000000
2.000000	1.000000	38.600000	48.000000	20.000000	3.000000	1.000000	1.000000	1.000000	1.000000	3.000000	2.000000	2.000000	1.000000	0.000000	3.000000	2.000000	50.000000	7.300000	1.000000	0.000000	1.000000
2.000000	1.000000	38.800000	48.000000	40.000000	1.000000	1.000000	3.000000	1.000000	3.000000	3.000000	4.000000	2.000000	0.000000	0.000000	0.000000	5.000000	41.000000	65.000000	0.000000	0.000000	1.000000
2.000000	1.000000	38.000000	48.000000	20.000000	3.000000	3.000000	4.000000	1.000000	1.000000	4.000000	2.000000	2.000000	0.000000	5.000000	0.000000	2.000000	49.000000	8.300000	1.000000	0.000000	1.000000
2.000000	1.000000	38.600000	52.000000	20.000000	1.000000	1.000000	1.000000	1.000000	3.000000	3.000000	2.000000	1.000000	1.000000	0.000000	1.000000	3.000000	36.000000	6.600000	1.000000	5.000000	1.000000
1.000000	1.000000	37.800000	60.000000	24.000000	1.000000	0.000000	3.000000	2.000000	0.000000	4.000000	4.000000	2.000000	3.000000	2.000000	0.000000	5.000000	52.000000	75.000000	0.000000	0.000000	0.000000
2.000000	1.000000	38.000000	42.000000	40.000000	3.000000	1.000000	1.000000	1.000000	3.000000	3.000000	1.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	1.000000
2.000000	1.000000	0.000000	0.000000	12.000000	1.000000	1.000000	2.000000	1.000000	2.000000	1.000000	2.000000	3.000000	1.000000	0.000000	1.000000	3.000000	44.000000	7.500000	2.000000	0.000000	1.000000
1.000000	1.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	4.000000	0.000000	0.000000	1.000000	1.000000	0.000000	0.000000	5.000000	35.000000	58.000000	2.000000	1.000000	1.000000
1.000000	1.000000	38.300000	42.000000	24.000000	0.000000	0.000000	0.000000	1.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	40.000000	8.500000	0.000000	0.000000	0.000000
2.000000	1.000000	39.500000	60.000000	10.000000	3.000000	0.000000	0.000000	2.000000	3.000000	3.000000	2.000000	2.000000	1.000000	0.000000	3.000000	0.000000	38.000000	56.000000	1.000000	0.000000	1.000000
1.000000	1.000000	38.000000	66.000000	20.000000	1.000000	3.000000	3.000000	1.000000	5.000000	3.000000	1.000000	1.000000	1.000000	0.000000	3.000000	0.000000	46.000000	46.000000	3.000000	2.000000	0.000000
1.000000	1.000000	38.700000	76.000000	0.000000	1.000000	1.000000	5.000000	2.000000	3.000000	3.000000	2.000000	2.000000	2.000000	0.000000	4.000000	4.000000	50.000000	8.000000	0.000000	0.000000	1.000000
1.000000	1.000000	39.400000	120.000000	48.000000	0.000000	0.000000	5.000000	1.000000	0.000000	3.000000	3.000000	1.000000	0.000000	0.000000	4.000000	0.000000	56.000000	64.000000	1.000000	2.000000	0.000000
1.000000	1.000000	38.300000	40.000000	18.000000	1.000000	1.000000	1.000000	1.000000	3.000000	1.000000	1.000000	0.000000	0.000000	0.000000	2.000000	1.000000	43.000000	5.900000	1.000000	0.000000	1.000000
2.000000	1.000000	0.000000	44.000000	24.000000	1.000000	1.000000	1.000000	1.000000	3.000000	3.000000	1.000000	2.000000	1.000000	0.000000	0.000000	1.000000	0.000000	6.300000	0.000000	0.000000	1.000000
1.000000	1.000000	38.400000	104.000000	40.000000	1.000000	1.000000	3.000000	1.000000	2.000000	4.000000	2.000000	2.000000	3.000000	6.500000	0.000000	4.000000	55.000000	8.500000	0.000000	0.000000	1.000000
1.000000	1.000000	0.000000	65.000000	24.000000	0.000000	0.000000	0.000000	2.000000	5.000000	0.000000	4.000000	3.000000	1.000000	0.000000	0.000000	5.000000	0.000000	0.000000	0.000000	0.000000	0.000000
2.000000	1.000000	37.500000	44.000000	20.000000	1.000000	1.000000	3.000000	1.000000	0.000000	1.000000	1.000000	0.000000	0.000000	0.000000	1.000000	0.000000	35.000000	7.200000	0.000000	0.000000	1.000000
2.000000	1.000000	39.000000	86.000000	16.000000	3.000000	3.000000	5.000000	0.000000	3.000000	3.000000	3.000000	0.000000	2.000000	0.000000	0.000000	0.000000	68.000000	5.800000	3.000000	6.000000	0.000000
1.000000	1.000000	38.500000	129.000000	48.000000	3.000000	3.000000	3.000000	1.000000	2.000000	4.000000	3.000000	1.000000	3.000000	2.000000	0.000000	0.000000	57.000000	66.000000	3.000000	2.000000	1.000000
1.000000	1.000000	0.000000	104.000000	0.000000	3.000000	3.000000	5.000000	2.000000	2.000000	4.000000	3.000000	0.000000	3.000000	0.000000	4.000000	4.000000	69.000000	8.600000	2.000000	3.400000	0.000000
2.000000	1.000000	0.000000	0.000000	0.000000	3.000000	4.000000	6.000000	0.000000	4.000000	0.000000	4.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000
1.000000	1.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	1.000000
1.000000	1.000000	38.200000	60.000000	30.000000	1.000000	1.000000	3.000000	1.000000	3.000000	3.000000	1.000000	2.000000	1.000000	0.000000	3.000000	2.000000	48.000000	66.000000	0.000000	0.000000	1.000000
1.000000	1.000000	0.000000	68.000000	14.000000	0.000000	0.000000	4.000000	1.000000	4.000000	0.000000	0.000000	0.000000	1.000000	4.300000	0.000000	0.000000	0.000000	0.000000	2.000000	2.800000	0.000000
1.000000	1.000000	0.000000	60.000000	30.000000	3.000000	3.000000	4.000000	2.000000	5.000000	4.000000	4.000000	1.000000	1.000000	0.000000	4.000000	0.000000	45.000000	70.000000	3.000000	2.000000	1.000000
2.000000	1.000000	38.500000	100.000000	0.000000	3.000000	3.000000	5.000000	2.000000	4.000000	3.000000	4.000000	2.000000	1.000000	0.000000	4.000000	5.000000	0.000000	0.000000	0.000000	0.000000	0.000000
1.000000	1.000000	38.400000	84.000000	30.000000	3.000000	1.000000	5.000000	2.000000	4.000000	3.000000	3.000000	2.000000	3.000000	6.500000	4.000000	4.000000	47.000000	7.500000	3.000000	0.000000	0.000000
2.000000	1.000000	37.800000	48.000000	14.000000	0.000000	0.000000	1.000000	1.000000	3.000000	0.000000	2.000000	1.000000	3.000000	5.300000	1.000000	0.000000	35.000000	7.500000	0.000000	0.000000	1.000000
1.000000	1.000000	38.000000	0.000000	24.000000	3.000000	3.000000	6.000000	2.000000	5.000000	0.000000	4.000000	1.000000	1.000000	0.000000	0.000000	0.000000	68.000000	7.800000	0.000000	0.000000	0.000000
2.000000	1.000000	37.800000	56.000000	16.000000	1.000000	1.000000	2.000000	1.000000	2.000000	1.000000	1.000000	2.000000	1.000000	0.000000	1.000000	0.000000	44.000000	68.000000	1.000000	1.000000	1.000000
2.000000	1.000000	38.200000	68.000000	32.000000	2.000000	2.000000	2.000000	1.000000	1.000000	1.000000	1.000000	3.000000	1.000000	0.000000	1.000000	1.000000	43.000000	65.000000	0.000000	0.000000	1.000000
1.000000	1.000000	38.500000	120.000000	60.000000	4.000000	3.000000	6.000000	2.000000	0.000000	3.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	54.000000	0.000000	0.000000	0.000000	1.000000
1.000000	1.000000	39.300000	64.000000	90.000000	2.000000	3.000000	1.000000	1.000000	0.000000	3.000000	1.000000	1.000000	2.000000	0.000000	0.000000	0.000000	39.000000	6.700000	0.000000	0.000000	1.000000
1.000000	1.000000	38.400000	80.000000	30.000000	4.000000	3.000000	1.000000	1.000000	3.000000	3.000000	3.000000	3.000000	3.000000	0.000000	4.000000	5.000000	32.000000	6.100000	3.000000	4.300000	1.000000
1.000000	1.000000	38.500000	60.000000	0.000000	1.000000	1.000000	0.000000	1.000000	0.000000	1.000000	1.000000	0.000000	0.000000	0.000000	0.000000	0.000000	33.000000	53.000000	1.000000	0.000000	1.000000
1.000000	1.000000	38.300000	60.000000	16.000000	3.000000	1.000000	1.000000	1.000000	2.000000	1.000000	1.000000	2.000000	2.000000	3.000000	1.000000	4.000000	30.000000	6.000000	1.000000	3.000000	1.000000
1.000000	1.000000	37.100000	40.000000	8.000000	0.000000	1.000000	4.000000	1.000000	3.000000	3.000000	1.000000	1.000000	1.000000	0.000000	3.000000	3.000000	23.000000	6.700000	3.000000	0.000000	1.000000
2.000000	9.000000	0.000000	100.000000	44.000000	2.000000	1.000000	1.000000	1.000000	4.000000	1.000000	1.000000	0.000000	0.000000	0.000000	1.000000	0.000000	37.000000	4.700000	0.000000	0.000000	1.000000
1.000000	1.000000	38.200000	48.000000	18.000000	1.000000	1.000000	1.000000	1.000000	3.000000	3.000000	3.000000	1.000000	2.000000	0.000000	4.000000	0.000000	48.000000	74.000000	1.000000	2.000000	1.000000
1.000000	1.000000	0.000000	60.000000	48.000000	3.000000	3.000000	4.000000	2.000000	4.000000	3.000000	4.000000	0.000000	0.000000	0.000000	0.000000	0.000000	58.000000	7.600000	0.000000	0.000000	0.000000
2.000000	1.000000	37.900000	88.000000	24.000000	1.000000	1.000000	2.000000	1.000000	2.000000	2.000000	1.000000	0.000000	0.000000	0.000000	4.000000	1.000000	37.000000	56.000000	0.000000	0.000000	1.000000
2.000000	1.000000	38.000000	44.000000	12.000000	3.000000	1.000000	1.000000	0.000000	0.000000	1.000000	2.000000	0.000000	0.000000	0.000000	1.000000	0.000000	42.000000	64.000000	0.000000	0.000000	1.000000
2.000000	1.000000	38.500000	60.000000	20.000000	1.000000	1.000000	5.000000	2.000000	2.000000	2.000000	1.000000	2.000000	1.000000	0.000000	2.000000	3.000000	63.000000	7.500000	2.000000	2.300000	0.000000
2.000000	1.000000	38.500000	96.000000	36.000000	3.000000	3.000000	0.000000	2.000000	2.000000	4.000000	2.000000	1.000000	2.000000	0.000000	4.000000	5.000000	70.000000	8.500000	0.000000	0.000000	0.000000
2.000000	1.000000	38.300000	60.000000	20.000000	1.000000	1.000000	1.000000	2.000000	1.000000	3.000000	1.000000	0.000000	0.000000	0.000000	3.000000	0.000000	34.000000	66.000000	0.000000	0.000000	1.000000
2.000000	1.000000	38.500000	60.000000	40.000000	3.000000	1.000000	2.000000	1.000000	2.000000	1.000000	2.000000	0.000000	0.000000	0.000000	3.000000	2.000000	49.000000	59.000000	0.000000	0.000000	1.000000
1.000000	1.000000	37.300000	48.000000	12.000000	1.000000	0.000000	3.000000	1.000000	3.000000	1.000000	3.000000	2.000000	1.000000	0.000000	3.000000	3.000000	40.000000	6.600000	2.000000	0.000000	1.000000
1.000000	1.000000	38.500000	86.000000	0.000000	1.000000	1.000000	3.000000	1.000000	4.000000	4.000000	3.000000	2.000000	1.000000	0.000000	3.000000	5.000000	45.000000	7.400000	1.000000	3.400000	0.000000
1.000000	1.000000	37.500000	48.000000	40.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	1.000000	1.000000	0.000000	0.000000	5.000000	41.000000	55.000000	3.000000	2.000000	0.000000
2.000000	1.000000	37.200000	36.000000	9.000000	1.000000	1.000000	1.000000	1.000000	2.000000	3.000000	1.000000	2.000000	1.000000	0.000000	4.000000	1.000000	35.000000	5.700000	0.000000	0.000000	1.000000
1.000000	1.000000	39.200000	0.000000	23.000000	3.000000	1.000000	3.000000	1.000000	4.000000	4.000000	2.000000	2.000000	0.000000	0.000000	0.000000	0.000000	36.000000	6.600000	1.000000	3.000000	1.000000
2.000000	1.000000	38.500000	100.000000	0.000000	3.000000	3.000000	5.000000	2.000000	4.000000	3.000000	4.000000	2.000000	1.000000	0.000000	4.000000	5.000000	0.000000	0.000000	0.000000	0.000000	0.000000
1.000000	1.000000	38.500000	96.000000	30.000000	2.000000	3.000000	4.000000	2.000000	4.000000	4.000000	3.000000	2.000000	1.000000	0.000000	3.000000	5.000000	50.000000	65.000000	0.000000	0.000000	1.000000
1.000000	1.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	45.000000	8.700000	0.000000	0.000000	0.000000
1.000000	1.000000	37.800000	88.000000	80.000000	3.000000	3.000000	5.000000	2.000000	0.000000	3.000000	3.000000	2.000000	3.000000	0.000000	4.000000	5.000000	64.000000	89.000000	0.000000	0.000000	0.000000
2.000000	1.000000	37.500000	44.000000	10.000000	3.000000	1.000000	1.000000	1.000000	3.000000	1.000000	2.000000	2.000000	0.000000	0.000000	3.000000	3.000000	43.000000	51.000000	1.000000	1.000000	1.000000
1.000000	1.000000	37.900000	68.000000	20.000000	0.000000	1.000000	2.000000	1.000000	2.000000	4.000000	2.000000	0.000000	0.000000	0.000000	1.000000	5.000000	45.000000	4.000000	3.000000	2.800000	0.000000
1.000000	1.000000	38.000000	86.000000	24.000000	4.000000	3.000000	4.000000	1.000000	2.000000	4.000000	4.000000	1.000000	1.000000	0.000000	4.000000	5.000000	45.000000	5.500000	1.000000	10.100000	0.000000
1.000000	9.000000	38.900000	120.000000	30.000000	1.000000	3.000000	2.000000	2.000000	3.000000	3.000000	3.000000	3.000000	1.000000	3.000000	0.000000	0.000000	47.000000	6.300000	1.000000	0.000000	1.000000
1.000000	1.000000	37.600000	45.000000	12.000000	3.000000	1.000000	3.000000	1.000000	0.000000	2.000000	2.000000	2.000000	1.000000	0.000000	1.000000	4.000000	39.000000	7.000000	2.000000	1.500000	1.000000
2.000000	1.000000	38.600000	56.000000	32.000000	2.000000	1.000000	1.000000	1.000000	1.000000	1.000000	1.000000	2.000000	0.000000	0.000000	2.000000	0.000000	40.000000	7.000000	2.000000	2.100000	1.000000
1.000000	1.000000	37.800000	40.000000	12.000000	1.000000	1.000000	1.000000	1.000000	1.000000	2.000000	1.000000	2.000000	1.000000	0.000000	1.000000	2.000000	38.000000	7.000000	0.000000	0.000000	1.000000
2.000000	1.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	1.000000
1.000000	1.000000	38.000000	76.000000	18.000000	0.000000	0.000000	0.000000	2.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	71.000000	11.000000	0.000000	0.000000	1.000000
1.000000	1.000000	38.100000	40.000000	36.000000	1.000000	2.000000	2.000000	1.000000	2.000000	2.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000
1.000000	1.000000	0.000000	52.000000	28.000000	3.000000	3.000000	4.000000	1.000000	3.000000	4.000000	3.000000	2.000000	1.000000	0.000000	4.000000	4.000000	37.000000	8.100000	0.000000	0.000000	1.000000
1.000000	1.000000	39.200000	88.000000	58.000000	4.000000	4.000000	0.000000	2.000000	5.000000	4.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	2.000000	2.000000	0.000000
1.000000	1.000000	38.500000	92.000000	40.000000	4.000000	3.000000	0.000000	1.000000	2.000000	4.000000	3.000000	0.000000	0.000000	0.000000	4.000000	0.000000	46.000000	67.000000	2.000000	2.000000	1.000000
1.000000	1.000000	0.000000	112.000000	13.000000	4.000000	4.000000	4.000000	1.000000	2.000000	3.000000	1.000000	2.000000	1.000000	4.500000	4.000000	4.000000	60.000000	6.300000	3.000000	0.000000	1.000000
1.000000	1.000000	37.700000	66.000000	12.000000	1.000000	1.000000	3.000000	1.000000	3.000000	3.000000	2.000000	2.000000	0.000000	0.000000	4.000000	4.000000	31.500000	6.200000	2.000000	1.600000	1.000000
1.000000	1.000000	38.800000	50.000000	14.000000	1.000000	1.000000	1.000000	1.000000	3.000000	1.000000	1.000000	1.000000	1.000000	0.000000	3.000000	5.000000	38.000000	58.000000	0.000000	0.000000	1.000000
2.000000	1.000000	38.400000	54.000000	24.000000	1.000000	1.000000	1.000000	1.000000	1.000000	3.000000	1.000000	2.000000	1.000000	0.000000	3.000000	2.000000	49.000000	7.200000	1.000000	8.000000	1.000000
1.000000	1.000000	39.200000	120.000000	20.000000	4.000000	3.000000	5.000000	2.000000	2.000000	3.000000	3.000000	1.000000	3.000000	0.000000	0.000000	4.000000	60.000000	8.800000	3.000000	0.000000	0.000000
1.000000	9.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	45.000000	6.500000	2.000000	0.000000	1.000000
1.000000	1.000000	37.300000	90.000000	40.000000	3.000000	0.000000	6.000000	2.000000	5.000000	4.000000	3.000000	2.000000	2.000000	0.000000	1.000000	5.000000	65.000000	50.000000	3.000000	2.000000	0.000000
1.000000	9.000000	38.500000	120.000000	70.000000	0.000000	0.000000	0.000000	0.000000	0.000000	1.000000	0.000000	2.000000	0.000000	0.000000	1.000000	0.000000	35.000000	54.000000	1.000000	1.000000	1.000000
1.000000	1.000000	38.500000	104.000000	40.000000	3.000000	3.000000	0.000000	1.000000	4.000000	3.000000	4.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	1.000000
2.000000	1.000000	39.500000	92.000000	28.000000	3.000000	3.000000	6.000000	1.000000	5.000000	4.000000	1.000000	0.000000	3.000000	0.000000	4.000000	0.000000	72.000000	6.400000	0.000000	3.600000	0.000000
1.000000	1.000000	38.500000	30.000000	18.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	40.000000	7.700000	0.000000	0.000000	1.000000
1.000000	1.000000	38.300000	72.000000	30.000000	4.000000	3.000000	3.000000	2.000000	3.000000	3.000000	3.000000	2.000000	1.000000	0.000000	3.000000	5.000000	43.000000	7.000000	2.000000	3.900000	1.000000
2.000000	1.000000	37.500000	48.000000	30.000000	4.000000	1.000000	3.000000	1.000000	0.000000	2.000000	1.000000	1.000000	1.000000	0.000000	1.000000	1.000000	48.000000	8.600000	0.000000	0.000000	1.000000
1.000000	1.000000	38.100000	52.000000	24.000000	1.000000	1.000000	5.000000	1.000000	4.000000	3.000000	1.000000	2.000000	3.000000	7.000000	1.000000	0.000000	54.000000	7.500000	2.000000	2.600000	0.000000
2.000000	1.000000	38.200000	42.000000	26.000000	1.000000	1.000000	1.000000	1.000000	3.000000	1.000000	2.000000	0.000000	0.000000	0.000000	1.000000	0.000000	36.000000	6.900000	0.000000	0.000000	1.000000
2.000000	1.000000	37.900000	54.000000	42.000000	2.000000	1.000000	5.000000	1.000000	3.000000	1.000000	1.000000	0.000000	1.000000	0.000000	0.000000	2.000000	47.000000	54.000000	3.000000	1.000000	1.000000
2.000000	1.000000	36.100000	88.000000	0.000000	3.000000	3.000000	3.000000	1.000000	3.000000	3.000000	2.000000	2.000000	3.000000	0.000000	0.000000	4.000000	45.000000	7.000000	3.000000	4.800000	0.000000
1.000000	1.000000	38.100000	70.000000	22.000000	0.000000	1.000000	0.000000	1.000000	5.000000	3.000000	0.000000	0.000000	0.000000	0.000000	0.000000	5.000000	36.000000	65.000000	0.000000	0.000000	0.000000
1.000000	1.000000	38.000000	90.000000	30.000000	4.000000	3.000000	4.000000	2.000000	5.000000	4.000000	4.000000	0.000000	0.000000	0.000000	4.000000	5.000000	55.000000	6.100000	0.000000	0.000000	0.000000
1.000000	1.000000	38.200000	52.000000	16.000000	1.000000	1.000000	2.000000	1.000000	1.000000	2.000000	1.000000	1.000000	1.000000	0.000000	1.000000	0.000000	43.000000	8.100000	0.000000	0.000000	1.000000
1.000000	1.000000	0.000000	36.000000	32.000000	1.000000	1.000000	4.000000	1.000000	5.000000	3.000000	3.000000	2.000000	3.000000	4.000000	0.000000	4.000000	41.000000	5.900000	0.000000	0.000000	0.000000
1.000000	1.000000	38.400000	92.000000	20.000000	1.000000	0.000000	0.000000	2.000000	0.000000	3.000000	3.000000	0.000000	0.000000	0.000000	1.000000	0.000000	0.000000	0.000000	0.000000	0.000000	1.000000
1.000000	9.000000	38.200000	124.000000	88.000000	1.000000	3.000000	2.000000	1.000000	2.000000	3.000000	4.000000	0.000000	0.000000	0.000000	0.000000	0.000000	47.000000	8.000000	1.000000	0.000000	1.000000
2.000000	1.000000	0.000000	96.000000	0.000000	3.000000	3.000000	3.000000	2.000000	5.000000	4.000000	4.000000	0.000000	1.000000	0.000000	4.000000	5.000000	60.000000	0.000000	0.000000	0.000000	0.000000
1.000000	1.000000	37.600000	68.000000	32.000000	3.000000	0.000000	3.000000	1.000000	4.000000	2.000000	4.000000	2.000000	2.000000	6.500000	1.000000	5.000000	47.000000	7.200000	1.000000	0.000000	1.000000
1.000000	1.000000	38.100000	88.000000	24.000000	3.000000	3.000000	4.000000	1.000000	5.000000	4.000000	3.000000	2.000000	1.000000	0.000000	3.000000	4.000000	41.000000	4.600000	0.000000	0.000000	0.000000
1.000000	1.000000	38.000000	108.000000	60.000000	2.000000	3.000000	4.000000	1.000000	4.000000	3.000000	3.000000	2.000000	0.000000	0.000000	3.000000	4.000000	0.000000	0.000000	3.000000	0.000000	1.000000
2.000000	1.000000	38.200000	48.000000	0.000000	2.000000	0.000000	1.000000	2.000000	3.000000	3.000000	1.000000	2.000000	1.000000	0.000000	0.000000	2.000000	34.000000	6.600000	0.000000	0.000000	1.000000
1.000000	1.000000	39.300000	100.000000	51.000000	4.000000	4.000000	6.000000	1.000000	2.000000	4.000000	1.000000	1.000000	3.000000	2.000000	0.000000	4.000000	66.000000	13.000000	3.000000	2.000000	0.000000
2.000000	1.000000	36.600000	42.000000	18.000000	3.000000	3.000000	2.000000	1.000000	1.000000	4.000000	1.000000	1.000000	1.000000	0.000000	0.000000	5.000000	52.000000	7.100000	0.000000	0.000000	0.000000
1.000000	9.000000	38.800000	124.000000	36.000000	3.000000	1.000000	2.000000	1.000000	2.000000	3.000000	4.000000	1.000000	1.000000	0.000000	4.000000	4.000000	50.000000	7.600000	3.000000	0.000000	0.000000
2.000000	1.000000	0.000000	112.000000	24.000000	3.000000	3.000000	4.000000	2.000000	5.000000	4.000000	2.000000	0.000000	0.000000	0.000000	4.000000	0.000000	40.000000	5.300000	3.000000	2.600000	1.000000
1.000000	1.000000	0.000000	80.000000	0.000000	3.000000	3.000000	3.000000	1.000000	4.000000	4.000000	4.000000	0.000000	0.000000	0.000000	4.000000	5.000000	43.000000	70.000000	0.000000	0.000000	1.000000
1.000000	9.000000	38.800000	184.000000	84.000000	1.000000	0.000000	1.000000	1.000000	4.000000	1.000000	3.000000	0.000000	0.000000	0.000000	2.000000	0.000000	33.000000	3.300000	0.000000	0.000000	0.000000
1.000000	1.000000	37.500000	72.000000	0.000000	2.000000	1.000000	1.000000	1.000000	2.000000	1.000000	1.000000	1.000000	1.000000	0.000000	1.000000	0.000000	35.000000	65.000000	2.000000	2.000000	0.000000
1.000000	1.000000	38.700000	96.000000	28.000000	3.000000	3.000000	4.000000	1.000000	0.000000	4.000000	0.000000	0.000000	3.000000	7.500000	0.000000	0.000000	64.000000	9.000000	0.000000	0.000000	0.000000
2.000000	1.000000	37.500000	52.000000	12.000000	1.000000	1.000000	1.000000	1.000000	2.000000	3.000000	2.000000	2.000000	1.000000	0.000000	3.000000	5.000000	36.000000	61.000000	1.000000	1.000000	1.000000
1.000000	1.000000	40.800000	72.000000	42.000000	3.000000	3.000000	1.000000	1.000000	2.000000	3.000000	1.000000	2.000000	1.000000	0.000000	0.000000	0.000000	54.000000	7.400000	3.000000	0.000000	0.000000
2.000000	1.000000	38.000000	40.000000	25.000000	0.000000	1.000000	1.000000	1.000000	4.000000	3.000000	2.000000	1.000000	1.000000	0.000000	4.000000	0.000000	37.000000	69.000000	0.000000	0.000000	1.000000
2.000000	1.000000	38.400000	48.000000	16.000000	2.000000	1.000000	1.000000	1.000000	1.000000	0.000000	2.000000	2.000000	1.000000	0.000000	0.000000	2.000000	39.000000	6.500000	0.000000	0.000000	1.000000
2.000000	9.000000	38.600000	88.000000	28.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	35.000000	5.900000	0.000000	0.000000	1.000000
1.000000	1.000000	37.100000	75.000000	36.000000	0.000000	0.000000	3.000000	2.000000	4.000000	4.000000	2.000000	2.000000	3.000000	5.000000	4.000000	4.000000	48.000000	7.400000	3.000000	3.200000	0.000000
1.000000	1.000000	38.300000	44.000000	21.000000	3.000000	1.000000	2.000000	1.000000	3.000000	3.000000	3.000000	2.000000	1.000000	0.000000	1.000000	5.000000	44.000000	6.500000	2.000000	4.400000	1.000000
2.000000	1.000000	0.000000	56.000000	68.000000	3.000000	1.000000	1.000000	1.000000	3.000000	3.000000	1.000000	2.000000	1.000000	0.000000	1.000000	0.000000	40.000000	6.000000	0.000000	0.000000	0.000000
2.000000	1.000000	38.600000	68.000000	20.000000	2.000000	1.000000	3.000000	1.000000	3.000000	3.000000	2.000000	1.000000	1.000000	0.000000	1.000000	5.000000	38.000000	6.500000	1.000000	0.000000	1.000000
2.000000	1.000000	38.300000	54.000000	18.000000	3.000000	1.000000	2.000000	1.000000	2.000000	3.000000	2.000000	0.000000	3.000000	5.400000	0.000000	4.000000	44.000000	7.200000	3.000000	0.000000	1.000000
1.000000	1.000000	38.200000	42.000000	20.000000	0.000000	0.000000	1.000000	1.000000	0.000000	3.000000	0.000000	0.000000	0.000000	0.000000	3.000000	0.000000	47.000000	60.000000	0.000000	0.000000	1.000000
1.000000	1.000000	39.300000	64.000000	90.000000	2.000000	3.000000	1.000000	1.000000	0.000000	3.000000	1.000000	1.000000	2.000000	6.500000	1.000000	5.000000	39.000000	6.700000	0.000000	0.000000	1.000000
1.000000	1.000000	37.500000	60.000000	50.000000	3.000000	3.000000	1.000000	1.000000	3.000000	3.000000	2.000000	2.000000	2.000000	3.500000	3.000000	4.000000	35.000000	6.500000	0.000000	0.000000	0.000000
1.000000	1.000000	37.700000	80.000000	0.000000	3.000000	3.000000	6.000000	1.000000	5.000000	4.000000	1.000000	2.000000	3.000000	0.000000	3.000000	1.000000	50.000000	55.000000	3.000000	2.000000	1.000000
1.000000	1.000000	0.000000	100.000000	30.000000	3.000000	3.000000	4.000000	2.000000	5.000000	4.000000	4.000000	3.000000	3.000000	0.000000	4.000000	4.000000	52.000000	6.600000	0.000000	0.000000	1.000000
1.000000	1.000000	37.700000	120.000000	28.000000	3.000000	3.000000	3.000000	1.000000	5.000000	3.000000	3.000000	1.000000	1.000000	0.000000	0.000000	0.000000	65.000000	7.000000	3.000000	0.000000	0.000000
1.000000	1.000000	0.000000	76.000000	0.000000	0.000000	3.000000	0.000000	0.000000	0.000000	4.000000	4.000000	0.000000	0.000000	0.000000	0.000000	5.000000	0.000000	0.000000	0.000000	0.000000	0.000000
1.000000	9.000000	38.800000	150.000000	50.000000	1.000000	3.000000	6.000000	2.000000	5.000000	3.000000	2.000000	1.000000	1.000000	0.000000	0.000000	0.000000	50.000000	6.200000	0.000000	0.000000	0.000000
1.000000	1.000000	38.000000	36.000000	16.000000	3.000000	1.000000	1.000000	1.000000	4.000000	2.000000	2.000000	3.000000	3.000000	2.000000	3.000000	0.000000	37.000000	75.000000	2.000000	1.000000	0.000000
2.000000	1.000000	36.900000	50.000000	40.000000	2.000000	3.000000	3.000000	1.000000	1.000000	3.000000	2.000000	3.000000	1.000000	7.000000	0.000000	0.000000	37.500000	6.500000	0.000000	0.000000	1.000000
2.000000	1.000000	37.800000	40.000000	16.000000	1.000000	1.000000	1.000000	1.000000	1.000000	1.000000	1.000000	0.000000	0.000000	0.000000	1.000000	1.000000	37.000000	6.800000	0.000000	0.000000	1.000000
2.000000	1.000000	38.200000	56.000000	40.000000	4.000000	3.000000	1.000000	1.000000	2.000000	4.000000	3.000000	2.000000	2.000000	7.500000	0.000000	0.000000	47.000000	7.200000	1.000000	2.500000	1.000000
1.000000	1.000000	38.600000	48.000000	12.000000	0.000000	0.000000	1.000000	0.000000	1.000000	1.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	36.000000	67.000000	0.000000	0.000000	1.000000
2.000000	1.000000	40.000000	78.000000	0.000000	3.000000	3.000000	5.000000	1.000000	2.000000	3.000000	1.000000	1.000000	1.000000	0.000000	4.000000	1.000000	66.000000	6.500000	0.000000	0.000000	0.000000
1.000000	1.000000	0.000000	70.000000	16.000000	3.000000	4.000000	5.000000	2.000000	2.000000	3.000000	2.000000	2.000000	1.000000	0.000000	4.000000	5.000000	60.000000	7.500000	0.000000	0.000000	0.000000
1.000000	1.000000	38.200000	72.000000	18.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	35.000000	6.400000	0.000000	0.000000	1.000000
2.000000	1.000000	38.500000	54.000000	0.000000	1.000000	1.000000	1.000000	1.000000	3.000000	1.000000	1.000000	2.000000	1.000000	0.000000	1.000000	0.000000	40.000000	6.800000	2.000000	7.000000	1.000000
1.000000	1.000000	38.500000	66.000000	24.000000	1.000000	1.000000	1.000000	1.000000	3.000000	3.000000	1.000000	2.000000	1.000000	0.000000	4.000000	5.000000	40.000000	6.700000	1.000000	0.000000	1.000000
2.000000	1.000000	37.800000	82.000000	12.000000	3.000000	1.000000	1.000000	2.000000	4.000000	0.000000	3.000000	1.000000	3.000000	0.000000	0.000000	0.000000	50.000000	7.000000	0.000000	0.000000	0.000000
2.000000	9.000000	39.500000	84.000000	30.000000	0.000000	0.000000	0.000000	1.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	28.000000	5.000000	0.000000	0.000000	1.000000
1.000000	1.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	1.000000
1.000000	1.000000	38.000000	50.000000	36.000000	0.000000	1.000000	1.000000	1.000000	3.000000	2.000000	2.000000	0.000000	0.000000	0.000000	3.000000	0.000000	39.000000	6.600000	1.000000	5.300000	1.000000
2.000000	1.000000	38.600000	45.000000	16.000000	2.000000	1.000000	2.000000	1.000000	1.000000	1.000000	0.000000	0.000000	0.000000	0.000000	1.000000	1.000000	43.000000	58.000000	0.000000	0.000000	1.000000
1.000000	1.000000	38.900000	80.000000	44.000000	3.000000	3.000000	3.000000	1.000000	2.000000	3.000000	3.000000	2.000000	2.000000	7.000000	3.000000	1.000000	54.000000	6.500000	3.000000	0.000000	0.000000
1.000000	1.000000	37.000000	66.000000	20.000000	1.000000	3.000000	2.000000	1.000000	4.000000	3.000000	3.000000	1.000000	0.000000	0.000000	1.000000	5.000000	35.000000	6.900000	2.000000	0.000000	0.000000
1.000000	1.000000	0.000000	78.000000	24.000000	3.000000	3.000000	3.000000	1.000000	0.000000	3.000000	0.000000	2.000000	1.000000	0.000000	0.000000	4.000000	43.000000	62.000000	0.000000	2.000000	0.000000
2.000000	1.000000	38.500000	40.000000	16.000000	1.000000	1.000000	1.000000	1.000000	2.000000	1.000000	1.000000	0.000000	0.000000	0.000000	3.000000	2.000000	37.000000	67.000000	0.000000	0.000000	1.000000
1.000000	1.000000	0.000000	120.000000	70.000000	4.000000	0.000000	4.000000	2.000000	2.000000	4.000000	0.000000	0.000000	0.000000	0.000000	0.000000	5.000000	55.000000	65.000000	0.000000	0.000000	0.000000
2.000000	1.000000	37.200000	72.000000	24.000000	3.000000	2.000000	4.000000	2.000000	4.000000	3.000000	3.000000	3.000000	1.000000	0.000000	4.000000	4.000000	44.000000	0.000000	3.000000	3.300000	0.000000
1.000000	1.000000	37.500000	72.000000	30.000000	4.000000	3.000000	4.000000	1.000000	4.000000	4.000000	3.000000	2.000000	1.000000	0.000000	3.000000	5.000000	60.000000	6.800000	0.000000	0.000000	0.000000
1.000000	1.000000	36.500000	100.000000	24.000000	3.000000	3.000000	3.000000	1.000000	3.000000	3.000000	3.000000	3.000000	1.000000	0.000000	4.000000	4.000000	50.000000	6.000000	3.000000	3.400000	1.000000
1.000000	1.000000	37.200000	40.000000	20.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	0.000000	4.000000	1.000000	36.000000	62.000000	1.000000	1.000000	0.000000


　　
 完整代码：

#-*_coding:utf-8_*_
import math
from numpy import *

# Sigmoid函数的计算
def sigmoid(inX):
    return 1.0/(1+exp(-inX))

# 改进的随机梯度上升算法
def stocGradAscent1(dataMatrix,classLabels,numIter=150):
    m,n = shape(dataMatrix)
    weights = ones(n)
    for j in range(numIter):
        dataIndex = list(range(m))
        for i in range(m):
            alpha = 4/(1.0+j+i)+0.01
            randIndex = int(random.uniform(0,len(dataIndex)))
            h = sigmoid(sum(dataMatrix[randIndex]*weights))
            error = classLabels[randIndex] - h
            weights = weights + alpha *error*dataMatrix[randIndex]
            del(dataIndex[randIndex])
    return weights


# Logistic回归分类函数
def  classifyVetor(inX,weights):
    prob = sigmoid(sum(inX*weights))
    if prob > 0.5:
        return 1.0
    else:
        return 0.0

def colicTest(filetrain,filetest):
    frTrain = open(filetrain)
    frTest = open(filetest)
    trainingSet = []
    trainingLabeles = []
    for line in frTrain.readlines():
        currLine = line.strip().split('\t')
        lineArr = []
        for i in range(21):
            lineArr.append(float(currLine[i]))
        trainingSet.append(lineArr)
        trainingLabeles.append(float(currLine[21]))
    trainWeights = stocGradAscent1(array(trainingSet),trainingLabeles,500)
    errorCount = 0
    numTestVec = 0
    for line in frTest.readlines():
        numTestVec += 1.0
        currLine = line.strip().split('\t')
        lineArr =[]
        for i in range(21):
            lineArr.append(float(currLine[i]))
        if int(classifyVetor(array(lineArr),trainWeights)) != int(currLine[21]):
            errorCount += 1
    errorRate = (float(errorCount)/numTestVec)
    print('the error rate of this test is  %f'%errorRate)
    return errorRate

def multTest(filetrain,filetest):
    numTests = 6
    errorSum = 1.0
    for k in range(numTests):
        errorSum += colicTest(filetrain,filetest)
    print('after %d iterations  the average error rate is %f'%(numTests,errorSum/float(numTests)))


if __name__ == '__main__':
    filetrain = 'horseColicTraining.txt'
    filetest = 'horseColicTest.txt'
    multTest(filetrain,filetest)


　　运行结果：

the error rate of this test is  0.373134
the error rate of this test is  0.358209
the error rate of this test is  0.402985
the error rate of this test is  0.432836
the error rate of this test is  0.462687
the error rate of this test is  0.343284
after 6 iterations  the average error rate is 0.562189


　　从结果来看，6次迭代之后的平均错误率为0.56，事实上，这个结果还不错，因为数据有30%的数据缺失，当然如果调整colicTest（）中的迭代次数和stochGradAscent1()中的步长，平均错误率就可以降到20%左右。
 10，总结
　　Logistic回归的目的是寻找一个非线性函数Sigmoid的最佳拟合参数，求解过程可以由最优化算法来完成。在最优化算法中，最常用的就是梯度上升算法，而梯度上升算法又可以简化为随机梯度上升算法。
　　随机梯度上升算法与梯度上升算法的效果相当，但是占用更少的计算资源。此外，随机梯度上升是一个在线算法，它可以在新数据到来时就完成参数更新，不需要重新读取整个数据集来进行批处理运算。
　　机器学习的一个重要问题就是如何处理缺失数据，这个问题没有标准答案，取决于实际应用中的需求。
11，推广
　　前面也说了Logistic回归模型主要用于二分类，那么下面说一下多分类问题中的推广——softmax回归。
　　softmax与Logistic回归的主要区别就是，Logistic处理二分类问题，只有一组权重参数θ，而softmax处理多分类问题，如果有k个类别，那么softmax就有k组权值参数。每组权值对应一种分类，通过k组权值求解出样本数据对应每个类别的概率，最后取概率最大的类别作为该数据的分类结果，它的概率函数为：

　　softmax函数经常用于神经网络的最后一层，用于对神经网络已经处理好的特征进行分类。
基于Sklearn构建Logistic回归分类器 
　　下面让我们看一下Sklearn的Logistic回归分类器
　　英文的Sklearn文档地址：请点击我
　　sklearn.linear_model模块提供了很多模型供我们使用，比如Logistic回归、Lasso回归、贝叶斯脊回归等，可见需要学习的东西还有很多很多。本次，我们使用LogisticRegressioin。
1，LogisticRegression
　　让我们先看一下LogisticRegression这个函数，一共有14个参数

参数说明如下：
　　penalty：惩罚项，str类型，可选参数为l1和l2，默认为l2。用于指定惩罚项中使用的规范。newton-cg、sag和lbfgs求解算法只支持L2规范。L1G规范假设的是模型的参数满足拉普拉斯分布，L2假设的模型参数满足高斯分布，所谓的范式就是加上对参数的约束，使得模型更不会过拟合(overfit)，但是如果要说是不是加了约束就会好，这个没有人能回答，只能说，加约束的情况下，理论上应该可以获得泛化能力更强的结果。　　dual：对偶或原始方法，bool类型，默认为False。对偶方法只用在求解线性多核(liblinear)的L2惩罚项上。当样本数量>样本特征的时候，dual通常设置为False。　　tol：停止求解的标准，float类型，默认为1e-4。就是求解到多少的时候，停止，认为已经求出最优解。　　c：正则化系数λ的倒数，float类型，默认为1.0。必须是正浮点型数。像SVM一样，越小的数值表示越强的正则化。　　fit_intercept：是否存在截距或偏差，bool类型，默认为True。　　intercept_scaling：仅在正则化项为”liblinear”，且fit_intercept设置为True时有用。float类型，默认为1。　　class_weight：用于标示分类模型中各种类型的权重，可以是一个字典或者balanced字符串，默认为不输入，也就是不考虑权重，即为None。如果选择输入的话，可以选择balanced让类库自己计算类型权重，或者自己输入各个类型的权重。举个例子，比如对于0,1的二元模型，我们可以定义class_weight={0:0.9,1:0.1}，这样类型0的权重为90%，而类型1的权重为10%。如果class_weight选择balanced，那么类库会根据训练样本量来计算权重。某种类型样本量越多，则权重越低，样本量越少，则权重越高。当class_weight为balanced时，类权重计算方法如下：n_samples / (n_classes * np.bincount(y))。n_samples为样本数，n_classes为类别数量，np.bincount(y)会输出每个类的样本数，例如y=[1,0,0,1,1],则np.bincount(y)=[2,3]。 　　　　那么class_weight有什么作用呢？ 　　　　　　在分类模型中，我们经常会遇到两类问题：

　　　　　　　　1.第一种是误分类的代价很高。比如对合法用户和非法用户进行分类，将非法用户分类为合法用户的代价很高，我们宁愿将合法用户分类为非法用户，这时可以人工再甄别，但是却不愿将非法用户分类为合法用户。这时，我们可以适当提高非法用户的权重。
　　　　　　　　第二种是样本是高度失衡的，比如我们有合法用户和非法用户的二元样本数据10000条，里面合法用户有9995条，非法用户只有5条，如果我们不考虑权重，则我们可以将所有的测试集都预测为合法用户，这样预测准确率理论上有99.95%，但是却没有任何意义。这时，我们可以选择balanced，让类库自动提高非法用户样本的权重。提高了某种分类的权重，相比不考虑权重，会有更多的样本分类划分到高权重的类别，从而可以解决上面两类问题。






　　random_state：随机数种子，int类型，可选参数，默认为无，仅在正则化优化算法为sag,liblinear时有用。　　solver：优化算法选择参数，只有五个可选参数，即newton-cg,lbfgs,liblinear,sag,saga。默认为liblinear。solver参数决定了我们对逻辑回归损失函数的优化方法，有四种算法可以选择，分别是： 

liblinear：使用了开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数。
lbfgs：拟牛顿法的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。
newton-cg：也是牛顿法家族的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。
sag：即随机平均梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅仅用一部分的样本来计算梯度，适合于样本数据多的时候。
saga：线性收敛的随机优化算法的的变重。






总结： 

　　liblinear适用于小数据集，而sag和saga适用于大数据集因为速度更快。
对于多分类问题，只有newton-cg,sag,saga和lbfgs能够处理多项损失，而liblinear受限于一对剩余(OvR)。啥意思，就是用liblinear的时候，如果是多分类问题，得先把一种类别作为一个类别，剩余的所有类别作为另外一个类别。一次类推，遍历所有类别，进行分类。
newton-cg,sag和lbfgs这三种优化算法时都需要损失函数的一阶或者二阶连续导数，因此不能用于没有连续导数的L1正则化，只能用于L2正则化。而liblinear和saga通吃L1正则化和L2正则化。
同时，sag每次仅仅使用了部分样本进行梯度迭代，所以当样本量少的时候不要选择它，而如果样本量非常大，比如大于10万，sag是第一选择。但是sag不能用于L1正则化，所以当你有大量的样本，又需要L1正则化的话就要自己做取舍了。要么通过对样本采样来降低样本量，要么回到L2正则化。
从上面的描述，大家可能觉得，既然newton-cg, lbfgs和sag这么多限制，如果不是大样本，我们选择liblinear不就行了嘛！错，因为liblinear也有自己的弱点！我们知道，逻辑回归有二元逻辑回归和多元逻辑回归。对于多元逻辑回归常见的有one-vs-rest(OvR)和many-vs-many(MvM)两种。而MvM一般比OvR分类相对准确一些。郁闷的是liblinear只支持OvR，不支持MvM，这样如果我们需要相对精确的多元逻辑回归时，就不能选择liblinear了。也意味着如果我们需要相对精确的多元逻辑回归不能使用L1正则化了。






max_iter：算法收敛最大迭代次数，int类型，默认为10。仅在正则化优化算法为newton-cg, sag和lbfgs才有用，算法收敛的最大迭代次数。multi_class：分类方式选择参数，str类型，可选参数为ovr和multinomial，默认为ovr。ovr即前面提到的one-vs-rest(OvR)，而multinomial即前面提到的many-vs-many(MvM)。如果是二元逻辑回归，ovr和multinomial并没有任何区别，区别主要在多元逻辑回归上。 

OvR和MvM有什么不同？ 







　　OvR的思想很简单，无论你是多少元逻辑回归，我们都可以看做二元逻辑回归。具体做法是，对于第K类的分类决策，我们把所有第K类的样本作为正例，除了第K类样本以外的所有样本都作为负例，然后在上面做二元逻辑回归，得到第K类的分类模型。其他类的分类模型获得以此类推。
　　而MvM则相对复杂，这里举MvM的特例one-vs-one(OvO)作讲解。如果模型有T类，我们每次在所有的T类样本里面选择两类样本出来，不妨记为T1类和T2类，把所有的输出为T1和T2的样本放在一起，把T1作为正例，T2作为负例，进行二元逻辑回归，得到模型参数。我们一共需要T(T-1)/2次分类。
　　可以看出OvR相对简单，但分类效果相对略差（这里指大多数样本分布情况，某些样本分布下OvR可能更好）。而MvM分类相对精确，但是分类速度没有OvR快。如果选择了ovr，则4种损失函数的优化方法liblinear，newton-cg,lbfgs和sag都可以选择。但是如果选择了multinomial,则只能选择newton-cg, lbfgs和sag了。






　　verbose：日志冗长度，int类型。默认为0。就是不输出训练过程，1的时候偶尔输出结果，大于1，对于每个子模型都输出。　　warm_start：热启动参数，bool类型。默认为False。如果为True，则下一次训练是以追加树的形式进行（重新使用上一次的调用作为初始化）。　　n_jobs：并行数。int类型，默认为1。1的时候，用CPU的一个内核运行程序，2的时候，用CPU的2个内核运行程序。为-1的时候，用所有CPU的内核运行程序。
除此之外，LogisticRegression也有一些方法供我们使用：

有一些方法和MultinomialNB的方法都是类似的，因此不再累述。
 2，编写代码
　　了解了基本知识点，我们就可以编写Sklearn分类器的代码了，代码如下：

#_*_ codingLutf-8_*_

from sklearn.linear_model import LogisticRegression

def colicSklearn(filetrain,filetest):
    frTrain = open(filetrain)
    frTest = open(filetest)
    trainingSet = []
    trainingLabels = []
    testSet = []
    testLabels = []
    for line in frTrain.readlines():
        currLine = line.strip().split('\t')
        lineArr = []
        for i in range(len(currLine)-1):
            lineArr.append(float(currLine[i]))
        trainingSet.append(lineArr)
        trainingLabels.append(float(currLine[-1]))
    for line in frTest.readlines():
        currLine = line.strip().split('\t')
        lineArr = []
        for i in range(len(currLine)-1):
            lineArr.append(float(currLine[i]))
        testSet.append(lineArr)
        testLabels.append(float(currLine[-1]))
    classifier = LogisticRegression(solver='liblinear',max_iter=20).fit(trainingSet,trainingLabels)
    test_accurcy = classifier.score(testSet,testLabels)*100
    print("正确率为%s%%"%test_accurcy)

if __name__ == '__main__':
    filetrain = 'horseColicTraining.txt'
    filetest = 'horseColicTest.txt'
    colicSklearn(filetrain,filetest)


　　执行结果如下：

正确率为73.13432835820896%


　　可以看到，正确率又搞了，更改solver参数，比如设置为sag，使用随机平均梯度下降算法，看一看效果，你会发现：正确率高了，但是发出了警告，发出警告是因为算法还没有收敛，更改迭代次数即可。
 当我们迭代到5000的时候，就不会报错了，所以说，对于小数据集，sag算法需要迭代上千次才收敛，而liblinear只需要不到10次。
　　所以，我们需要根据数据集情况，选择最优化算法。
3，使用Sklearn的Logistic回归算法计算鸢尾花
　　对鸢尾花进行分类代码：

from sklearn import datasets
from numpy import *
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split


def colicSklearn():
    iris = datasets.load_iris()
    X = iris.data
    Y = iris.target
    trainingSet,testSet,trainingLabels,testLabels = train_test_split(X,Y,test_size=0.25,random_state=40)
    classifier = LogisticRegression(solver='sag', max_iter=5000).fit(trainingSet, trainingLabels)
    test_accurcy = classifier.score(testSet, testLabels) * 100
    print("正确率为%s%%" % test_accurcy)

if __name__  == '__main__':
    colicSklearn()


　　结果：

正确率为100.0%


　　
参考：https://www.cnblogs.com/bonelee/p/7253508.html
https://blog.csdn.net/c406495762/article/details/77851973
https://www.cnblogs.com/wj-1314/p/10181876.html
**************************************************
浏览器与android移动端视频互播技术实现
       手机端与平台之间的视频直播功能，主要通过集成多种开源视频框架以及采购第三方视频直播服务器产品来实现预定业务需求。视频直播对话功能的实现，主要经历了三个阶段：利用开源视频框架实现视频直播、采购第三方视频服务器实现视频直播、系统平台集成开源框架实现视频互播。

 
图1.视频互播逻辑框架
       1 利用开源视频框架实现视频直播
       首先，视频流转播需要独立服务器软件构建视频流转播通道，经过框架调研与分析，我们选择SRS服务器软件作为视频转播服务器软件，而SRS是基于Linux操作系统的服务器软件。通过配置搭建CentOS服务器虚拟机作为SRS软件的基础系统环境，再安装配置开源SRS流媒体服务器软件作为推流和播流的服务软件，为视频直播模块的开发提供基础的测试环境。流媒体服务器（SRS）在视频流播放的过程中，起到建立视频流通道的作用，每个唯一的URL地址代表一个视频流通道，推流和拉流都是针对该视频流通道的操作。流媒体服务器同时可以担负起上千条视频流通道转发操作。
       其次，手机端视频直播客户端采用开源的主流yaesa框架，实现了较好视频推流效果。依据业务需求把yaesa推流模块集成封装，顺利合并到原有的Android系统框架中，完成android客户端的视频推流功能，将rtmp格式视频流推送到流媒体服务器（SRS）的视频流通道地址。
       再次，系统平台的视频画面播放功能采用video.js播放框架实现，video.js是基于html5的网络视频播放器，根据指定的视频流通道地址从流媒体服务器自动拉取rtmp格式视频流，并播放实时的视频画面。Web页面中嵌入多个video控件对象，从多个视频流通道读取视频流，实现了多视频直播画面窗口效果。
       2 采购第三方视频服务器实现视频直播
       为了保障视频流转发过程的稳定性，从硬件厂商采购了第三方视频服务器TFMS，包含硬件设备和流媒体服务器软件两个部分，替换开源流媒体服务器软件SRS产品，其发挥的视频流转发作用与SRS是一样。
       对于系统功能实现来说，无需新增开发工作量，只需要修改视频流通道地址url即可实现流媒体服务器的切换。
       3 系统平台集成开源框架实现视频互播
       上述第一阶段实现了视频流从手机端到浏览器端的单向传播。根据业务需求的深入，需要再实现视频流从浏览器端到手机端的单向传播，构建两条双向视频流通道，从而达到视频播放互动对话的效果。
       技术实现上，需要两个方面的技术准备。浏览器集成插件从摄像头和麦克风获取视频流和声音，并推流到指定的视频流通道url；手机端集成拉流框架，从指定视频流通道url拉取视频流和声音，并播流到播放窗口控件中。
       系统平台的推流插件方面，通过市场产品调研和比较，选择了NodeMediaClient Web推流框架，该框架基于flash实现了连接PC的Camera和microphone硬件设备端口，直接获取Camera视频流和microphone声音，并以rtmp格式把视频流推送到流媒体服务器的指定视频流通道url。

       手机端采用vitamio播流框架，从指定的视频流通道url拉取视频流，并在手机端弹出视频播放的浮动窗口，该窗口中实时播放拉取的rtmp格式视频流播放，从而完成从浏览器端到手机端的视频流的推流和拉流过程。

https://www.cnblogs.com/chuzhouGIS/p/10291621.html
**************************************************
Jenkins持续集成学习-Windows环境进行.Net开发
目录

Jenkins持续集成学习-Windows环境进行.Net开发
前言
目标
使用Jenkins
安装
添加.net环境配置
部署

结语
参考文档


Jenkins持续集成学习-Windows环境进行.Net开发


前言
本文探究在.net环境下的持续集成环境研究并使用。关于为什么使用Jenkins，可以参考一下jenkins持续集成原理
目标
学习jenkins的基本使用，完成以下2点任务。

搭建jenkins任务完成自动编译。
自动从nuget上获取需要的包。

使用Jenkins
安装
到Jenkins官网下载安装包，进行安装。安装完成后会自动打开一个页面。默认是网站是localhost:8080。若端口已被占用则需要修改成别的端口。
具体安装问题可以看这里浅谈.net jenkins svn下自动化集成环境安装 搭建 配置
主要是安装完后需要安装.net的环境的一些插件及svn(或git)等相关插件。

根据官网描述是需要安装java环境的

添加.net环境配置
安装完后如果要在.net环境使用，在MSBuild插件安装完的前提下。配置msbuild.exe的路径。这样jenkins就能通过msbuild来编译.net项目了。具体配置可以可以参考《为 Jenkins 配置 .NET 持续集成环境》
我本地的jenkins的全局配置配的MSBuild也为14.0

安装了不同版本的VS的MSBuild版本可能不一样。


部署
我们通过VS先新建一个控制台项目.net framework4.5的项目，项目名称叫做JenkinsTest。在Program.cs中简单的输出hello world
static void Main(string[] args)
{
    Console.WriteLine("Hello World!");
}
程序目录如下

│  JenkinsTest.sln
│
└─JenkinsTest
    │  App.config
    │  JenkinsTest.csproj
    │  Program.cs
    │
    └─Properties
            AssemblyInfo.cs

编译通过后将项目上传到SVN上，我在我本地建了SVN的服务，并增加了用户名和密码分别为test。

新建项目

创建一个名为test的自由风格软件项目。

我本地使用的是SVN，因此选择Subversion，输入SVN的路径。Local module directory为SVN获取代码的路径。.表示获取到jenkins的根目录下。

首次创建的时候需要创建SVN的登录凭据，在Credentials项点击Add添加一个新的凭据，类型就选择Username with password即可,id需要输入一个唯一凭据标识。

在Build Environment下勾选Add timestamps to the Console Output,这样可以显示时间戳。

在Build选择MSBuild的版本，这个版本在全局配置设置过，在这里就可以显示出来。MSBuild Build File输入需要编译的程序集文件名。在Command Line Arguments输入编译的参数，我们编译成Realse版本。完成后点击保存即可。具体MSBuild指令不做具体探究，有什么问题直接可以看官方文档
点击立即构建就会自动编译，完成就会显示一个结果


点击#1即可调转到该次编译的详细信息中，在左侧点击Console Output可以看到编译的过程日志。

构建过程分析

先从SVN获取代码
16:21:55 由用户 jake 启动
16:21:55 构建中 在工作空间 D:\Program Files (x86)\Jenkins\workspace\test 中
16:21:55 Updating https://jakepc/svn/JenkinsTest/trunk/JenkinsTest at revision '2019-01-18T16:21:55.792 +0800' --quiet
16:21:55 Using sole credentials test/****** (本地svn服务器) in realm ‘<https://jakepc:443> VisualSVN Server’
16:21:55 At revision 3
调用MSBuild命令进行编译，这里会查找我们全局配置的MSBuild.exe执行我们在创建时输入的Command Line Arguments指令。
16:21:56 Path To MSBuild.exe: C:\Program Files (x86)\MSBuild\14.0\Bin\MSBuild.exe
16:21:56 Executing the command cmd.exe /C " "C:\Program Files (x86)\MSBuild\14.0\Bin\MSBuild.exe" /t:Build /p:Configuration=Release JenkinsTest.csproj " && exit %%ERRORLEVEL%% from D:\Program Files (x86)\Jenkins\workspace\test
16:21:56 [test] $ cmd.exe /C " "C:\Program Files (x86)\MSBuild\14.0\Bin\MSBuild.exe" /t:Build /p:Configuration=Release JenkinsTest.csproj " && exit %%ERRORLEVEL%%
编译完成，显示警告，错误和结果
...
16:21:57 _CopyAppConfigFile:
16:21:57   正在将文件从“App.config”复制到“bin\Release\JenkinsTest.exe.config”。
16:21:57 CopyFilesToOutputDirectory:
16:21:57   正在将文件从“obj\Release\JenkinsTest.exe”复制到“bin\Release\JenkinsTest.exe”。
16:21:57   JenkinsTest -> D:\Program Files (x86)\Jenkins\workspace\test\bin\Release\JenkinsTest.exe
16:21:57   正在将文件从“obj\Release\JenkinsTest.pdb”复制到“bin\Release\JenkinsTest.pdb”。
16:21:57 已完成生成项目“D:\Program Files (x86)\Jenkins\workspace\test\JenkinsTest.csproj”(Build 个目标)的操作。
16:21:57 
16:21:57 已成功生成。
16:21:57     0 个警告
16:21:57     0 个错误
16:21:57 
16:21:57 已用时间 00:00:01.22
16:21:59 Finished: SUCCESS

通过以上三步骤，实际和我们自己使用VS编译过程也是一样的。
增加依赖dll
上面我们创建了一个最简单的项目，并通过jenkins获取并编译成功了，下面我们增加项目复杂性，增加其他依赖项。
新建一个jenkins.Common的类库。我们把HelloWorld的字符串通过该类库获取到，然后主项目进行输出。
增加HelloWolrdHelper类获取字符串
public class HelloWolrdHelper
{
    public static string GetString()
    {
        return "Hello World!";
    }
}
修改原项目
static void Main(string[] args)
{
    Console.WriteLine(HelloWolrdHelper.GetString());
    Console.ReadKey();
}

然后提交代码到SVN后在jenkins再次构建,结构如下。

│  JenkinsTest.sln
│
├─Jenkins.Common
│  │  HelloWolrdHelper.cs
│  │  Jenkins.Common.csproj
│  │
│  └─Properties
│          AssemblyInfo.cs
│
└─JenkinsTest
    │  App.config
    │  JenkinsTest.csproj
    │  Program.cs
    │
    └─Properties
            AssemblyInfo.cs

构建一下,编译成功了，MSBuild会根据csproj文件内的依赖关系编译其他程序集。

生成启动时间为 2019/1/18 17:16:03。
17:16:03 项目“D:\Program Files (x86)\Jenkins\workspace\test\JenkinsTest\JenkinsTest.csproj”在节点 1 上(Build 个目标)。
17:16:03 PrepareForBuild:
17:16:03   正在创建目录“bin\Release\”。
17:16:03   正在创建目录“obj\Release\”。
17:16:03 项目“D:\Program Files (x86)\Jenkins\workspace\test\JenkinsTest\JenkinsTest.csproj”(1)正在节点 1 上生成“D:\Program Files (x86)\Jenkins\workspace\test\Jenkins.Common\Jenkins.Common.csproj”(2) (默认目标)。
17:16:03 PrepareForBuild:
17:16:03   正在创建目录“bin\Release\”。
17:16:03   正在创建目录“obj\Release\”。
17:16:03 GenerateTargetFrameworkMonikerAttribute:
17:16:03 正在跳过目标“GenerateTargetFrameworkMonikerAttribute”，因为所有输出文件相对于输入文件而言都是最新的。
增加Nuget依赖
新建一个Jenkins.Core项目，并生成Nuget包上传到Nuget服务器上。

关于Nuget如何打包可以看我之前的博客NuGet的使用、部署、搭建私有服务,这里不做讨论。

为了简单起见，创建一个和Jenkins.Common程序集一样的输出HelloWorld的方法，但是为了作为区分，分别略作修改。
Jenkins.Common的代码
public class HelloWolrdHelper
{
    public static string GetString()
    {
        return "Hello World! Jenkins.Common";
    }
}
Jenkins.Core的代码
public class HelloWolrdHelper
{
    public static string GetString()
    {
        return "Hello World! Jenkins.Core";
    }
}
程序结构如下
│  JenkinsTest.sln
│
├─.nuget
│      NuGet.Config
│      NuGet.exe
│      NuGet.targets
│
├─Jenkins.Common
│  │  HelloWolrdHelper.cs
│  │  Jenkins.Common.csproj
│  │
│  └─Properties
│          AssemblyInfo.cs
│
├─Jenkins.Core
│  │  HelloWorldHelper.cs
│  │  Jenkins.Core.csproj
│  │  Jenkins.Core.sln
│  │
│  └─Properties
│          AssemblyInfo.cs
│
└─JenkinsTest
   │  App.config
   │  JenkinsTest.csproj
   │  packages.config
   │  Program.cs
   │
   └─Properties
           AssemblyInfo.cs

注意:Jenkins.Core我放到一个单独的项目中打包的。原本我是放到原项目一起。但是Nuget似乎有个bug，导致编译通不过。


因为控制台项目更新的时候会向JenkinsTest.csproj文件写入nuget包还原的指令

 <Target Name="EnsureNuGetPackageBuildImports" BeforeTargets="PrepareForBuild">
    <PropertyGroup>
      <ErrorText>This project references NuGet package(s) that are missing on this computer. Enable NuGet Package Restore to download them.  For more information, see http://go.microsoft.com/fwlink/?LinkID=322105. The missing file is {0}.</ErrorText>
    </PropertyGroup>
    <Error Condition="!Exists('$(SolutionDir)\.nuget\NuGet.targets')" Text="$([System.String]::Format('$(ErrorText)', '$(SolutionDir)\.nuget\NuGet.targets'))" />
  </Target>


而中解决方案的目录是在上一层目录<SolutionDir Condition="$(SolutionDir) == '' Or $(SolutionDir) == '*Undefined*'">..\</SolutionDir>,最终编译的时候会校验pacakges是否存在已下载的包。通过路径$(SolutionDir)\packages\下查找，即..\\packages\。多了一个\导致存在包仍然报错查找不到，最终编译不过。

上传nuget包

然后从Nuget本地nuget服务器上获取。

Program的代码增加新的helloworld输出

class Program
{
    static void Main(string[] args)
    {
        Console.WriteLine(HelloWolrdHelper.GetString());
        Console.WriteLine(Jenkins.Core.HelloWolrdHelper.GetString());
        Console.ReadKey();
    }
}

重新编译项目后上传代码到SVN后再次到Jenkins上构建项目。

可以看到构建失败了，去看下具体失败原因。
正在将文件从“obj\Release\Jenkins.Common.pdb”复制到“bin\Release\Jenkins.Common.pdb”。
12:51:14 已完成生成项目“D:\Program Files (x86)\Jenkins\workspace\test\Jenkins.Common\Jenkins.Common.csproj”(默认目标)的操作。
12:51:14 ResolveAssemblyReferences:
12:51:14   主引用“Jenkins.Core, Version=0.2.0.0, Culture=neutral, processorArchitecture=MSIL”。
12:51:14 C:\Program Files (x86)\MSBuild\14.0\bin\Microsoft.Common.CurrentVersion.targets(1820,5): warning MSB3245: 未能解析此引用。未能找到程序集“Jenkins.Core, Version=0.2.0.0, Culture=neutral, processorArchitecture=MSIL”。请检查磁盘上是否存在该程序集。 如果您的代码需要此引用，则可能出现编译错误。 [D:\Program Files (x86)\Jenkins\workspace\test\JenkinsTest\JenkinsTest.csproj]
12:51:14           用于 SearchPath“{HintPathFromItem}”。
12:51:14           已考虑使用“..\packages\Jenkins.Core.0.2.0\lib\net45\Jenkins.Core.dll”，但它不存在。
12:51:14           用于 SearchPath“{TargetFrameworkDirectory}”。
12:51:14           已考虑使用“C:\Program Files (x86)\Reference Assemblies\Microsoft\Framework\.NETFramework\v4.5\Jenkins.Core.winmd”，但它不存在。
12:51:14           已考虑使用“C:\Program Files (x86)\Reference Assemblies\Microsoft\Framework\.NETFramework\v4.5\Jenkins.Core.dll”，但它不存在
...
12:51:14 Program.cs(15,22): error CS0234: The type or namespace name 'Core' does not exist in the namespace 'Jenkins' (are you missing an assembly reference?) [D:\Program Files (x86)\Jenkins\workspace\test\JenkinsTest\JenkinsTest.csproj]
12:51:14 已完成生成项目“D:\Program Files (x86)\Jenkins\workspace\test\JenkinsTest\JenkinsTest.csproj”(Build 个目标)的操作 - 失败。
12:51:15 
12:51:15 生成失败。
12:51:15 
12:51:15 “D:\Program Files (x86)\Jenkins\workspace\test\JenkinsTest\JenkinsTest.csproj”(Build 目标) (1) ->
12:51:15 (ResolveAssemblyReferences 目标) -> 
12:51:15   C:\Program Files (x86)\MSBuild\14.0\bin\Microsoft.Common.CurrentVersion.targets(1820,5): warning MSB3245: 未能解析此引用。未能找到程序集“Jenkins.Core, Version=0.2.0.0, Culture=neutral, processorArchitecture=MSIL”。请检查磁盘上是否存在该程序集。 如果您的代码需要此引用，则可能出现编译错误。 [D:\Program Files (x86)\Jenkins\workspace\test\JenkinsTest\JenkinsTest.csproj]
12:51:15 
12:51:15 
12:51:15 “D:\Program Files (x86)\Jenkins\workspace\test\JenkinsTest\JenkinsTest.csproj”(Build 目标) (1) ->
12:51:15 (CoreCompile 目标) -> 
12:51:15   Program.cs(15,22): error CS0234: The type or namespace name 'Core' does not exist in the namespace 'Jenkins' (are you missing an assembly reference?) [D:\Program Files (x86)\Jenkins\workspace\test\JenkinsTest\JenkinsTest.csproj]
12:51:15 
12:51:15     1 个警告
12:51:15     1 个错误
12:51:15 
12:51:15 已用时间 00:00:01.43
12:51:15 Build step 'Build a Visual Studio project or solution using MSBuild' marked build as failure
12:51:16 Finished: FAILURE
一开始会去..\packages\Jenkins.Core.0.2.0\lib\net45\Jenkins.Core.dll获取，但是因为没有包，后面就去遍历其他目录获取，最终都没有找到包导致编译失败。因此我们需要使用nuget，在MSBuild编译之前将包下载下来。
为了方便我将nuget一同上传到SVN上，SVN的程序目录如下
│  JenkinsTest.sln
│
├─.nuget
│      NuGet.Config
│      NuGet.exe
│      NuGet.targets
│
├─Jenkins.Common
│  │  HelloWolrdHelper.cs
│  │  Jenkins.Common.csproj
│  │
│  └─Properties
│          AssemblyInfo.cs
│
└─JenkinsTest
   │  App.config
   │  JenkinsTest.csproj
   │  packages.config
   │  Program.cs
   └─Properties
           AssemblyInfo.cs


由于Jenkins.Core可以理解为第三方依赖，不是和JenkinsTest放一起。

在Build配置中新增一项批处理命令，执行nuget包还原。如图拖动到编译之前。

根据我们目录结构调用".nuget/nuget.exe" restore JenkinsTest.sln命令使用nuget进行包还原。

由于.nuget目录带有.因此需要用双引号引起来执行。

再次编译仍然失败了。可以通过日志看到已经在编译前已经执行了脚本，但是没有从我们想要的地方获取包。

13:26:47 No changes for https://jakepc/svn/JenkinsTest/trunk/JenkinsTest since the previous build
13:26:47 [test] $ cmd /c call C:\WINDOWS\TEMP\jenkins8156278399083484284.bat
13:26:47 
13:26:47 D:\Program Files (x86)\Jenkins\workspace\test>".nuget/nuget.exe" restore JenkinsTest.sln 
13:26:47 MSBuild auto-detection: using msbuild version '15.9.21.664' from 'D:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\MSBuild\15.0\bin'.
13:26:48 Restoring NuGet package Jenkins.Core.0.2.0.
13:26:49   GET https://api.nuget.org/v3-flatcontainer/jenkins.core/0.2.0/jenkins.core.0.2.0.nupkg
13:26:49   NotFound https://api.nuget.org/v3-flatcontainer/jenkins.core/0.2.0/jenkins.core.0.2.0.nupkg 191ms
13:26:49 警告: Unable to find version '0.2.0' of package 'Jenkins.Core'.
13:26:49   C:\Program Files (x86)\Microsoft SDKs\NuGetPackages\: Package 'Jenkins.Core.0.2.0' is not found on source 'C:\Program Files (x86)\Microsoft SDKs\NuGetPackages\'.
13:26:49   https://api.nuget.org/v3/index.json: Package 'Jenkins.Core.0.2.0' is not found on source 'https://api.nuget.org/v3/index.json'.
13:26:49 
13:26:49 警告: Unable to find version '0.2.0' of package 'Jenkins.Core'.
13:26:49   C:\Program Files (x86)\Microsoft SDKs\NuGetPackages\: Package 'Jenkins.Core.0.2.0' is not found on source 'C:\Program Files (x86)\Microsoft SDKs\NuGetPackages\'.
13:26:49   https://api.nuget.org/v3/index.json: Package 'Jenkins.Core.0.2.0' is not found on source 'https://api.nuget.org/v3/index.json'.
13:26:49 
13:26:49 
13:26:49 Errors in packages.config projects
13:26:49     Unable to find version '0.2.0' of package 'Jenkins.Core'.
13:26:49       C:\Program Files (x86)\Microsoft SDKs\NuGetPackages\: Package 'Jenkins.Core.0.2.0' is not found on source 'C:\Program Files (x86)\Microsoft SDKs\NuGetPackages\'.
13:26:49       https://api.nuget.org/v3/index.json: Package 'Jenkins.Core.0.2.0' is not found on source 'https://api.nuget.org/v3/index.json'.
13:26:49 
13:26:49 NuGet Config files used:
13:26:49     D:\Program Files (x86)\Jenkins\workspace\test\.nuget\NuGet.Config
13:26:49     C:\WINDOWS\system32\config\systemprofile\AppData\Roaming\NuGet\NuGet.Config
13:26:49     C:\Program Files (x86)\NuGet\Config\Microsoft.VisualStudio.Offline.config
13:26:49 
13:26:49 Feeds used:
13:26:49     https://api.nuget.org/v3/index.json
13:26:49     C:\Program Files (x86)\Microsoft SDKs\NuGetPackages\
13:26:49 
13:26:49 D:\Program Files (x86)\Jenkins\workspace\test>exit 1 
13:26:49 Build step '执行 Windows 批处理命令' marked build as failure
13:26:50 Finished: FAILURE
指定nuget包下载源地址。
找到Nuget.exe的配置Nuget.Config,在configuration节点内增加packageSources节点。

  <packageSources>
    <add key="Jake Package source" value="http://127.0.0.1:10080/nuget" />
  </packageSources>

再次编译,终于编译成功了,日志如下。
13:38:08 No changes for https://jakepc/svn/JenkinsTest/trunk/JenkinsTest since the previous build
13:38:08 [test] $ cmd /c call C:\WINDOWS\TEMP\jenkins5326599668058283263.bat
13:38:08 
13:38:08 D:\Program Files (x86)\Jenkins\workspace\test>".nuget/nuget.exe" restore JenkinsTest.sln 
13:38:08 MSBuild auto-detection: using msbuild version '15.9.21.664' from 'D:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\MSBuild\15.0\bin'.
13:38:09 Restoring NuGet package Jenkins.Core.0.2.0.
13:38:09 Adding package 'Jenkins.Core.0.2.0' to folder 'D:\Program Files (x86)\Jenkins\workspace\test\packages'
13:38:09 Added package 'Jenkins.Core.0.2.0' to folder 'D:\Program Files (x86)\Jenkins\workspace\test\packages'
13:38:09 
13:38:09 NuGet Config files used:
13:38:09     D:\Program Files (x86)\Jenkins\workspace\test\.nuget\NuGet.Config
13:38:09     C:\WINDOWS\system32\config\systemprofile\AppData\Roaming\NuGet\NuGet.Config
13:38:09     C:\Program Files (x86)\NuGet\Config\Microsoft.VisualStudio.Offline.config
13:38:09 
13:38:09 Feeds used:
13:38:09     C:\WINDOWS\system32\config\systemprofile\.nuget\packages\
13:38:09     http://127.0.0.1:10080/nuget
13:38:09     https://api.nuget.org/v3/index.json
13:38:09     C:\Program Files (x86)\Microsoft SDKs\NuGetPackages\
13:38:09 
13:38:09 Installed:
13:38:09     1 package(s) to packages.config projects
13:38:09 
13:38:09 D:\Program Files (x86)\Jenkins\workspace\test>exit 0 
13:38:09 Path To MSBuild.exe: C:\Program Files (x86)\MSBuild\14.0\Bin\MSBuild.exe
13:38:09 Executing the command cmd.exe /C " "C:\Program Files (x86)\MSBuild\14.0\Bin\MSBuild.exe" /t:Build /p:Configuration=Release JenkinsTest/JenkinsTest.csproj " && exit %%ERRORLEVEL%% from D:\Program Files (x86)\Jenkins\workspace\test
13:38:09 [test] $ cmd.exe /C " "C:\Program Files (x86)\MSBuild\14.0\Bin\MSBuild.exe" /t:Build /p:Configuration=Release JenkinsTest/JenkinsTest.csproj " && exit %%ERRORLEVEL%%
结语
通过N次尝试，最终完成了.Net开发下Jenkins进行持续集成。总结起来如下：

安装Java环境，由于我本地已有java环境，因此该步骤跳过。
安装MSBuild，由于我本地已安装过VS，因此该步骤跳过。
下载Nuget,由于我本地已安装VS，同时已安装过Nuget，因此该步骤跳过。
安装Jenkins，并安装MSbuild插件。
修改Jenkins的MSBuild插件的配置。
新建一个工程，修改配置

选择源码管理，我本地是SVN，因此选择SVN(增加登录凭据)
增加windows批处理脚本用于Nuget下载库包。（在Nuget.Config中增加自己的包服务地址）
增加MSBuild编译VS的项目


通过以上步骤，基本就完成了自动编译的功能。但是光编译还是不够的，最终还要完成自动跑单元测试，下一篇博客再进行研究。
参考文档

Jenkins部署.net自动化构建
为 Jenkins 配置 .NET 持续集成环境
浅谈.net jenkins svn下自动化集成环境安装 搭建 配置
jenkins持续集成原理
Jenkins结合.net平台工具之Nuget
Make Jenkins aware of custom NuGet Package Source

https://www.cnblogs.com/Jack-Blog/p/10291612.html
**************************************************
诡异的druid链接池链接断开故障经验总结


背景
症状
排查
修复

背景
最近在陆续做机房升级相关工作，配合DBA对产线数据库链接方式做个调整，将原来直接链接读库的地址切换到统一的读负载均衡的代理 haproxy 上，方便机柜和服务器的搬迁。
切换之后线上时不时的会发生 discard connection 错误，导致程序报 500 错误，但不是每次都必现的。

开发框架: spring boot+mybatis+druid+shardingJDBC
网络架构:
appserver->mysql(master) 写
appserver->haproxy->mysql(slave)/n 读

第一反应肯定是因为这次的读库地址的变动引起的问题，觉得问题应该是 druid 链接池中的 connection 保活策略没起作用，只要做下配置修改应该就可以了。结果这个问题让我们排查了好几天，我们竟然踩到了千年难遇的深坑。
这个问题排查的很坎坷，一次次的吐血，最终我们定位到问题并且优雅的修复了，我们一起来体验下这个一次一次让你绝望一次一次打脸的过程。
症状
先说故障症状，经常出现如下错误：

discard connection
com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure


The last packet successfully received from the server was 72,557 milliseconds ago. The last packet sent successfully to the server was 0 milliseconds ago.

根据错误日志初步判断肯定是与 db 之间的链接已经断开，尝试使用了一个已经断开的链接才会引起这个错误发生。但是根据我们对 druid 了解，druid 有链接检查功能，按理不会拿到一个无效链接才对，带着这个线索我们上路了。
排查
为了准确的知道 db 的链接的存活时间，了解到 haproxy 对转发的 db tcp 链接空闲时间在 1m 之内，超过 1m 不活动就会被关掉。也就说我们与 db 之间的原来的长链接在 1m 之内会被断开。我们先不管这个时间设置的是否符合所有的大并发场景，至少在 druid 的链接池里会有有效链接检查，应该不会拿到无效链接才对，我们做了配置调整。
我们看下 druid 跟链接时间相关的配置：
datasource.druid.validationQuery=SELECT 1
datasource.druid.validationQueryTimeout=2000
datasource.druid.testWhileIdle=true
datasource.druid.minEvictableIdleTimeMillis=100000
datasource.druid.timeBetweenEvictionRunsMillis=20000
配置的每项的意思这里就不解释了。
我们启用了 testWhileIdle 配置，让每次拿取链接的时候发起检查。根据 timeBetweenEvictionRunsMillis 的配置只有大于这个时间 druid 才会发起检查，所以可能的场景是拿到一个即将过期的链接，根据这个线索我们调整这个时间为 20000ms，也就是超过 20s 会检查当前拿取的链接确定是否有效，检查的方式应该是使用 validationQuery 配置的 sql 语句才对，但是发现我们并找不到任何有关于 SELECT 1 的痕迹。
为什么你死活找不到 SELECT 1
首先要搞清楚 validationQuery 为什么没起作用，带着这个疑问开始 debug druid 源码。
if (isTestWhileIdle()) {
                    final long currentTimeMillis = System.currentTimeMillis();
                    final long lastActiveTimeMillis = poolableConnection.getConnectionHolder().getLastActiveTimeMillis();
                    final long idleMillis = currentTimeMillis - lastActiveTimeMillis;
                    long timeBetweenEvictionRunsMillis = this.getTimeBetweenEvictionRunsMillis();
                    if (timeBetweenEvictionRunsMillis <= 0) {
                        timeBetweenEvictionRunsMillis = DEFAULT_TIME_BETWEEN_EVICTION_RUNS_MILLIS;
                    }

                    if (idleMillis >= timeBetweenEvictionRunsMillis) {
                        boolean validate = testConnectionInternal(poolableConnection.getConnection());
                        if (!validate) {
                            if (LOG.isDebugEnabled()) {
                                LOG.debug("skip not validate connection.");
                            }

                            discardConnection(realConnection);
                            continue;
                        }
                    }
                }
            }
闲置时间肯定会有大于 timeBetweenEvictionRunsMillis 时间的，会发起 testConnectionInternal 方法检查。我们继续跟进去看，
protected boolean testConnectionInternal(DruidConnectionHolder holder, Connection conn) {
   boolean valid = validConnectionChecker.isValidConnection(conn, validationQuery, validationQueryTimeout);
   
内部会使用 validConnectionChecker 检查对象发起检查。
public boolean isValidConnection(Connection conn, String validateQuery, int validationQueryTimeout) throws Exception {
        if (conn.isClosed()) {
            return false;
        }

        if (usePingMethod) {
            if (conn instanceof DruidPooledConnection) {
                conn = ((DruidPooledConnection) conn).getConnection();
            }

            if (conn instanceof ConnectionProxy) {
                conn = ((ConnectionProxy) conn).getRawObject();
            }

            if (clazz.isAssignableFrom(conn.getClass())) {
                if (validationQueryTimeout < 0) {
                    validationQueryTimeout = DEFAULT_VALIDATION_QUERY_TIMEOUT;
                }

                try {
                    ping.invoke(conn, true, validationQueryTimeout * 1000);
                } catch (InvocationTargetException e) {
                    Throwable cause = e.getCause();
                    if (cause instanceof SQLException) {
                        throw (SQLException) cause;
                    }
                    throw e;
                }
                return true;
            }
        }

        String query = validateQuery;
        if (validateQuery == null || validateQuery.isEmpty()) {
            query = DEFAULT_VALIDATION_QUERY;
        }

        Statement stmt = null;
        ResultSet rs = null;
        try {
            stmt = conn.createStatement();
            if (validationQueryTimeout > 0) {
                stmt.setQueryTimeout(validationQueryTimeout);
            }
            rs = stmt.executeQuery(query);
            return true;
        } finally {
            JdbcUtils.close(rs);
            JdbcUtils.close(stmt);
        }

    }
debug 这里才发现，druid 默认采用的是 mysql.ping 来做链接有效性检查。
druid 默认采用msyql.ping 协议检查
那是不是用 msyql.ping 协议并不会让 mysql 重新滑动 session 闲置时间，带着这个问题打开 information_schema.processlist 进程列表查看会不会刷新会话时间，通过 debug发现是会刷新时间的，说明没有问题，这条线索算是断了。
haproxy tiemout主动close上下游链接
调整方向，开始怀疑是不是 haproxy 的一些策略导致链接失效，开始初步怀疑 haproxy 的轮训转发后端链接是不是有相关会话保持方式，是不是我们配置有误导致 haproxy 的链接和 mysql 链接篡位了。
当然这个猜想有点夸张，但是没办法，技术人员就要有怀疑一切的态度。
为了还原产线的网络路线，我在本地搭了一个 haproxy，了解下他的工作原理和配置，图方便我就用了yum顺手装了一个，版本是 HA-Proxy version 1.5.18 不知道是我本地环境问题还是这个版本的 bug，我们配置的 mode tcp 活动检查一直不生效。
listen service 127.0.0.1:60020
  mode tcp
  balance roundrobin
  option tcplog
  server server1 192.168.36.66:3306 check inter 2000 rise 2 fall 3
  server server2 192.168.36.66:3306 check inter 2000 rise 2 fall 3
由于 haproxy 活动检查一直不通过，所以无法转发我的链接，搞了半天我只能手动装了一个低版本的 haproxy HA-Proxy version 1.4.14 。
完整的配置：
defaults
        mode tcp               
        retries 3              
        option redispatch      
        option abortonclose    
        maxconn 32000          
        timeout connect 2s 
        timeout client 5m 
        timeout server 5m 


listen test1
        bind 0.0.0.0:60000
        mode tcp
        balance roundrobin
        server s1 192.168.36.66:3306 weight 1 maxconn 10000 check inter 10s
        server s2 192.168.36.66:3306 weight 1 maxconn 10000 check inter 10s
        server s3 192.168.36.66:3306 weight 1 maxconn 10000 check inter 10s
1.4 的版本顺利完成活动检查。
我使用 haproxy 进行debug，调试下来也都没有问题，也翻了下 haproxy 如何转发链接的，内部通过会话的方式保持两个链接的关系，如果是 tcp 长链接应该不会出现什么问题。haproxy 在 http 模式下有会话保持方式，tcp 应该是直接捆绑的方式，一旦到 timeout 时间会主动 close 和 mysql 的链接，而且没有出现篡位的问题。到这里线索又断了。
自定义 ValidConnectionChecker 埋点日志
没有办法，只能试着埋点 druid 的检查日志，排查内部上一次的 check和报错之间的时间差和 connectionId 是不是一致的。
public class MySqlValidConnectionCheckerDebug extends MySqlValidConnectionChecker {

    @Override
    public boolean isValidConnection(Connection conn, String validateQuery, int validationQueryTimeout) {
        
            Long connId = 0L;
            try {
                Field connField = ConnectionImpl.class.getDeclaredField("connectionId");
                connField.setAccessible(true);
                connId = (Long) connField.get(((ConnectionProxyImpl) conn).getConnectionRaw());
            } catch (Exception e) {
                log.error("valid connection error", e);
            } finally {
                log.info("valid connection ok. conn:" + connId);
            }

            return true;
}
为了拿到 connectionId 只能反射获取，在本地debug下没问题，能正常拿到 connectionId，但是发到验证环境进行验证的时候报错了，觉得奇怪，仔细看了下原来开发环境的配置和验证和生产的不一样，开发环境没有走读写分离。
验证和生产都是使用了 mysql 的 replication 的机制，所以导致我反射获取的代码报错。
datasource.druid.url=jdbc:mysql:replication
通过debug发现，原来 __druid__的 connection 是 JDBC4Connection ，变成了 ReplicationConnection ，而且里面包装了两个 connection ，一个 masterconnection ，一个 slaveconnection ，似乎问题有点浮现了。
通过debug发现 druid 的检查还是会正常走到，当走到 ReplicationConnection 内部的时候 ReplicationConnection 有一个 currentConnection ，这个链接是会在 masterConnection 和 slaveConnection 之间切换，切换的依据是 readOnly 参数。
在检查的时候由于 druid 并不感知上层的参数，readOnly 也就不会设置。所以走的是 masterConnection ，但是在程序里用的时候通过 spring 的 TransactionManager 将 readOnly 传播到了 ShardingJDBC ， ShardingJDBC 在设置到 ReplicationConnection 上，最后导致真正在使用的时候其实使用的是 slaveConnection。
找到这个问题之后去 druid github Issues 搜索了下果然有人提过这个问题，在高版本的 druid 中已经修复这个问题了。
修复
修复这个问题有两个方法，第一个方法，建议升级 druid，里面已经有 MySqlReplicationValidConnectionChecker 检查器专门用来解决这个问题。第二个方法就是自己实现 ValidConnectionChecker 检查器，但是会有在将来出现bug的可能性。
由于时间关系文章只讲了主要的排查路线，事实上我们陆续花了一周多时间，再加上周末连续趴上十几个小时才找到这根本问题。
这个问题之所以难定位的原因主要是牵扯的东西太多，框架层面、网络链接层面、mysql服务器层面，haproxy代理等等，当然其中也绕了很多弯路。。
下面分享在这个整个排查过程中的一些技术收获。
相关技术问题
1.mysqlConenction提供了ping方法用来做活动检查，默认MySqlValidConnectionChecker使用的是pinginternal。
ping = clazz.getMethod("pingInternal", boolean.class, int.class);
2.低版本的druid不支持自定义 ValidConnectionChecker 来做个性化的检查。
3.druid 的test方法使用注意事项，testOnBorrow 在获取链接的时候进行检查，与testWhileIdle是护持关系。
if (isTestOnBorrow()) {
            } else {
                if (isTestWhileIdle()) {
3.kill mysql processlist 进程会话到链接端tcp状态有延迟，这是tcp的四次断开延迟。
4.haproxy 1.5.18 版本 mode tcp check不执行，健康检查设置无效。
5.mysql replication connection master/slave切换逻辑需要注意，会不会跟上下油的链接池组合使用出现bug，尤其是分库不表、读写分离、自定义分片。
6.排查mysql服务器的问题时，打开各种日志，操作日志，binlog日志。
7.springtransactionmanagenent 事务传播特性会影响下游数据源的选择，setreadonly、setautocommit。
8.低版本的 druid MySqlValidConnectionChecker 永远执行不到 ReplicationConnection ping 方法。
作者：王清培(沪江网资深架构师)

https://www.cnblogs.com/wangiqngpei557/p/10291606.html
**************************************************
Salesforce Lightning开发学习（四）重写新建/更新按钮
重写新建/更新按钮的原因是因为项目需要用户在新建数据时从接口对数据进行校验，保证数据的有效性，同时获取接口返回的部分数据完成信息填充，而Sales force的trigger仅支持@future方法异步调用接口，不能实时完成数据校验
那么重写新建/更新按钮要面临的几个核心问题：
1、lightning暂时不支持lookup字段，如果重写可能要自定义组件
2、lightning新建按钮，怎么重写在项目下新建时自动填充的父对象
点击对象管理器，新建对象项目（Test_Project），部门（Test_Department）
以下是部门（Test_Department）的字段表



标签
API
数据类型
值


部门名称
Name
文本（80）
 


项目
Project_Dep__c
查找（项目）
 


角色
Role__c
选项列表

项目经理
项目顾问
项目开发
项目测试



邮箱
Email__c
电子邮件
 


报表权限
Report_Access__c
复选框
 


描述
Remarks__c
文本区（255）
 



 先创建一个lightning组件Test_NewDepartment

<aura:component implements="lightning:actionOverride,　　　　　　　　　　　　　　　　 flexipage:availableForRecordHome,　　　　　　　　　　　　　　　　 force:hasRecordId,　　　　　　　　　　　　　　　　 flexipage:availableForAllPageTypes" 
                access="global" 
                description="Test_NewDepartment">
</aura:component>

组件继承的几个接口说明下
lightning:actionOverride：继承该接口才能覆盖标准按钮

force:hasRecordId：继承该接口才能通过“v.recordId"获取当前页面的记录ID
然后来处理第一个问题，lightning暂时不支持lookup字段的问题
经过了解，lightning提供了一个<lightning:recordEditForm>组件，通过<lightning:inputField>可以操作查找字段.

<aura:component implements="lightning:actionOverride,　　　　　　　　　　　　　　　　 flexipage:availableForRecordHome,　　　　　　　　　　　　　　　　 force:hasRecordId,　　　　　　　　　　　　　　　　 flexipage:availableForAllPageTypes" 
                access="global" 
                description="Test_NewDepartment">　　<!-- 部门 -->
    <aura:attribute name="simpleDepartmentRecord" type="Test_Department__c" default="{'SobjectType':'Test_Department__c'}"/>
　　<!-- 错误消息-->    <aura:attribute name="recordError" type="String"/>　　<!-- 标记按钮能否点击-->
    <aura:attribute name="flag" type="Boolean" default="true"/>

    <div aura:id="editDialog" role="dialog" tabindex="-1" aria-labelledby="header43" class="slds-modal slds-fade-in-open">
        <div class="slds-modal__container">
            <div class="slds-modal__header">
                <h2 class="slds-text-heading--medium">{!(v.recordId == null?'新增':'更新') + '部门'}</h2>
            </div>
            <lightning:messages />
            <lightning:recordEditForm objectApiName="Test_Department__c">
                <div class="slds-modal__content slds-p-around--medium slds-wrap" >
                    <lightning:input    aura:id="departmentId" 
                                        label="用户名" 
                                        name="userName" 
                                        placeholder="请输入完整的用户名"
                                        required="true"        
                                        value="{!v.simpleDepartmentRecord.Name}" />
                    <lightning:inputField   class="customRequired" 
                                            aura:id="roleId" 
                                            fieldName="Role__c" 
                                            value="{!v.simpleDepartmentRecord.Role__c}"/>
                    <lightning:inputField   class="customRequired" 
                                            fieldName="Email__c" 
                                            value="{!v.simpleDepartmentRecord.Email__c}"/>
                    <lightning:inputField   class="customRequired" 
                                            aura:id="projectLookupId" 
                                            fieldName="Project_Dep__c" 
                                            value="{!v.simpleDepartmentRecord.Project_Dep__c}"/>
                    <lightning:inputField   class="customRequired" 
                                            aura:id="reportId" 
                                            fieldName="Report_Access__c"
                                            value="{!v.simpleDepartmentRecord.Report_Access__c}"/>
                    <lightning:inputField   fieldName="Remarks__c" 
                                            value="{!v.simpleDepartmentRecord.Remarks__c}"/>
                </div>

                <div class="slds-modal__footer">                    
                    <lightning:button variant="neutral" label="Cancel" onclick="{!c.cancelDialog}" />
                    <lightning:button variant="brand"   label="Submit" onclick="{!c.saveRecord}" disabled="{!v.flag}"/>
                </div>
            </lightning:recordEditForm>
        </div>
    </div>　　<!-- 弹窗打开的时候，用一个遮罩层将页面变暗-->　　<div aura:id="overlay" class="slds-backdrop slds-backdrop--open"></div>
</aura:component>

此时预览下页面查看效果

通过<lightning:recordEditForm>组件，就能比较方便的重写新建/更新按钮，但此时会出现一个问题
当点击更新按钮的时候，会发现使用<lightning:input>的用户名没有办法进行更新，此时查阅相关文档
<Lightning:recordEditForm> 发现这样一句话：

The lightning:inputField component is used inside the lightning:recordEditForm to create editable fields. The lightning:outputField component and other display components such as lightning:formattedName can be used to display read-only information in your form.

简单说，除了使用<lightning:inputField>的字段是可编辑的，lightning:input，lightning:formattedName等其他标签都是只读的
于是这里就遇到一个很严重的问题，查阅文档知道lightning:inputField字段不支持onblur属性，它支持onchange属性，而需求是在输入用户名后通过接口实时对输入的数据进行校验并获取返回的用户ID.
直接操作看起来不太可行，于是想到了迂回解决的办法，把更新也当作新建处理.
在点击更新的时候，将页面记录ID传到后台进行初始化，把查询出来的部门信息反向填充到表单中，这样就能绕开<lightning:recordEditForm>更新时lightning:input无法编辑，而lightning:inputField又不支持onblur属性的问题.

PS：此处补充一个<lightning:recordEditForm>组件的小问题，<lightning:recordEditForm>在提交按钮
<lightning:button variant="brand"   label="Submit" onclick="{!c.saveRecord}"  type="submit"/>
会有一个默认的提交行为，所以不小心就会出现提交两次的问题，此时在JSController提交方法中设置// 取消默认的提交行为event.preventDefault();就能取消组件的默认提交行为，以下是文档的描述To customize the behavior of your form when it loads or when data is submitted, use the onload and onsubmit attributes to specify event handlers. If you capture the submit event and submit the form programmatically, use event.preventDefault() to cancel the default behavior of the event. This prevents a duplicate form submission.

接下来处理第二个问题：在父对象项目的相关列表中创建部门对象时，如何自动填充父对象的问题.
很遗憾， Salesforce没有提供类似v.recordId这样的属性可以直接获取父级id，甚至在查阅解决方案的时候发现有人提了Idea

后来有个想法就是说不管怎么设计，相关列表创建子对象记录一定绕不开的就是通过url传递父对象的ID
所以在组件的Helper.js中写一个解析URL的方法

//  获取URL参数
getUrlParameter : function(component,event,name) {
    name = name.replace(/[\[\]]/g, "\\$&"); 
    var url = window.location.href; 
    var regex = new RegExp("[?&]" + name + "(=1\.([^&#]*)|&|#|$)"); 
    var results = regex.exec(url); 
    if (!results) return null; 
    if (!results[2]) return ''; 
    return decodeURIComponent(results[2].replace(/\+/g, " "));
}

在组件的Controller.js中打印父级ID

doInit : function(component, event, helper){
    // 获取可能的父级id
    var value = helper.getUrlParameter(component,event,'inContextOfRef');
    var context = JSON.parse(window.atob(value));
    var parentId = context.attributes.recordId;
    console.log('*** parentId:' + parentId);
},


判断如果parentId存在，则将Id传递到后台，查询父对象的信息并返回数据进行填充

// 从父级对象创建部门的时候给对应的字段预填充
@AuraEnabled
public static String initFunction(String parentId){
    System.debug('*** parentId:' + parentId);

    Test_Department__c permiss = new Test_Department__c();
    // 考虑权部门以后除了项目还有其他的主表，这里要检验parentId属于那个对象
    String prefix = Test_Project__c.sobjecttype.getDescribe().getKeyPrefix();
    if (parentId.substring(0, 3) == prefix) {
        permiss.Project_Dep__c = parentId;    
    }    
    System.debug('*** 部门:' + permiss);
    return JSON.serialize(permiss);
}

 填充效果如下
 
 
https://www.cnblogs.com/luqinghua/p/10291452.html
**************************************************
正则表达式实现密码检查
起因
起因是一个朋友问怎么实现一个密码检查功能：


密码只能由大写字母，小写字母，数字构成；
密码不能以数字开头；
密码中至少出现大写字母，小写字母和数字这三种字符类型中的两种；
密码长度8-100位


然后他贴了写的代码：
$value = 'A1234567890a';
$rule = '/^[A-Z][A-Za-z]{7,100}|^[A-Z][A-Z0-9]{7,100}|^[a-z][A-Za-z]{7,100}|^[a-z][0-9a-z]{7,100}$/';
var_dump(preg_match($rule,(string)$value));    
一看这变量名以$开头，大概是php的，但我不怎么懂PHP，只看得懂中间的正则，猜最后一行是输出匹配结果。
这个正则显然是不满足上面说的条件的，|的优先级比$低，所以前面的表达式没有控制密码长度的作用。我依稀记得曾经学正则也看过相关的题目，这种复杂的正则检查是要用零宽断言。
然而我觉得零宽断言这东西写起来复杂，别人看更觉得复杂的东西，团队里面显然不是所有人都会去弄懂的。写这种复杂正则，以后需求变更，修改起来简直是噩梦，所以没怎么认真看，只知道有这么个东西，于是我就认真翻了一下相关的文章。
链接如下：
https://deerchao.net/tutorials/regex/regex.htm
这个是我觉得写得比较好的，内容很充实的一个文章了。
解决问题
对于这个需求，我们先完成第1、2和4点，这三个比较简单，正则写出来如下
var regex = /^[A-Za-z][A-Za-z0-9]{7,99}$/
这个问题难写的就是第3点，如果不用零宽断言，是要用很多个|去做判断，非常长。
第三点是

密码中至少出现大写字母，小写字母和数字这三种字符类型中的两种；

换个说法就是

不能全是大写字母，不能全是小写字母，不能全是数字。

或者说

存在一个非大写字母，存在一个非小写字母，存在一个非数字。

第一种
对于第一种说法，可以用
var regex = /^(?![A-Z]*$)/; // 不能全是大写字母
var regex2 = /^(?![a-z]*$)/; // 不能全是小写字母
var regex3 = /^(?![0-9]*$)/; // 不能全是数字
这里的*可以换成+，只会在空字符串的时候有差异，对于我们上面的需求是没有差异的，因为限定了8-100的长度。
与前面的正则组合起来就是
var regex = /^(?![A-Z]*$)(?![a-z]*$)(?![0-9]*$)[A-Za-z][A-Za-z0-9]{7,99}$/;
第二种
对于第二种说法，可以用
var regex = /^(?=.*[^A-Z])/ // 存在一个非大写字母
var regex2 = /^(?=.*[^a-z])/ // 存在一个非小写字母
var regex3 = /^(?=.*[^0-9])/ // 存在一个非数字
这里*不能替换成+，为什么可以自己思考一下，我会给一反例证明替换有问题。
组合起来就是
var regex = /^(?=.*[^A-Z])(?=.*[^a-z])(?=.*[^0-9])[A-Za-z][A-Za-z0-9]{7,99}$/;
如果将所有的*替换成+，那么对于Aaaaaaaa，正则匹配会失败，根据需求应该是成功。
第三种
什么？居然还有第三种？是的，当然还有第三种，和第四种。
刚刚我们写的断言，按照链接给的名字是零宽度正预测先行断言(?=exp)和零宽度负预测先行断言(?!exp)，当然名字其实不重要的，翻译过来的名字有很多种，不同的文章也不同，也不知道哪个是比较官方的。
所以我们可以用另外两种断言实现，分辨是零宽度正回顾后发断言(?<=exp)和零宽度负回顾后发断言(?<!exp)
对于第三种和第四种，它是从后面检查前面的断言，这种断言据说js是不支持的，但是chrome的引擎似乎是支持的，也不清楚是怎么回事。
用(?<!exp)形式写第一种说法
var regex = /(?<!^[A-Z]*)$/; // 不能全是大写字母
var regex2 = /(?<!^[a-z]*)$/; // 不能全是小写字母
var regex3 = /(?<!^[0-9]*)$/; // 不能全是数字
同样的*可以替换成+，组合起来就是
var regex = /^[A-Za-z][A-Za-z0-9]{7,99}(?<!^[A-Z]*)(?<!^[a-z]*)(?<!^[0-9]*)$/
第四种
不废话了直接贴代码
var regex = /(?<=[^A-Z].*)$/ // 存在一个非大写字母
var regex2 = /(?<=[^a-z].*)$/ // 存在一个非小写字母
var regex3 = /(?<=[^0-9].*)$/ // 存在一个非数字
组合后如下，同样的不能把*替换成+，反例是aaaaaaaA
var regex = /^[A-Za-z][A-Za-z0-9]{7,99}(?<=[^A-Z].*)(?<=[^a-z].*)(?<=[^0-9].*)$/;
第五种、第六种……
组合有很多种，因为条件都说的很清楚了，四种断言是可以组合的，所以其实远不止以上说的四种情况，重要的是要掌握四种断言的本质。
结尾
本文只是一个引子，并不是说怎么学正则，学习正则可以参考上面给的链接，这里再给一次https://deerchao.net/tutorials/regex/regex.htm
警告：不要写过于复杂的正则，多人协作中，代码的可读性远比性能和炫技重要。

https://www.cnblogs.com/Weilence/p/10291459.html
**************************************************
发布xxl-job executor dotnet core 执行器的实现
DotXxlJob
[github][https://github.com/xuanye/DotXxlJob]
xxl-job的dotnet core 执行器实现，支持XXL-JOB 2.0+
1 XXL-JOB概述
[XXL-JOB][1]是一个轻量级分布式任务调度平台，其核心设计目标是开发迅速、学习简单、轻量级、易扩展。现已开放源代码并接入多家公司线上产品线，开箱即用。以下是它的架构图

2. 关于DotXxlJob产生
在工作中调研过多个任务调度平台，如Hangfire、基于Quatz.NET的第三方扩展，都与实际的需求有一点差距。 之前一直使用Hangfire，Hangfire的执行器在同步调用业务服务时，如果即时业务服务正在重新部署或者重启，有一定概率会出现死锁，导致CPU100%，后来全部调整为异步，但是这样就无法获得执行结果，这样的设计有蛮大问题，XxlJob的回调机制很好的解决了这个问题。本身如果通过http的方式调用，只要部署springbootd的一个执行器就可以解决问题，但是扩展性较差。所以萌生了实现DotNet版本的执行器的想法，为避免重复造轮子，开始之前也进行过调研，以下仓库[https://github.com/yuniansheng/xxl-job-dotnet][2]给了较大的启发，但是该库只支持1.9版本的xxljob，还有一些其他小问题，所以还是自力更生。
3. 如何使用
目前只实现了BEAN的方式，即直接实现IJobHandler调用的方式，Glue源码的方式实际上实现起来也并不复杂（有需求再说把），或者各位有需求Fork 实现一下
可参考sample
安装:

dotnet add package DotXxlJob.Core

3.1 在AspNetCore中使用

生命一个AspNet的Middleware中间件,并扩展ApplicationBuilder，本质是拦截Post请求，解析Body中的流信息

public class XxlJobExecutorMiddleware
{
    private readonly IServiceProvider _provider;
    private readonly RequestDelegate _next;

    private readonly XxlRpcServiceHandler _rpcService;
    public XxlJobExecutorMiddleware(IServiceProvider provider, RequestDelegate next)
    {
        this._provider = provider;
        this._next = next;
        this._rpcService = _provider.GetRequiredService<XxlRpcServiceHandler>();
    }

    public async Task Invoke(HttpContext context)
    {

        if ("POST".Equals(context.Request.Method, StringComparison.OrdinalIgnoreCase) && 
            "application/octet-stream".Equals(context.Request.ContentType, StringComparison.OrdinalIgnoreCase))
        {
            var rsp =  await _rpcService.HandlerAsync(context.Request.Body);
            context.Response.StatusCode = (int) HttpStatusCode.OK;
            context.Response.ContentType = "text/plain;utf-8";
            await context.Response.Body.WriteAsync(rsp,0,rsp.Length);
            return;
        }
        await _next.Invoke(context);
    }
}
扩展ApplicationBuilderExtensions,可根据实际情况绑定在特殊的Url Path上
public static class ApplicationBuilderExtensions
{
    public static IApplicationBuilder UseXxlJobExecutor(this IApplicationBuilder @this)
    {
       return @this.UseMiddleware<XxlJobExecutorMiddleware>();
    }
}
在Startup中添加必要的引用,其中自动注册。
public class Startup
{
    public Startup(IConfiguration configuration)
    {
        Configuration = configuration;
    }

    private IConfiguration Configuration { get; }
    
    public void ConfigureServices(IServiceCollection services)
    {
      
        services.AddXxlJobExecutor(Configuration);
        services.AddSingleton<IJobHandler, DemoJobHandler>(); // 添加自定义的jobHandler
        services.AddAutoRegistry(); // 自动注册
    }


    public void Configure(IApplicationBuilder app,IHostingEnvironment env)
    {
        //启用XxlExecutor
        app.UseXxlJobExecutor();
    }
}
编写JobHandler,继承AbstractJobHandler或者直接实现接口IJobHandler，通过context.JobLogger 记录执行过程和结果，在AdminWeb上可查看的哦
[JobHandler("demoJobHandler")]
public class DemoJobHandler:AbstractJobHandler
{
    public override Task<ReturnT> Execute(JobExecuteContext context)
    {
        context.JobLogger.Log("receive demo job handler,parameter:{0}",context.JobParameter);

        return Task.FromResult(ReturnT.SUCCESS);
    }
}
3.2 配置信息
管理端地址和端口是必填信息，其他根据实际情况，选择配置，配置项说明见下代码中的注释
 public class XxlJobExecutorOptions
{
   
    /// <summary>
    /// 管理端地址，多个以;分隔
    /// </summary>
    public string AdminAddresses { get; set; }
    /// <summary>
    /// appName自动注册时要去管理端配置一致
    /// </summary>
    public string AppName { get; set; } = "xxl-job-executor-dotnet";
    /// <summary>
    /// 自动注册时提交的地址，为空会自动获取内网地址
    /// </summary>
    public string SpecialBindAddress { get; set; }
    /// <summary>
    /// 绑定端口
    /// </summary>
    public int Port { get; set; }
    /// <summary>
    /// 是否自动注册
    /// </summary>
    public bool AutoRegistry { get; set; }
    /// <summary>
    /// 认证票据
    /// </summary>
    public string AccessToken { get; set; }
    /// <summary>
    /// 日志目录，默认为执行目录的logs子目录下，请配置绝对路径
    /// </summary>
    public string LogPath { get; set; } = Path.Combine(AppContext.BaseDirectory, "./logs");
    /// <summary>
    /// 日志保留天数
    /// </summary>
    public int LogRetentionDays { get; set; } = 30;
}
在其他Http服务中使用
只需要实现Http请求的拦截，并判断post请求中content-Type="application/octet-stream",并使用XxlRpcServiceHandler来处理流 即可。
其他说明
XXL-JOB内置的RPC是使用Hessian协议，这个有点坑。很多都是java特有的属性和标识，比如类名什么的。在本项目中，并没有实现完整的Hessian2协议，只实现了使用到的类型，当然扩展起来也非常方便。如果有人要单独使用Hessian 这个类库的话，要特别注意这个问题。
有任何问题，可Issue反馈 ,最后感谢 xxl-job

https://www.cnblogs.com/xuanye/p/xxl-job-executor-dotnet-port.html
**************************************************
软件管理yum--技术流ken
 
软件管理yum
 
也是一个rpm包的管理工具，可以实现自动解决rpm包的依赖关系（自动安装依赖顺序进行rpm包的安装）
 
为何rpm包会有依赖关系？
制作rpm的人，在制作rpm包的时候，就将这个rpm的所依赖的软件的信息保存在这个rpm包的内部
 
yum的工作原理
 
1. 需要首先创建一个yum仓库（rpm包仓库、软件仓库）
1）仓库其实就是一个目录
2）仓库中存放的是rpm包
3）仓库中还保存了一个文件，文件中记录了该仓库中所有rpm包的元数据信息
4）元数据信息包括
软件名
软件版本
软件是否已经安装
软件的依赖关系
 
2. 用 yum 来从仓库中找软件进行安装的过程（比如要安装软件A，A依赖B 和C, B依赖C D）
1）yum首先会扫描保存元数据信息的文件，检查A是否已经安装，如果已经安装，那提示已经安装
2）如果A尚未安装，那么扫描A的依赖关系信息，会发现A 依赖B 和C
3）yum会再次扫描元数据文件，检测B C 是否安装，如果都已经安装，那么会开始开始安装A
4）如果B C尚未安装，那么会检测 B C的依赖关系，会发现B依赖C D
5）yum会再次扫描元数据文件，检测C D是否安装，如果都已经安装，那么会开始开始安装B，然后安装C,，然后安装A
 
yum仓库
 
本地：将本地的一个目录做成yum仓库，只有当前系统可以
网络：通过网络将服务器上的一个目录作为yum仓库，网络中的全部主机都可以用
 
使用yum仓库的方式，就是修改yum的配置文件
 
yum的配置文件
主：/etc/yum.conf
子：/etc/yum.repos.d/*.repo
 
/etc/yum.conf文件
cachedir=/var/cache/yum/$basearch/$releasever
指定缓存文件的保存位置,默认：/var/cache/yum/x86_64/7/
keepcache=0
指定是否保留缓存文件
 
 
用yum安装软件过程中会从yum仓库下载并缓存多个资源
1）会将yum仓库的元数据文件缓存到配置文件所指定的路径中
2）会将要安装的软件及其依赖的软件一并缓存到配置文件指定的目录中
 
yum配置文件的构成
 
[localRpm]      <<< 指定yum仓库的id，可以随便写，但是中间不能有空格
name=xxx        <<< 指定yum仓库的名称，可以随便写
enabled=0|1     <<< 指定是否使用该yum仓库，0表示不使用；1表示使用
gpgcheck=0|1    <<< 指定是否对rpm包做完整性和来源合法性验证，0表示不做验证；1表示必须做验证
gpgkey=         <<< 指定公钥文件（如果gpgcheck=1，那么该项不能省略）
baseurl=        <<< 指定yum仓库的url
 
yum仓库的url的表示方式
 
注意：在指定yum仓库的时候，其实不是指向rpm包的目录，而是执行repodata所在目录
 
本地yum仓库
file://
例子：比如我的yum仓库 /myrpm，此时yum仓库的表示方式就是file:///myrpm
网络yum仓库
http://
 
创建本地yum仓库
 
案例1：
第一步：创建目录
# mkdir  /myrpm
 
第二步：在仓库中准备一个用于测试的rpm
# cp /mnt/Packages/* /myrpm -rvf
 
第三步：安装用于生成元数据文件的工具（createrepo）
# rpm -ivh /mnt/Packages/deltarpm-3.5-0.5.20090913git.el6.x86_64.rpm
# rpm -ivh /mnt/Packages/python-deltarpm-3.5-0.5.20090913git.el6.x86_64.rpm
# rpm -ivh /mnt/Packages/createrepo-0.9.9-22.el6.noarch.rpm
 
第四步：生成元数据
# createrepo /myrpm
 
第五步：检查
# ls /myrpm
 
 
 
案例2：
使用光盘中的rpm包作为yum仓库
第一步：挂载光盘
# mount /dev/cdrom /media
 
第二步：备份yum自带的配置文件
# cd /etc/yum.repos.d
# mkdir bak
# mv *.repo bak
 
第三步：创建yum的配置文件，使用光盘作为yum仓库
# vim 1.repo
[myrepo]
name=my repo
enabled=1
gpgcheck=0
baseurl=file:///media
 
第四步：执行yum命令检测结果
# yum clean all    <<< 清空yum缓存的全部数据
# yum repolist     <<< 检查yum仓库中有多少个可用的rpm包
 
yum使用
 
1. 安装软件
yum -y install 软件1 软件2 软件3 ....
 
2. 卸载软件
yum -y remove 软件1 软件2 软件3 ....
 
3. 管理包组
        grouplist：查看系统中的全部的包组
        groupinfo 包组名：查看指定包组的信息（包组的作用、包含的软件）
        groupinstall 包组名：安装指定的包组
        groupremove 包组名：卸载指定的包组
       
4. 管理yum的缓存数据
     clean [ packages | metadata | all ]
     all：清除所有数据
     packages：仅仅清除rpm包
     metadata：仅仅清理缓存元数据
 
5. 查看所有已经安装和尚未安装rpm包
      yum list all 或者yum list 可以列出所有的软件包
      
6. yum list available：仅仅显示可以安装但是尚未安装的rpm包
 
7. yum list  installed：仅仅显示已经安装rpm包
     @：表示已经安装rpm
 
8. 查看yum仓库的信息
      yum repolist
 
网络yum源的使用方式
 
国内主流的网络yum仓库地址
mirrors.aliyun.com
mirrors.163.com
mirrors.sohu.com
 
rpm的yum源
centos提供的
epel提供的：
 
例子：使用aliyun提供的epel 和centos的yum源
cd /etc/yum.repos.d
mkdir bak
mv *.repo bak
vim new.repo
[centos]
name=centos repo
enabled=1
gpgcheck=0
baseurl=http://mirrors.163.com
 
[epel]
name=epel repo
enabled=1
enabled=1
baseurl=http://xxxx
 
制作一个网络yum源
 
思路：在网络上准备一个主机，在其中安装一个web服务器软件（比如apache），然后创建一个目录，在这个目录中准备上yum仓库的全部资源，如果用户可以通过网络访问到该主机的这个目录，那么这个目录就成为网络yum仓库
 
实现过程
第一步：配置yum源主机
1）安装配置apache
略
2）配置yum仓库相关资源
1. 创建一个目录，作为存储yum资源的目录
# mkdir /usr/local/apache/htdocs/mysource
2. 挂载光盘
# mount /dev/cdrom /mnt
3. 将光盘中的资源复制到前面所创建的目录下
# cp /mnt/* /usr/local/apache/htdocs/mysource -rvf
4. 用浏览器访问一下yum仓库，检查是否可以看到相关文件
http://ip/mysource
 
第二步：配置客户端主机
修改yum配置文件
1）备份原有的配置文件
# mkdir bak
# mv *.repo bak
# vim a.repo
[]
name
enabled=
gpgcheck=
baseurl=http://ip/mysource
2）测试一下
# yum repolist
https://www.cnblogs.com/kenken2018/p/10287781.html
**************************************************
2018年年终总结
  2018年最大的感受是人生太艰难了。
  1. 首先说说2018年的成果，做了如下事情：
 　1. 在北京定居买房了
    2. 跳槽到了某厂商做语音助手
    3. 和女友约定明年结婚
  2. 2018年做的遗憾的事情: 
  　1. 因为买房背负了高额的负债，每月房贷还的很艰难
  　2.工作加班非常的多，工作没有明显的产出，但身体健康下降的非常明显，严重影响了家庭的和谐
  　3.本身的成长速度严重低于预期
  3. 2018年回顾之买房
     买房是每个在北京工作的人绕不去的坎，通过家里的支持也好，自己的努力也好，能够上车的尽量上车。但是买房，不应该是生活的全部。只是现在的房贷还的很艰难，都快成为生活的全部了，心累。这个到底如何权衡，看个人自己的取舍。
     之前的想法很简单，在北京找一个同职业的的女朋友，两个人一起努力，大概率有机会在北京定居的，多久买房是时间的问题。从现在来看，这个目标算是提前实现了。然而这中间出来很多的变故，导致现在负债太高，又是出乎意料的。这件事情得到了两个教训，第一个事情，做事情一定要想明白了，自己想要什么，太过贪婪，超过自己的承受极限很容易就把人压垮了。第二个，一段时间的重点最好聚焦，比如今年我工作很忙，买房的事情前期基本交给了女朋友，导致她一个人承受了太多压力，出现很多的争吵。她认为我没有担当，我认为她不够理解我，导致了太多了争吵。两个人最重要的是目标一致，大家做的事情互相符合对方预期，家庭才能和睦。如果今年一开始就能达成一致，今年会好过很多。
  4.2018年回顾之跳槽到某互厂商做语音助手
    坦诚一点来讲今年是工作以来，最为痛苦的一年。一方面是和mentor沟通有很严重的问题，他也许自负聪明，或者是本身真的很厉害，或者本身也没有想明白，说话感觉老是只说一半，剩下一半完全是靠你自己领悟。做了大量工作，尝试了大量错误，每天从早上九点工作到晚上十点，做的工作却连日报里面都没有资格写入。一方面mentor 本身脾气也很急躁，说话也非常伤人。我经常做事做到怀疑人生。大家都是打工的，何必呢。
    一开始女友安慰我说，mentor 本身是有问题的，我承认，然后这个事实对解决问题没有什么帮助，除了换组，我只能改变自己。他经常吐槽我做事很慢，那么首先一点我得把能做的事情，更高效率的做完，更熟练的使用基本工具，基本命令，每次做事一次比一次快，这个需要经常总结，把常用命令记住，而不是每次去网上查，这一块新的一年要继续加强。第二他吐槽我代码很丑，基本语法不熟悉，工程能力差，那么我就把基本的语法再仔细看看，代码更加模块化，目前的代码我觉得最大的问题应该是变量的命名还有很大问题，这一块在code review 方面老是被打回, 其次是代码不够简洁，对基本的API还是不够了解，代码很冗余，估计新的一年还要继续优化。第三做了很多业务逻辑方面的优化，感觉对自己其实是没有任何提升的。
    在新的工作岗位呆了一年，做的很辛苦，有两点感受是很明显的，团队氛围是很重要的，团队氛围好，沟通顺畅才有更大概率把事情做好。现在这个团队大家都充满了戾气，沟通就一副你是傻逼的样子，这种状态把事情做好, 难度是很高的。以前我以为自己足够厉害，做事出彩完全是个人能力的因素，现在发现和上家公司的领导对我的帮助也是有很大关系的，跟对人真的太重要了，做什么反而还是其次。第二个就是做事情要么不做，要做就要做的足够出彩，这个时代，只要你足够出彩，任何人都很难限制你的发展，一件事情做到80%的程度，和没做是没什么区别的，今年十月以前做的的事情都不值得一提，最后两个月的事情让我从C到B了，给了我很大的信心。
    今年年初的目标是工作绩效要争取到B, 虽然最后的结果还没有出，但是大概率是能达到的。至于明年的计划，我想了很久，在这个团队我会有前途吗，在这个团队我会有成长吗，新的一年和mentor 关系可能沟通更顺畅吗。我内心是不抱指望的。互联网寒冬的到来，我也需要做好足够的准备。
   5.职业规划
     在曾经的一篇博客当中，我谈到以前的职业规划，希望专注算法，跟随自己的兴趣，同时贴合业务，让业务有落地的场景。时至今日，回首当初的选择，因为之前一直没有靠近过业务，所以对业务部门有天然的憧憬，面对当初的选择，放弃了纯算法的机会，放弃了看好自己的老大，固执的扎进了自己心仪的业务部门语音助手。现在每天和很多业务，产品经理打交道，做的工作越来越横向，以前和别人的沟通时候一直很难听懂别人的意思，如今慢慢能听懂了一部分，然而发现这个并非是我的兴趣，我本人的兴趣明显是更偏向算法一点，希望做的事情更加聚焦一点，业务部门天天一堆鸡毛蒜皮，扯皮拉筋的事情，太让人疲惫了。当然最疲惫的还是和团队老大关系不好，别人根本看不上你，不会带着你飞。
     到底什么是合适个人的职业规划，见仁见智了。目前在我看来最重要的：
     1 还是自己需要不断努力，提高面试的基本能力，保证自己想走的时候，可以去到更加心仪的公司，这个是一切的前提，保证实现职业规划的关键。
     2 我说了这么多团队氛围的事情，归根到底还是要自己做事足够优秀，这样老大才会带你飞。不管多什么傻逼的老大，你做事足够出成果了，都很难否定你。当然和领导关系好，对于把事情做好帮助也是巨大的。
     3 表达能力很重要，包括听懂别人的话，以及正面回应别人的能力，老大和你离的这么远，怎么发现你这个人牛逼，就是日常或者周会的一两次机会，这一两次机会，你把握住了，就把握住了。
     4 具体的方向，我会更加注重nlp 算法 + 工程能力方面，在这个部门呆了一年，我发现工程能力其实比算法重要太多了，很多应届生和想要转行的读了若干篇paper, 跑了一些demo, 就敢自称精通深度学习了，真要落地了，一地鸡毛，我之前也是这个状态，深感惭愧。
   6.新的一年计划
     1. 希望继续往模型复现方向这个方向深入下去，今年计划保底要复现四个模型，包括tensorflow 版本以及paddlePaddle 版本，训练以及预测模块，保证训练速度以及预测速度不低于竞品的50%
     2. 今年六月前 leetcode medium 400题 刷两遍
     3. 技术类博客更新6篇以上
     4. 参加两场线下分享
     5. 一周锻炼两次以上身体
   最后新的一年祝大家新年快乐，任何时候都不要放弃努力，大家一起加油。
https://www.cnblogs.com/ModifyRong/p/10285343.html
**************************************************
